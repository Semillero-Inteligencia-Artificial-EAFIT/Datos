Capítulo 7
Tratar con pocas o ninguna etiqueta UNA NOTA PARA LOS LECTORES DE LANZAMIENTO TEMPRANO Con los libros electrónicos de lanzamiento anticipado, obtiene libros en su forma más temprana, el contenido sin editar y sin editar del autor mientras escribe, para que pueda aprovechar estas tecnologías mucho antes del lanzamiento oficial de estos títulos.
Este será el capítulo 7 del libro final.
Tenga en cuenta que el repositorio de GitHub se activará más adelante
Si tiene comentarios sobre cómo podríamos mejorar el contenido y/o los ejemplos de este libro, o si nota que falta material en este capítulo, comuníquese con el editor a mpotter@oreilly
com
Hay una pregunta tan profundamente arraigada en la mente de todos los científicos de datos que suele ser lo primero que preguntan al comienzo de un nuevo proyecto: ¿hay datos etiquetados? La mayoría de las veces, la respuesta es "no" o "un poco", seguida de la expectativa del cliente de que los modelos sofisticados de aprendizaje automático de su equipo deberían funcionar bien.
Dado que entrenar modelos en conjuntos de datos muy pequeños no suele dar buenos resultados, una solución obvia es anotar más datos
Sin embargo, esto toma tiempo y puede ser muy costoso, especialmente si cada anotación requiere experiencia en el dominio para validarla.
Afortunadamente, existen varios métodos que son adecuados para tratar con pocas o ninguna etiqueta. Es posible que ya esté familiarizado con algunos de ellos, como el aprendizaje de disparos cero o pocos disparos de la impresionante capacidad de GPT-3 para realizar una amplia gama de tareas a partir de solo unas pocas docenas de ejemplos.
En general, el método con mejor rendimiento dependerá de la tarea, la cantidad de datos disponibles y la fracción etiquetada.
En la Figura 7-1 se muestra un árbol de decisiones para ayudarnos a guiarnos a través del proceso.
Figura 7-1
Varias técnicas que se pueden utilizar para mejorar el rendimiento del modelo en ausencia de grandes cantidades de datos etiquetados
Recorramos este árbol de decisiones paso a paso: ¿Tiene datos etiquetados? Incluso un puñado de muestras etiquetadas puede marcar la diferencia en cuanto a qué método funciona mejor.
Si no tiene datos etiquetados en absoluto, puede comenzar con el enfoque de aprendizaje de tiro cero en la SECCIÓN X, que a menudo establece una línea de base sólida para trabajar a partir de
¿Cuántas etiquetas? Si los datos etiquetados están disponibles, el factor decisivo es cuánto. Si tiene muchos datos de entrenamiento disponibles, puede usar el enfoque de ajuste fino estándar que se analiza en el Capítulo 2.
¿Tiene datos sin etiquetar? Si solo tiene un puñado de muestras etiquetadas, puede ser de gran ayuda si tiene acceso a grandes cantidades de datos sin etiquetar.
Si tiene acceso a datos no etiquetados, puede usarlos para ajustar el modelo de lenguaje en el dominio antes de entrenar a un clasificador o puede usar métodos más sofisticados como Universal Data Augmentation (UDA)1 o Uncertainty-Aware Self-Training (UST)2
Si tampoco tiene datos sin etiquetar disponibles, significa que ni siquiera puede anotar más datos si quisiera.
En este caso, puede usar el aprendizaje de pocas tomas o usar las incrustaciones de un modelo de lenguaje previamente entrenado para realizar búsquedas con una búsqueda de vecino más cercano.
En este capítulo, nos abriremos camino a través de este árbol de decisiones al abordar un problema común que enfrentan muchos equipos de soporte que usan rastreadores de problemas como Jira o GitHub para ayudar a sus usuarios: etiquetar problemas con metadatos según la descripción del problema.
Estas etiquetas pueden definir el tipo de problema, el producto que causa el problema o qué equipo es responsable de manejar el problema informado.
La automatización de este proceso puede tener un gran impacto en la productividad y permite que los equipos de soporte se concentren en ayudar a sus usuarios.
Como ejemplo de ejecución, usaremos los problemas de GitHub asociados con un popular proyecto de código abierto: ¡Hugging Face Transformers! Ahora echemos un vistazo a la información contenida en estos problemas, cómo enmarcar la tarea y cómo obtener los datos.
NOTA Los métodos presentados en este capítulo funcionan bien para la clasificación de texto, pero pueden ser necesarias otras técnicas, como el aumento de datos, para abordar tareas más complejas, como el reconocimiento de entidades nombradas, la respuesta a preguntas o el resumen.
Creación de un etiquetador de problemas de GitHub Si navega a la pestaña Problemas del repositorio de Transformers, encontrará problemas como el que se muestra en la Figura 7-2, que contiene un título, una descripción y un conjunto de etiquetas o etiquetas que caracterizan el problema.
Esto sugiere una forma natural de enmarcar la tarea de aprendizaje supervisado: dado un título y una descripción de un tema, predecir una o más etiquetas
Dado que a cada número se le puede asignar un número variable de etiquetas, esto significa que estamos tratando con un problema de clasificación de texto de etiquetas múltiples.
Este problema suele ser más desafiante que la configuración multiclase que encontramos en el Capítulo 2, donde cada tweet se asignó a una sola emoción.
Figura 7-2
Un problema típico de GitHub en el repositorio de Transformers
Ahora que hemos visto cómo se ven los problemas de GitHub, veamos cómo podemos descargarlos para crear nuestro conjunto de datos.
Obtener los datos Para obtener todos los problemas del repositorio, usaremos la API REST de GitHub para sondear el punto final de problemas
Este punto final devuelve una lista de objetos JSON, donde cada elemento contiene una gran cantidad de campos sobre el problema, incluido su estado (abierto o cerrado), quién abrió el problema, así como el título, el cuerpo y las etiquetas que vimos en la Figura 7-2.
Para sondear el punto final, puede ejecutar el siguiente comando curl para descargar el primer problema en la primera página:Dado que lleva un tiempo recuperar todos los problemas, hemos incluido un problema
jsonlfile en el repositorio de GitHub de este libro, junto con una función fetch_issues para descargarlos usted mismo
NOTALa API REST de GitHub trata las solicitudes de extracción como problemas, por lo que nuestro conjunto de datos contiene una combinación de ambos
Para simplificar las cosas, desarrollaremos nuestro clasificador para ambos tipos de problemas, aunque en la práctica podría considerar crear dos clasificadores separados para tener un control más detallado sobre el rendimiento del modelo.
Preparación de los datos Una vez que hayamos descargado todos los problemas, podemos cargarlos usando Pandas: importar pandas como Hay casi 10,000 problemas en nuestro conjunto de datos y al mirar una sola fila podemos ver que la información recuperada de la API de GitHub contiene muchos campos como URL , ID, fechas, usuarios, título, cuerpo y etiquetas: las columnas de etiquetas son lo que nos interesa, y cada fila contiene una lista de objetos JSON con metadatos sobre cada etiqueta: para nuestros propósitos, estamos solo está interesado en el campo de nombre de cada objeto de etiqueta, así que sobrescribamos la columna de etiquetas con solo los nombres de las etiquetas: A continuación, echemos un vistazo a las 10 etiquetas más frecuentes en el conjunto de datos
En Pandas podemos hacer esto "explotando" la columna de etiquetas para que cada etiqueta en la lista se convierta en una fila, y luego simplemente contamos la aparición de cada etiqueta: Podemos ver que hay 65 etiquetas únicas en el conjunto de datos y que las clases están muy desequilibradas , siendo wontfix y model card las etiquetas más comunes
Para que el problema de clasificación sea más manejable, nos centraremos en crear un etiquetador para un subconjunto de las etiquetas.
Por ejemplo, algunas etiquetas como Good First Issue o Help Wanted son potencialmente muy difíciles de predecir a partir de la descripción del problema, mientras que otras, como la tarjeta modelo, podrían clasificarse con una regla simple que detecta cuándo se agrega una tarjeta modelo en Hugging Face Hub.
El siguiente código muestra el subconjunto de etiquetas con las que trabajaremos, junto con una estandarización de los nombres para que sean más fáciles de leer: Ahora veamos la distribución de las nuevas etiquetas: tokenización nuevo modelo modelo entrenamiento uso canalización tensor flujo o tf pytorch documentación ejemplos Dado que se pueden asignar varias etiquetas a un solo problema, también podemos ver la distribución de recuentos de etiquetas: la gran mayoría de los problemas no tienen etiquetas en absoluto, y solo un puñado tiene más de una
Más adelante en este capítulo, encontraremos útil tratar los problemas sin etiquetar como una división de capacitación separada, así que vamos a crear una nueva columna que indique si los problemas están sin etiquetar o no: Ahora veamos un ejemplo: Google propuso recientemente un nuevo En este ejemplo, se propone una nueva arquitectura de modelo, por lo que la nueva etiqueta de modelo tiene sentido
También podemos ver que el título contiene información que será útil para nuestro clasificador, así que concatenémoslo con la descripción del problema en el campo del cuerpo: Como hemos hecho en otros capítulos, es una buena idea echar un vistazo rápido al número de palabras en nuestros textos para ver si perdemos mucha información durante el paso de tokenización: La distribución tiene una cola larga característica de muchos conjuntos de datos de texto
La mayoría de los textos son bastante cortos, pero también hay problemas con más de 1000 palabras.
Es común tener problemas muy extensos, especialmente cuando se publican mensajes de error y fragmentos de código.
Creación de conjuntos de entrenamiento Ahora que hemos explorado y limpiado nuestro conjunto de datos, lo último que queda por hacer es definir nuestros conjuntos de entrenamiento para comparar nuestros clasificadores.
Queremos asegurarnos de que las divisiones estén equilibradas, lo cual es un poco más complicado para un problema de múltiples etiquetas porque no hay un equilibrio garantizado para todas las etiquetas.
Sin embargo, se puede aproximar y, aunque scikit-learn no es compatible, podemos usar la biblioteca scitkit-multilearn que está configurada para problemas de etiquetas múltiples.
Lo primero que debemos hacer es transformar nuestro conjunto de etiquetas, como pytorch y tokenización, en un formato que el modelo pueda procesar.
Aquí podemos usar la clase de transformador MultiLabelBinarizer de Scikit-Learn, que toma una lista de nombres de etiquetas y crea un vector con ceros para las etiquetas ausentes y unos para las etiquetas presentes.
Podemos probar esto ajustando MultiLabelBinarizer en all_labels para aprender la asignación del nombre de la etiqueta a la ID de la siguiente manera: En este ejemplo simple, podemos ver que la primera fila tiene dos correspondientes a la tokenización y las etiquetas del nuevo modelo, mientras que la segunda fila tiene solo un hit con pytorch
Para crear las divisiones, podemos usar la función iterative_train_test_split, que crea las divisiones de entrenamiento/prueba iterativamente para lograr etiquetas equilibradas.
Lo envolvemos en una función que podemos aplicar a DataFrames
Dado que la función espera una matriz de características bidimensional, necesitamos agregar una dimensión a los posibles índices antes de realizar la división: con esa función en su lugar, podemos dividir los datos en conjuntos de datos supervisados ​​y no supervisados ​​y crear conjuntos equilibrados de tren, validación y prueba para la parte supervisada. :Finalmente, creemos un DatasetDict con todas las divisiones para que podamos tokenizar fácilmente el conjunto de datos e integrarlo con el Entrenador
Aquí usaremos el ingenioso conjunto de datos
función from_pandas de conjuntos de datos para cargar cada división directamente desde el marco de datos de Pandas correspondiente: esto se ve bien, por lo que lo último que debe hacer es crear algunos segmentos de entrenamiento para que podamos evaluar el rendimiento de cada clasificador en función del tamaño del conjunto de entrenamiento
Creación de segmentos de entrenamiento El conjunto de datos tiene las dos características que nos gustaría investigar en este capítulo: datos etiquetados dispersos y clasificación de múltiples etiquetas.
El conjunto de entrenamiento consta de solo 220 ejemplos para entrenar, lo que sin duda es un desafío, incluso con el aprendizaje por transferencia.
Para profundizar en el rendimiento de cada método de este capítulo con pocos datos etiquetados, también crearemos segmentos de los datos de entrenamiento con aún menos muestras.
Luego podemos trazar el número de muestras contra el rendimiento e investigar varios regímenes.
Comenzaremos con solo 8 muestras por etiqueta y aumentaremos hasta que el segmento cubra el conjunto de entrenamiento completo usando la función iterative_train_test_split: Genial, finalmente hemos preparado nuestro conjunto de datos en divisiones de entrenamiento. Bayesline Siempre que comience un nuevo proyecto de PNL, siempre es una buena idea implementar un conjunto de líneas base sólidas por dos razones principales: 1
Una línea de base basada en expresiones regulares, reglas hechas a mano o un modelo muy simple ya podría funcionar muy bien para resolver el problema.
En estos casos, no hay motivo para utilizar transformadores de gran tamaño que, por lo general, son más complejos de implementar y mantener en entornos de producción.
