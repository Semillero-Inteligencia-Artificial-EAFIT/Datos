Chapter 10
 TrainingTransformers from ScratchA NOTE FOR EARLY RELEASE READERSWith Early Release ebooks, you get books in their earliest form—theauthor’s raw and unedited content as they write—so you can takeadvantage of these technologies long before the official release of thesetitles
This will be the 10th chapter of the final book
 Please note that theGitHub repo will be made active later on
If you have comments about how we might improve the content and/orexamples in this book, or if you notice missing material within thischapter, please reach out to the editor at mpotter@oreilly
com
In Chapter 1 we looked at a sophisticated application called GitHub Copilotthat uses a GPT-like transformer to generate code from a variety ofprompts
 Tools like Copilot enable programmers to write code moreefficiently by generating a lot of the boilerplate code automatically or bydetecting likely mistakes
 Later in Chapter 8 we had a closer look at GPTlike models and how we can use them to generate high-quality text
 In thischapter we complete the circle and build our very own code generationmodel based on the GPT architecture! Since programming languages use avery specific syntax and vocabulary that is distinct from natural language, itmakes sense to train a new model from scratch rather than fine-tune anexisting one
So far we’ve mostly worked on data-constrained application where theamount of labeled training data is limited
 In these cases, transfer learning –in which we start from a model pretrained on a much larger corpus – helpedus build performant models
 The transfer learning approach peaked inChapter 7 where we barely used any training data at all
In this chapter we’ll move to the other extreme; what can we do when weare drowning with data? With that question we will explore the pretrainingstep itself and learn how to train a transformer from scratch
 Solving thistask will show us aspects of training that we have not paid attention to yet:Gathering and handling a very large dataset
Creating a custom tokenizer for our dataset
Training a model at scale
To efficiently train large models with billions of parameters, we’ll needspecial tools for distributed training and pipelining
 Fortunately there is alibrary called Hugging Face Accelerate which is designed for preciselythese applications! We’ll end up touching on some of the largest NLPmodels today
TODO add picture of transfer learning from chapter 1 with focus onpretraining?Large Datasets and Where to Find ThemThere are many domains and tasks where you may actually have a largeamount of data at hand
 These range from legal documents, to biomedicaldatasets, and even programming codebases
 In most cases, these datasetsare unlabeled and their large size means that they can usually only belabeled through the use of heuristics or by using accompanying metadatathat is stored during the gathering process
 For example, given a corpus ofPython functions, we could separate the docstrings from the code and treatthem as the targets we wish to generate in a seqs2seq task
A very large corpus can be useful even when it is unlabeled or onlyheuristically-labeled
 We saw an example of this in Chapter 7 where weused the unlabeled part of a dataset to fine-tune a language model fordomain adaptation, which yielded a performance gain especially in the lowdata regime
 Here are some high-level examples that we will illustrate in thenext section:If an unsupervised or self-supervised training objective similar toyour downstream task can be designed, you can train a model foryour task without labeling your dataset
If heuristics or metadata can be used to label the dataset at scalewith labels related to your downstream task, it’s also possible touse this large dataset to train a model useful for your downstreamtask in a supervised setting
The decision to train from scratch rather than fine-tune an existing model ismostly dictated by the size of your fine-tuning corpus and the divergingdomain between the available pretrained models and the corpus
In particular, using a pretrained model forces you to use the tokenizerassociated with this pretrained model
 But using a tokenizer that is trainedon a corpus from another domain is typically sub-optimal
 For example,using GPT’s pretrained tokenizer on another field such as legal documents,other languages or even completely different sequences such as musicalnotes or DNA sequences will results in bad tokenization as we will seeshortly
As the amount of training dataset you have access to gets closer to theamount of data used for pretraining, it thus becomes interesting to considertraining the model and the tokenizer from scratch
 Before we discuss thedifferent pretraining objectives further we first need to build a large corpussuitable for pretraining which comes with its own set of challenges
Challenges with Building a Large Scale CorpusThe quality of a model after pretraining largely reflects the quality of thepretraining corpus, and defect in the pretraining corpus will be inherited bythe model
 Thus, this is a good occasion to discuss some of the commonissues and challenges that are associated with building large corpora suitedfor pretraining before creating our own
As the dataset gets larger and larger, the chances that you can fully control –or at least have a precise idea of – what is inside the dataset diminish
 Avery large dataset will most likely not have been created by dedicatedcreators that craft one example at a time, while being aware andknowledgeable of the full pipeline and task that the machine learning modelwill be applied to
 Instead it is much more likely that a very large datasetwill have been created in an automatic or semi-automatic way by collectingdata that is generated as a side effect of other activities; for instance as datasent to a company for a task that the user is performing, or gathered on theInternet in a semi-automated way
There are several important consequences that follow from the fact thatlarge-scale datasets are mostly created with a high degree of automation
There is limited control on both their content and the way they are created,and thus the risk of training a model on biased and lower quality dataincreases
 Good examples of this are recent investigations on famous largescale datasets like BookCorpus1 or C42, which were used to train BERTand T5 respectively and are two models we have used in previous chapters
These “after-the-fact” investigations have uncovered among other things:A significant proportion of the C4 corpus is machine translatedwith automatic methods rather than humans
3Disparate erasure of African-American English as a result of stopwords filtering in C4 produced an under-representation of suchcontent
It is typically difficult in a large text corpus to find a middleground between (1) including (often too much) sexually-explicit orother type of explicit content or (2) totally erasing all mention ofsexuality or gender and, as a consequence, any legitimatediscussion around these questions
 As a surprising consequence ofthis, tokenizing a rather common word like “sex” with a neutralmeaning in addition to an explicit one is unnatural for a tokenizerthat is trained on C4 since this word is fully absent from the corpusand thus completely unknown
Many occurrences of copyright violation in BookCorpus andprobably in other large scale datasets as well
4Genre skew toward “romance” novels in BookCorpusThese discoveries might not be incompatible with downstream usage of themodels trained on these corpus
 For instance the strong over-representationof “romance” novels in BookCorpus is probably fine if the model isintended to be used as a romance novel writing tool (for instance to helpovercome creative angst) or game
Let’s illustrate the notion of a model being skewed by the data bycomparing text generations from GPT which was trained in large part onBookCorpus and GPT-2 which was trained on webpages/blogs/news linkedto from Reddit
 We compare similar sized versions of both models on thesame prompt so that the main difference is the training dataset
 We’ll usethe text generation pipeline to investigate the model outputs:Next let’s create a simple function to count the number of parameters ineach model:Now we can generate 3 different completions from each model, each withthe same input prompt:1
When they came back
" we need all we can get, " jason said once they had settledinto the back ofthe truck without anyone stopping them
 " after getting outhere, it 'll beup to us what to find
 for now2
When they came back
his gaze swept over her body
 he 'd dressed her, too, in theborrowed clothesthat she 'd worn for the journey
" i thought it would be easier to just leave you there
 " awoman like3
When they came back to the house and she was sitting there withthe little boy
" don't be afraid, " he told her
 she nodded slowly, her eyeswide
 she was solost in whatever she discovered that tom knew her mistakeBy just sampling a handful of outputs from both models we can already seethe distinctive “romance” skew in GPT generation which will typicallyimagine two characters of the opposite sex (a woman and a man), and adialog with a romance interaction between them
 On the other hand, GPT-2was trained on webtext linked to and from Reddit articles and mostly adoptsa neutral “they” in its generations that contain “blog-like” or adventurerelated elements like travels
In general, any model trained on a dataset will reflect the language bias andover- or under-representation of populations and events in its training data
These biases in the behavior of the model are important to take intoconsideration with regards to the target audience interacting with the model
In our case, our programming codebase will be mostly comprised of coderather than natural language but we still may want to:Balance the programming language we use
Filter low quality or duplicated code samples
Take the copyright information into account
Investigate the language contained in the included documentation,comments or docstrings, for instance to account for personalidentifying data
This should give a glimpse of the difficult challenges you face whencreating large text corpora and we refer the reader to a paper by Googlewhich provides a framework on dataset development
 With these in mind,let’s now take a look at creating our own dataset!Building a Custom Code DatasetTo simplify the task a bit we’ll focus on building a code generation modelfor the Python programming language (GitHub Copilot supports over adozen languages)
 The first thing we’ll need then in a large pretrainingcorpus of Python source code
 Fortunately there is a natural resource thatevery engineer knows: GitHub itself! The famous code sharing websitehosts gigabytes of code repositories which are openly accessible and can bedownload and used according to their respective licenses
 At the time of thisbook’s writing, GitHub hosts more than 20 million code repositories
 Manyof these are small or test repositories created by users for learning, futureside-projects or testing purposes
 This leads to a somewhat noisy quality ofthese repositories
 Let’s look at a few strategies for dealing with thesequality issues
To Filter the Noise or Not?Since anybody can create a GitHub repository, the quality of projects isvery broad
 GitHub allows people to “star” repositories which can provide aproxy metric for project quality by indicating that other users are interestedin a repository
 Another way to filter repositories can be to select projectswhich are demonstrably used in at least one other project
There is some conscious choice to be made here, related to how we wantthe system to perform in a real-world setting
 Having some noise in thetraining dataset will make our system more robust to noisy inputs atinference time, but will also make its predictions more random
 Dependingon the intended use and whole system integration, we may want to chooseto use more or less noisy data and add pre- and post-filtering operations
Often, rather than removing noise, an interesting solution is to model thenoise and to find a way to be able to do conditional generation based on theamount of noise we want to have in our model generation
 For instance herewe could:Retrieve (noisy) information on the quality of the code, forinstance with stars or downstream usageAdd this information as input to our model during the training, forinstance in the first token of each input sequenceAt generation/inference time, choose the token corresponding tothe quality we want (typically the maximal quality)
This way, our model will both (1) be able to accept and handle noisy inputswithout them being out of distribution and (2) generate good quality output
GitHub repositories can be accessed in two main ways:Via the GitHub REST API like we saw in Chapter 7 where wedownloaded all the GitHub issues of the Transformers library 
Via public dataset inventories like the one of Google BigQuery
Since the REST API is rate-limited and we need a lot data for ourpretraining corpus, we’ll use Google BigQuery to extract all the Pythonrepositories
 The bigquery-public-data
github_repos
contents table containscopies of all ASCII files that are less than 10 MB
 In addition, projects alsoneed to be open-source to be included, as determined by GitHub’s LicenseAPI
The Google BigQuery dataset doesn’t contain stars or downstream usageinformation, and to filter by stars or usage in downstream libraries, wecould use the GitHub REST API or a service like Libraries
io whichmonitors open-source packages
 Indeed, a dataset called CodeSearchNetwas released recently by a team from GitHub which filtered repositoriesused in at least one downstream task using information from Libraries
io
This dataset is also preprocessed in various ways (extracting top-levelmethods, splitting comments from code, tokenizing code)For the educational purposes of the present chapter and to keep the datapreparation code rather concise, we will not filter according to stars orusage and will just grab all the Python files in the GitHub BigQuery dataset
Let’s have a look at what it takes to create such a code dataset with GoogleBigQuery
Creating a Dataset with Google BigQueryWe’ll begin by extracting all the Python files on GitHub public repositoriesfrom the snapshot on Google BigQuery
 The steps to export these files areadapted from the TransCoder implementation and are as follows:Create a Google Cloud account (a free trial should be sufficient)
Create a Google BigQuery project under your accountIn this project, create a datasetIn this dataset, create a table where the results of the SQL requestbelow will be stored
Prepare and run the following SQL query on the github_repostableBefore running the SQL request, make sure to change thequery settings to save the query results in the table(MORE  Query Settings  Destination Set adestination table for query results put table name)Run the SQL request!This command processes about 2
6 TB of data to extract 26
8 million files
The result is a dataset of about 47 GB of compressed JSON files, each ofwhich contain the source code of Python files
 We filtered to remove emptyand small files such as _init_
py which don’t contain much usefulinformation
 We also remove files larger than 1 MB which are not includedin the BigQuery dump any way
 We also downloaded the licenses for all thefile so we can filter the training data based on licenses if we want
Let’s download the results to our local machine
 If you try this at homemake sure you have good bandwidth available and at least 50 GB of freedisk space
 The easiest way to get the resulting table to your local machinefollows this two step process:Export your results to Google Cloud:Create a bucket and a folder in Google Cloud Storage(GCS)Export your table to this bucket by selecting EXPORTExport to GCS  export format JSON , compression GZIPTo download the bucket to your machine use the gsutil library:For the sake of reproducibility and if the policy around free usage ofBigQuery changes in the future, we will also share this dataset on theHugging Face Hub
 Indeed, in the next section we will actually upload thisrepository on the Hub together!For now, if you didn’t use the above steps on BigQuery, you can directlydownload the dataset from the Hub as follows:We can now retrieve the dataset simply with:Working with a 50 GB dataset can be a challenging task
 On the one hand itrequires enough disk space and on the other hand one must be careful not torun out of RAM
 In the following section we have a look how the datasetslibrary helps dealing with large datasets on small machines
Working with Large DatasetsLoading a very large dataset is often a challenging task, in particular whenthe data is larger than your machine’s RAM
 For a large-scale pretrainingdataset, this is very much a common situation
 In our example, we have 47GB of compressed data and about 200 GB uncompressed data which isquite likely not possible to extract and load into the RAM memory of astandard sized laptop or desktop computer
Thankfully, the Datasets library has been designed from the ground up toovercome this problem with two specific features which allow you to setyoursefl free from:1
 RAM limitations with memory-mapping2
 Hard-drive space limitations with streamingMemory-mappingTo overcome RAM limitations, Datasets uses a mechanism for zero-copyand zero-overhead memory-mapping which is activated by default
Basically, each dataset is cached on the drive in a file which is a directreflection of the content in RAM memory
 Instead of loading the dataset inRAM, Datasets opens a read-only pointer to this file and uses it as asubstitute for RAM, basically using the hard-drive as a direct extension ofthe RAM memory
 You may wonder if this might not make our training I/Obound
 In practice, NLP data is usually very lightweight to load incomparison to the model processing computations so this is rarely an issue
In addition, the zero-copy/zero-overhead format used under the hood isApache Arrow which makes it very efficient to access any element
Let’s have a look how we can make use of this with our datasets:Up to now we have mostly used the Datasets library to access remotedatasets on the Hugging Face Hub
 Here we will directly load our 48 GB ofcompressed JSON files that we have stored locally
 Since the JSON files arecompressed we first need to decompress them which Datasets takes care offor us
 Be careful because this requires about 380 GB of free disk space
 Atthe same time this will use almost no RAM at all
 By settingdelete_extracted=True in the dataset’s downloading configuration,we can make sure that we delete all the files we don’t need anymore as soonas possible:Under the hood, the Datasets extracted and read all the compressed JSONfiles by loading them in a single optimized cache file
 Let’s see how big thisdataset is once loaded:As we can see this dataset is much larger than our typical RAM memory,but we can still load and access it
 We are actually still using a very limitedamount of memory with our Python interpreter
 The file on drive is used asan extension of RAM
 Iterating on it is slightly slower than iterating on inmemory data, but typically more than sufficient for any type of NLPprocessing
 Let’s run a little experiment on a subset of the dataset toillustrate this:Depending on the speed of your hard-drive and the batch size, the speed ofiterating over the dataset can typically range from a few tenth of GB/s toseveral GB/s
 This is great but what if you can’t free enough disk space tostore the full dataset locally? Everybody knows the feeling of helplessnesswhen getting a full disk warning and then painfully reclaiming GB after GBlooking for hidden files to delete
 Luckily you don’t need to store the fulldataset locally if you use the streaming feature of the Datasets library!StreamingWhen scaling up, some datasets will be difficult to even fit on a standardhard-drive
 In this case, an alternative to scaling upthe server you are using is to stream the dataset
 This is also possible withthe Datasets library for a number of compressed or uncompressed fileformats which can be read line by line, like JSON Lines, CSV or text, eitherraw, zip, gzip or zstandard compressed
 Let’s load our dataset directly fromthe compressed JSON files instead of creating a cache file from them:As you can notice loading the dataset was instantaneous! In streamingmode, the dataset the compressed JSON files will be opened and read onthe fly
 Our dataset is now an IterableDataset object
 This means thatwe cannot access random elements of it likestreamed dataset[1264] but we need to read it in order, forinstance with next(iter(streamed_dataset))
 It’s still possible touse methods like shuffle() but these will operate by fetching a buffer ofexamples and shuffling within this buffer (the size of the buffer isadjustable)
 When several files are provided as raw files (like here our 188files) shuffle() will also randomize the order of files for the iteration
Let’s create and iterator for our streamed dataset and peek at the first fewexamples:Note that when we loaded our dataset we provided the names of all theJSON files
 But when our folder only contains a set of JSON, CSV or textfiles, we can also just provide the path to the folder and Datasets will takecare of listing the files, using the convenient files format loader anditerating through the files for us
A simpler way to load the dataset is thus:The main interest of using a streaming dataset is that loading this datasetwill not create a cache file on the drive when loaded or require any(significant) RAM memory
 The original raw files are extracted and read onthe fly when a new batch of examples is requested, and only the sample orbatch is loaded in memory
Streaming is especially powerful when the dataset is not stored locally butaccessed directly on a remote server without downloading the raw data fileslocally
 In such a setup, we can then use arbitrary large datasets on an(almost) arbitrarily small server
 Let’s push our dataset on the Hugging FaceHub and accessing it with streaming
Adding Datasets to the Hugging Face HubPushing our dataset to the Hugging Face Hub will in particular allow us to:Easily access it from our training serverSee how streaming dataset also work seamlessly with datasets fromthe HubShare it with the community including you, dear reader!To upload the dataset, we first need to login to our Hugging Face accountby runninghuggingface-cli loginin the terminal and providing the relevant credentials
 Once this is done, wecan directly create a new dataset on the Hub and upload the compressedJSON files
 To make it easy, we will create two repositories: one for thetrain split and one with the validation split
 We can do this by running therepo create command of the CLI as follows:Here we’ve specified that the repository should be a dataset (in contrast tothe model repositories used to store weights), along with the organizationwe’d like to store the repositories under
 If you’re running this code underyour personal account, you can omit the organisation flag
 Next weneed to clone these empty repositories to our local machine, copy the JSONfiles to them, and push the changes to the Hub
 We will take the lastcompressed JSON file out of the 184 we have as the validation file, i
e
 aroughly 0
5 percent of our dataset:The git add 
 step can take a couple of minutes since a hash of all thefiles is computed
 Uploading all the files will also take a little bit of time
Since we will be able to use streaming later in the chapter, this step ishowever not lost time and will allow us to go significantly faster in the restof our experiments
And that’s it! Our two splits of the dataset as well as the full dataset are nowlive on the Hugging Face Hub at the following URLs:We should add README cards that explain how both datasets were createdand as much useful information as possible
 A well documented dataset ismore likely to be useful for other people as well as your future self
Modifying the README can also be done directly on the Hub
Now that our dataset is online, we can download it or stream examples fromit from anywhere with:We can see that we get the same examples as with the local dataset which isgreat
 That means we can now stream the dataset to any machine withInternet access without worrying about disk space
 Now that we have alarge dataset it is time to think about the model
 In the next section weexplore several options for the pretraining objective
A Tale of Pretraining ObjectivesNow that we have access to a large-scale pretraining corpus we can startthinking about how to pretrain a language model
 With such a largecodebase consisting of code snippets like the one shown in Figure 10-1, wecan tackle several tasks which influences the choice of pretrainingobjectives
 Let’s have a look at three common choices
Figure 10
 An example of a Python function that could be found in our dataset
Causal Language ModelingA natural task with textual data is to provide a model with the beginningof a code sample and ask it to generate possible completions
 This is aself-supervised training objective in which we can use the datasetwithout annotations and is often referred to as Causal LanguageModeling or Auto-regressive Modeling
 The probability of a givensequence of tokens is modeled as the successive probabilities of eachtokens given past tokens, and we train a model to learn this distributionand predict the most likely token to complete a code snippet
 Adownstream task directly related to such a self-supervised training taskis AI-powered code auto-completion
 A decoder-only architecture suchas the GPT family of models is usually best suited for this task as shownin Figure 10-2
Figure 10-2
 The future tokens are masked in Causal Language Modeling and the model needs topredict them
 Typically a decoder model such as GPT is used for such task
Masked Language ModelingA related but slightly different task is to provide a model with a noisycode sample (for instance with a code instruction replaced by a randomword) and ask it to reconstruct the original clean sample as illustrated inFigure 10-3
 This is also a self-supervised training objective andcommonly called Masked Language Modeling or Denoising Objective
It’s harder to think about a downstream task directly related todenoising, but denoising is generally a good pretraining task to learngeneral representations for later downstream tasks
 Many of the modelsthat we have used in the previous chapters (like BERT) were pretrainedwith such a denoising objective
 Training a masked language model ona large corpus can thus be combined with a second step of fine-tuningthe model on a downstream task with a limited number of labeledexamples
 We took this approach in the previous chapters and for codewe could use the text classification task for code-languagedetection/classification
 This is the mechanism underlying thepretraining procedure of encoder models such as BERT
Figure 10-3
 In Masked Language Modeling are some of the input tokens either masked or replacedand the model’s task is to predict the original tokens
 This is the architecture underlying the BERTbranch of transformer models
Sequence-to-Sequence TrainingAn alternative task is to use a heuristic like regular expressions toseparate comments or docstrings from code and build a large scaledataset of (code, comments) pairs that can be used as an annotateddataset
 The training task is then a supervised training objective inwhich one category (code or comment) is used as input for the modeland the other category (respectively comment or code) is used as labels
This is a case of supervised learning with (input, labels) pairs ashighlighted in Figure 10-4
 With a large, clean and diverse dataset aswell as a model with sufficient capacity we can try to train a model thatlearn to transcript comments in code or vice-versa
 A downstream taskdirectly related to this supervised training task is then “Documentationfrom code generation” or “Code from documentation generation”depending on how we set our input/outputs
 Both tasks are not of equaldifficulty, and in particular generating code from documentation mightseem a priori like a harder task to tackle
 In general, the closest the taskwill be to “pattern recognition/matching”, the most likely theperformances will be decent with the type of deep learning techniqueswe’ve explored in this book
 In this setting a sequence is translated intoanother sequence which is where encoder-decoder architectures usuallyshine
Figure 10-4
 Using heuristics the inputs can be split into comment/code pairs
 The model gets oneelement as input and needs to generate the other one
 A natural architecture for such a sequence-tosequence task is an encoder-decoder setup
You may recognize that these approaches reflect how some of the majormodels that we have seen and used in the previous chapters are trained:Generative pretrained models like the GPT family are trained usinga Causal Language Modeling objectiveDenoising models like the BERT family are trained using aMasked Language Modeling objectiveEncoder-decoder models like the T5, BART or PEGASUS modelsare trained using heuristics to create pairs of (inputs, labels)
 Theseheuristics can be for instance a corpus of pairs of sentences in twolanguages for a machine translation model, a heuristic way toidentify summaries in a large corpus for a summarization model orvarious ways to corrupt inputs with associated uncorrupted inputsas labels which is a more flexible way to perform denoising thanthe previous masked language modeling
Since we want to build a code auto-completion model we select the firsttype objective and choose a GPT architecture for the task
 Code autocompletion is the task of providing suggestions to complete lines orfunctions of codes during programming in order to make the experience ofa programmer significantly easier
 Code auto-completion can be particularlyuseful when programming in a new language or framework, or whenlearning to code
 It’s also useful to automatically produce repetitive code
Typical examples of such commercial products using AI models in mid2021 are GitHub Copilot, TabNine or Kite among others
 The first stepwhen training a model from scratch is to create a new tokenizer tailored forthe task
 In the next section we have a look at what it takes to build atokenizer from scratch
Building a TokenizerNow that we have gathered and loaded our large dataset, let’s see how wecan process it to feed and train our model
 As we’ve seen since Chapter 2,the first step will be to tokenize the dataset to prepare it in a format that ourmodel can ingest, namely numbers rather than strings
In the previous chapters we’ve used tokenizers that were already providedwith their accompanying models
 This made sense since our models werepretrained using data passed through a specific preprocessing pipeline that’sdefined in the tokenizer
 When using a pretrained model, it’s important tostick with the same preprocessing design choices selected for pretraining
Otherwise the model can be fed out-of-distribution patterns or unknowntokens
However, when we train the model from scratch on a new dataset, using atokenizer prepared for another dataset can be sub-optimal
 Let’s illustratewhat we mean by sub-optimal with a few examples:The T5 tokenizer was trained on a very large corpus of text calledthe Colossal Clean Crawled Corpus (C4), but an extensive step ofstop-word filtering was used to create it
 As a result the T5tokenizer has never seen common English words such as “sex”
The CamemBERT tokenizer was also trained on a very largecorpus of text, but only comprising French text (the French subsetof the OSCAR corpus)
 As such it is unaware of common Englishwords such “being”
We can easily test these features of each tokenizer in practice:In many cases, splitting such very short and common words in subparts willis inefficient since this will increase the input sequence length of the model
Therefore it’s important to be aware of the domain and filtering of thedataset which was used to train the tokenizer
 The tokenizer and model canencode bias from the dataset which has an impact on their downstreambehavior
 To create an optimal tokenizer for our dataset, we thus need totrain one ourselves
 Let’s see how this can be done
NOTETraining a model involves starting from a given set of weights and using (in today’smachine learning landscape) back-propagation from an error signal on a designedobjective to minimize the loss of the model and (hopefully) find an optimal set ofweights for the model to perform the task defined by the training objective
 Training atokenizer on the other hand does not involve back-propagation or weights
 It is a way tocreate an optimal mapping to go from a string of text to a list of integers that can beingested by the model on a given corpus
 In today’s tokenizers, the optimal string tointeger conversion involves a vocabulary consisting of a list of atomic strings and anassociated method to convert, normalize, cut, or map a text string into a list of indiceswith this vocabulary
 This list of indices is then the input for our neural network
The Tokenizer PipelineSo far we have treated the tokenizer as a single operation that transformsstrings to integers we can pass through the model
 This is not entirely trueand if we take a closer look at the tokenizer we can see that it is a fullprocessing pipeline that usually consists of four steps as shown inFigure 10-5
Figure 10-5
 A tokenization pipeline usually consists of four processing steps
Let’s take a closer look at each processing step and illustrate their effectwith the unbiased example sentence "Transformers are awesome!:NormalizationThis step corresponds to the set of operations you apply to a raw stringto make it less random or “cleaner”
 Common operations includestripping whitespace, removing accented characters or lower-casing allthe text
 If you’re familiar with Unicode normalization, it is also a verycommon normalization operation applied in most tokenizers
 Thereoften exist various ways to write the same abstract character
 It canmake two version of the “same” string (i
e
 with the same sequence ofabstract character) appear different
 Unicode normalization schemes likeNFC, NFD, NFKC, NFKD replace the various ways to write the samecharacter with standard forms
 Another example of normalization is“lower-casing” which is sometime used to reduce the size of thevocabulary necessary for the model of if the model is expected to onlyaccept and use lower cased characters
 After that normalization step, ourexample string could look like transformers are awesome!"
PretokenizationThis step splits a text into smaller objects that give an upper bound towhat your tokens will be at the end of training
 A good way to think ofthis is that the pretokenizer will split your text into “words” and then,your final tokens will be parts of those words
 For the languages whichallow this (English, German and many western languages), strings canbe split into words, typically along white spaces and punctuation
 Forexample, this step might transform our example into something like
 These wordsare then simpler to split into subwords with Byte-Pair Encoding (BPE)or Unigram algorithms in the next step of the pipeline
 However,splitting into “words” is not always a trivial and deterministic operationor even an operation which make sense
 For instance in languages likeChinese, Japanese or Korean, grouping symbols in semantic unit likeWestern words can be a non-deterministic operation with severalequally valid groups
 In this case, it might be best to not pretokenize thetext and instead use a language-specific library for pretokenization
Tokenizer modelOnce the input texts are normalized and pretokenized, the tokenizerapplies a subword splitting model on the words
 This is the part of thepipeline that needs to be trained on your corpus (or that has been trainedif you are using a pretrained tokenizer)
 The role of the model is to splitthe “words” into subwords to reduce the size of the vocabulary and tryto reduce the number of out-of-vocabulary tokens
 Several subwordtokenization algorithms exist including BPE, Unigram and WordPiece
For instance, our running example might look like trans,formers, are, awesome,  after the tokenizer model isapplied
 Note that at this point we no longer have a list of strings but alist of integers with the input IDs
 To keep the example illustrative wekeep the words but drop the string apostrophes to indicate thetransformation
Post-processingThis is the last step of the tokenization pipeline, in which someadditional transformations can be applied on the list of tokens, forinstance adding potential special tokens at the beginning or end of theinput sequence of token indices
 For example, a BERT-style tokenizerwould transform add a classification and seperator token
 This sequenceof integers can then be fed to the model
The tokenizer model is obviously the heart of the whole pipeline so let’s diga bit deeper to fully understand what is going on under the hood
The Tokenizer ModelThe part of the pipeline which can be trained is the “tokenizer model”
 Herewe must also be careful about not getting confused
 The “model” of thetokenizer is not a neural network model
 It’s a set of tokens and rules to gofrom the string to a list of indices
As we’ve discussed in Chapter 2, there are several subword tokenizationalgorithms such as BPE, WordPiece, and Unigram
BPE starts from a list of basic units (single characters) and creates avocabulary by a process of progressively creating new tokens that consist ofthe merge of the most frequently co-occurring basic units and adding themto the vocabulary
 This process of progressively merging the vocabularypieces most frequently seen together is re-iterated until a predefinedvocabulary size is reached
Unigram starts from the other end by initializing its base vocabulary with alarge number of tokens (all the words in the corpus and potential subwordsbuild from them) and progressively removing or splitting the less usefultokens (mathematically the symbol which contributes least to the loglikelihood of the training corpus) to obtain a smaller and smaller vocabularyuntil the target vocabulary size is reached
The difference between these various algorithms and their impact ondownstream performance varies depending on the task and overall it’s quitedifficult to identify if one algorithm is clearly superior to the others
 BothBPE and Unigram have reasonable performance in most cases
 WordPieceis a predecessor of Unigram, and it’s official implementation was neveropen-sourced by Google
Measuring Tokenizer PerformanceThe optimality and performance of a tokenizer are also quite difficult tomeasure in practice
 Some ways to measure the optimality include:Subword Fertility which calculated the average number ofsubwords produced per tokenized word
Proportion of Continued Words which refers to the proportion ofwords in a corpus where the tokenized word is continued across atleast two sub-tokens
Coverage metrics like the proportion of unknown words or rarelyused tokens in a tokenized corpus
In addition to this, robustness to misspelling or noise is often estimated aswell as model performances on such out-of-domain examples as theystrongly depend on the tokenization process
These measures gives a set of different views on the tokenizer performancebut tend to ignore the interaction of the tokenizer with the model(e
g
 subword fertility is minimized by including all the possible words inthe vocabulary but this will produce an very large vocabulary for themodel)
In the end, the performance of the various tokenization approaches are thusgenerally best estimated by using the downstream performance of themodel as the ultimate metric
 For instance the good performance of earlyBPE approaches were demonstrated by showing improved performance onmachine-translation of the trained models using these tokenization andvocabularies instead of character or word based tokenization
WARNINGThe terms “tokenizer” and “tokenization” are overloaded terms and can mean differentthings in different fields
 For instance in linguistics, tokenization is sometimesconsidered the process of demarcating and possibly classifying sections of a string ofinput characters according to linguistically meaningful classes like nouns, verbs,adjectives, or punctuation
 In this book, the tokenizer and tokenization process is notparticularly aligned with linguistic units but is computed in a statistical way from thecharacter statistics of the corpus to group most likely or most often co-ocurring symbols
Let’s see how we can build our own tokenizer optimized for Python code
A Tokenization Pipeline for PythonNow that we have seen the workings of a tokenizer in details let’s startbuilding one for our use-case: tokenizing Python code
 Here the question ofpretokenization merits some discussion for programming languages
 If wesplit on white spaces and remove them we will lose all the indentationinformation in Python which is important for the semantics of the program
Just think about while loops and if-then-else statements
 On the other hand,line-breaks are not meaningful and can be added or removed without impacton the semantic
 Similarly, punctuation like an underscore is used tocreate a single variable name from several sub-parts and splitting onunderscore might not make as much sense as it would in natural language
Using a natural language pretokenizer for tokenizing code thus seemspotentially suboptimal
One way to solve this issue could be to use a pretokenizer specificallydesigned for Python, like the built-in tokenize module:We see that the tokenizer split our code string in meaningful units (codeoperation, comments, indent and dedent, etc)
 One issue with using thisapproach is that this pretokenizer is Python-based and as such typicallyrather slow and limited by the Python GIL
 On the other hand, most of thetokenizers in Transformers are provided by the Tokenizers library which arecoded in Rust
 The Rust tokenizers are many orders of magnitude faster totrain and to use and we would thus likely want to use them given the size ofour corpus
Let’s see what tokenizer could be interesting to use for us in the collectionprovided on the hub
 We want a tokenizer which preserves the spaces
 Agood candidate could be a byte-level tokenizer like the tokenizer of GPT-2
Let’s load this tokenizer and explore its tokenization properties:This is quite a strange output, let’s try to understand what is happening hereby running the various sub-modules of the pipeline that we’ve just seen
Let’s see what normalization is applied in this tokenizer:print(tokenizer
backend_tokenizer
normalizer)NoneThis tokenizer uses no normalization
 This tokenizer is working directly onthe raw Unicode inputs without cleaning/normalization steps
 Let’s take alook at the pretokenization:This output is a little strange
 What are all these symbols and what are thenumbers accompanying the tokens? Let’s explain both and understandbetter how this tokenizer work
Let’s start with the numbers
 The Tokenizers library has a very usefulfeature that we have discussed a little in previous chapters: offset tracking
All the operation on the input string are tracked so that it’s possible to knowexactly what part of the input string an output token correspond to
 Thesenumbers simply indicate where in the original string each token comesfrom
 For instance the word 'hello' corresponds to the characters 8 to 13in the original string
 If some characters were removed in a normalizationstep we would still be able to associate each token with the respective partin the original string
The other curious feature of the tokenized text are the odd lookingcharacters such as
 Byte-level means that our tokenizer works onbytes instead of Unicode characters
 Each Unicode character is composedof between 1 to 4 bytes depending on the Unicode character
 The nice thingabout bytes is that, while there exists 143,859 Unicode characters in all theUnicode alphabet, there are only 256 elements in the bytes’ alphabets andyou can express each Unicode character as a sequence of 1 to 4 of thesebytes
 If we work on bytes we can thus express all the strings in the UTF-8world as longer strings in this alphabet of 256 values
 Basically we couldthus have a model using an alphabet of only 256 words and be able toprocess any Unicode string
 Let’s have a look at what the byterepresentations of some characters look like:At this point you might wonder: why is it interesting to work on byte-level?Let’s investigate the various options we have to define a vocabulary for ourmodel and tokenizers
We could decide to build our vocabulary from the 143,859 Unicodecharacters and add to this base vocabulary frequent combinations of thesecharacters, also known as words and subwords
 But having a vocabulary ofover 140,000 words will be too much for a deep learning model
 We willneed to model each Unicode characters with one embedding vector andsome of these characters are very rarely seen and will be really hard tolearn
 Note also that this number of 140,000 will be a lower bound on thesize of our vocabulary since we would like to have also words,i
e
 combination of Unicode characters in our vocabulary!On the other extreme, if we only use the 256 byte values as our vocabulary,the input sequences will be segmented in many small pieces (each byteconstituting the Unicode characters) and as such our model will have towork on long inputs and spend a significant compute power onreconstructing Unicode characters from their separate bytes and then wordsfrom these characters
 See the paper accompanying the byteT5 modelrelease for a detailed study of this overhead5
A middle ground solution is to construct a medium size vocabulary byextending the 256 words vocabulary with the most common combination ofbytes
 This is the approach taken by the BPE algorithm
 The idea is toprogressively construct a vocabulary of a pre-defined size by creating newvocabulary tokens through iteratively merging the most frequently cooccurring pair of tokens in the vocabulary
 For instance, if t and h occurvery frequently together like in English, we’ll add a token th in thevocabulary to model this pair of tokens instead of keeping them separated
Starting from a basic vocabulary of elementary units (typically thecharacters or the 256 byte values in our case) we can thus model any stringefficiently
WARNINGBe careful not to confuse the “byte” in “Byte-Pair Encoding” with the “byte” in “ByteLevel”
 The name “Byte-Pair Encoding” comes from a data compression techniqueproposed by Philip Gage in 1994 and originally operating on bytes6
 Unlike its namemight indicate, standard BPE algorithms in NLP typically operate on Unicode stringsrather than bytes though, the byte-level byte-pair encoding is a new type of byte-pairencoding specifically working on bytes
 If we read our Unicode string in bytes we canthus reuse a simple byte-pair encoding subword splitting algorithm
There is just one issue to be able to use a typical BPE algorithm in NLP
 Aswe just mentioned these algorithm are usually designed to work with clean“Unicode string” as inputs and not bytes and usually expect regular ASCIIcharacters in the inputs and for instance no spaces or control characters
 Butin the Unicode character corresponding to the 256 first bytes there are manycontrol characters (newlines, tabs, escape, line feed and other non-printablecharacters)
 To overcome this problem, the GPT-2 tokenizer first maps allthe 256 inputs bytes to Unicode strings which can easily be digested by thestandard BPE algorithms, i
e
 we will map our 256 elementary values toUnicode strings which all correspond to standard printable Unicodecharacters
It’s not very important that these Unicode characters are encoded with 1byte or more, what is important is to have 256 single values at the end,forming our base vocabulary, and that these 256 values are correctlyhandled by our BPE algorithm
 Let’s see some examples of this mapping onthe GPT-2 tokenizer
 We can access the 256 values mapping as follow:And we can take a look at some common values of bytes and associatedmapped Unicode characters:All simple byte values corresponding to regular caracters likeWe could have used more explicit conversion like mapping newlines to aNEWLINE string but BPE algorithm are typically designed to word oncharacter so keep the equivalence of 1 Unicode character for each bytecharacter is easier to handle with an out-of-the-box BPE algorithm
 Nowthat we have been introduced to the dark magic of Unicode encodings, wecan understand our tokenization conversion a bit better:We can recognize here the newlines (that are mapped to  as we now know)and the spaces (mapped to)
 We also see that:Spaces, and in particular consecutive spaces, are conserved (forinstance the three spaces in),Consecutive spaces are considered as a single word,Each space preceding a word is attached and considered as beingpart of the subsequent wordNow that we’ve understood the preprocessing steps of our tokenizer, let’sexperiment with the byte-pair encoding model
 As we’ve mentioned, theBPE model is in charge of splitting the words in sub-units until all sub-unitsbelongs to the predefined vocabulary
The vocabulary of our GPT-2 tokenizer comprise 50257 words:the base vocabulary with the 256 values of the bytes50,000 additional tokens created by repeatedly merging the mostcommonly co-occurring tokensa special character added to the vocabulary to represent documentboundariesWe can easily check that by looking at the length attribute of the tokenizer:Running the full pipeline on our input code give us the following output:As we can see the BPE tokenizer keeps most of the words but will split themultiple spaces of our indentation into several consecutive spaces
 Thishappens because this tokenizer is not specifically trained on code andmostly on text where consecutive spaces are rare
 The BPE model thusdoesn’t include a specific token in the vocabulary for indentation
 This is acase of non-adaptation of the model to the corpus as we’ve discussed earlierand the solution, when the dataset is large enough, is to retrain the tokenizeron the target corpus
 So let’s get to it!Training a TokenizerLet’s retrain our byte-level BPE tokenizer on a slice of our corpus to get avocabulary better adapted to Python code
 Retraining a tokenizer providedin the Transformers library is very simple
 We just need to:Specify our target vocabulary size,Prepare an iterator to supply list of inputs string to process to trainthe tokenizer’s modelCall the train_new_from_iterator method
Unlike deep-learning models which are often expected to memorize a lot ofspecific details from the training corpus (often seen as common-sense orworld-knowledge), tokenizers are really just trained to extract the mainstatistics of a corpus and not focus on details
 In a nutshell, the tokenizer isjust trained to know which letter combinations are the most frequent in ourcorpus
You thus don’t always need to train your tokenizer on a very large corpusbut mostly on a corpus well representative of your domain and from whichthe model can extract statistically significant measures
 Depending on thevocabulary size and the corpus, the tokenizer can end up memorizing wordsthat would not expected
 For instance let’s have a look at the last words andthe longest words in our GPT-2 tokenizer:The first token <|endoftext|> is the special token used to specify theend of a text sequence and was added after building the BPE vocabulary
For each of these words our model will have to learn an associated wordembedding and we probably don’t want the model to focus too muchrepresentational power on some of these noisy words
 Also note how somevery time- and space-specific knowledge of the world (e
g
 proper nounslike Hitman or Commission) are embedded at a very low level in ourmodeling approach by being granted separate tokens with associatedvectors in the vocabulary
 This type of very specific tokens in a BPEtokenizer can also be an indication that the target vocabulary size is toolarge or the corpus contain idiosyncratic tokens
Let’s train a fresh tokenizer on our corpus and examine its learnedvocabulary
 Since we just need a corpus reasonably representative of ourdataset statistics, let’s select about 1-2 GB of data, i
e
 about 100,000documents from our corpus:Let’s investigate the first and last words created by our BPE algorithm tosee how relevant is our vocabulary
In the first words created we can see various standard levels of indentationssummarized in the whitespace tokens as well as short common Python keywords like self, or, in
 This is a good sign that our BPE algorithm isworking as intended
In the last words we still see some relatively common words like or`inspect`8 as well as a set of more noisy words comin from the comments
We can also tokenize our simple example of Python code to see how ourtokenizer is behaving on a simple example:Even though they are not code keywords, it’s a little annoying to seecommon English words like World or say being split by our tokenizersince we expect them to occur rather frequently in the corpus
 Let’s check ifall the Python reserved keywords are in the vocabulary:We see that several quite frequent keywords like finally are not in thevocabulary as well
 Let’s try to build a larger vocabulary on a larger sampleof our dataset
 For instance a vocabulary of 32,768 words (multiple of 8 arebetter for some efficient GPU/TPU computations later with the model) andtrain it on a two times larger slices of our corpus with 200,000 documents:We don’t expect the most frequent the most frequent tokens to change muchwhen adding more documents so let’s look at last tokens:A brief inspection doesn’t show any regular programming keyword in thelast created words
 Let’s try to tokenize on our sample code example withthe new larger tokenizer:Here also the indents are conveniently kept inside the vocabulary and wesee that common English words like Hello, World and say are alsoincluded as single tokens, which seems more in line with our expectationsof the data the model may see in the downstream task
 Let’s investigate thecommon Python keywords as we did before:We are still missing the nonlocal keyword but this keyword is also veryrarely used in practice as it make the syntax quite more complex
 Keeping itoutside of the vocabulary seems reasonable
 We have seen that verycommon Python keywords like def, in or for are very early in thevocabulary, indicating that they are very frequent as expected
 Obviously,the simplest solution if we wanted to use a smaller vocabulary would be toremove the code comments which account for a significant part of ourvocabulary
 However, in our case we’ll decide to keep them, assuming theymay help our model addressing the semantic of the task
After this manual inspection, our larger tokenizer seems well adapted forour task (remind that objectively evaluating the performance of a tokenizeris a challenging task in practice as we detailed before)
 We will thusproceed with it
After this manual inspection, our larger tokenizer seems well adapted forour task
 Remind that objectively evaluating the performance of a tokenizeris a challenging task in practice as we detailed before
 We will thus proceedwith this one and train a model to see how well it works in practice
NOTEYou can easily verify yourself that the new tokenizer is about 2x more efficient than thestandard tokenizer
 This means that the tokenizer uses half the tokens to encode a textthan the existing one which gives us twice the effective model context for free
 Whenwe thus train a new model with the new tokenizer on a context window of 1024 it isequivalent of training the same model with the old tokenizer on a context window of2048 with the advantage of being much faster and more memory efficient because of theattention scaling
Saving a Custom Tokenizer on the HubNow that our tokenizer is trained we need to save it
 The simplest way tosave it and be able to access it from anywhere later is to push it to theHugging Face Hub
 This will be especially useful when we later use aseparate training server
 Since this will be the first file we need to create anew model repository
To create a private model repository and save our tokenizer in it as a firstfile we can directly use the method push_to_hub of the tokenizer
 Sincewe already authenticated our account with huggingface-cli loginwe can simply push the tokenizer as follows:If you have your API token not stored in the cache, you can also pass itdirectly to the function with use_auth_token=your_token
 This willcreate a repository in your namespace name codeparrot, so anyone cannow load it by running:reloaded_tokenizer = AutoTokenizer
from_pretrained(model_ckpt)This tokenizer loaded from the Hub behaves exactly like our previouslytrained tokenizer:We can investigate its files and saved vocabulary on the Hub here atThis was a very deep dive into the inner workings of the tokenizer and howto train a tokenizer for a specific use-case
 Now that we can tokenize theinputs we can start building the model:Training a Model from ScratchNow is the part you have probably been waiting for: the model! To build aauto-complete function we need a rather efficient auto-regressive model sowe choose a GPT-2 style model
 In this section we initialize a fresh modelwithout pretrained weights, setup a data loading class and finally create ascalable training loop
 In the grand finale we then train a GPT-2 largemodel! Let’s start by initializing the model we want to train
This is the 1
5B parameters model! Pretty big capacity but we also have apretty large dataset with 180GB of data
 In general, large language modelsare more efficient to train so as long as our dataset is reasonably large wecan definitely use a large language model
Let’s push the newly initialized model on the Hub by adding it to ourtokenizer folder
 Our model is large so we will need to activate git+lfs in it
Let’s go to our burgeoning model repository that currently only thetokenizer files in it and activate git-lfs to track large files easily:Pushing the model to the Hub may take a minute given the size of thecheckpoint Since this model is quite large, let’s create a smallerversion for debugging and testing purposes
 We will take the standard GPT2size as base:Now that we have a model we can train we need to make sure we can feedit the input data efficiently during training
Data LoaderTo be able to train with maximal efficiency, we will want to supply ourmodel with full sequences as much as possible
 Let’s say our context lengthi
e
 the input to our model is 1024 tokens, we always want to provide 1024tokens sequences to our models
 But some of our code examples might beshorter than 1024 tokens and some might be longer
The simplest solution is to have a buffer and fill it with examples until wereach 1024 tokens
 We can build this by wrapping our streaming dataset inan iterable which takes care of tokenizing on-the-fly and makes sure toprovide constant length tokenized sequences as inputs to the model
 This ispretty easy to do with the tokenizers of the transformers library
 Tounderstand how we can do it, let’s request one element in our streamingdataset:We will now tokenize this example and explicitly ask the tokenizer to 
truncate the output to a specified maximum block length, and 
 return boththe overflowing tokens and the lengths of the tokenized elementsWe can specify this behavior when we call the tokenizer with a text totokenize:We can see that the tokenizer returns a batch for our input where our initialraw string has been tokenized and split in sequences of maxmax_length=10 tokens
 The dictionary item length provides us withthe length of each sequences
 The last element of the length sequencecontains the remaining tokens and is smaller than the sequence_lengthif the number of tokens of the tokenized string is not a multiple ofsequence_length
 The overflow_to_sample_mapping can beused to identify which segment belongs to which input string if a batch ofinputs is provided
To feed batches with full sequences of sequence_length to our model,we should thus either drop the last incomplete sequence or pad it but thiswill render our training slightly less efficient and force us to take care ofpadding and masking padded token labels
 We are much more compute thandata constrained and will thus go the easy and efficient way here
 We canuse a little trick to make sure we don’t loose too many trailing segments
We can concatenate several examples before passing them to the tokenizer,separated by the special  token and reach a minimumcharacter length before sending the string to the tokenizer
Let’s first estimate the average character length per tokens in our dataset:We can for instance make sure we have roughly 100 full sequences in ourtokenized examples by defining our input string character length as:per output token that we just estimated: 3
6If we input a string with input_characters characters we will thus geton average number_of_sequences output sequences and we can justeasily calculate how much input data we are losing by dropping the lastsequence
 If number_of_sequences is equal to 100 it means that weare ok to lose at most one percent of our dataset which is definitelyacceptable in our case with a very large dataset
 At the same time thisensures that we don’t introduce a bias by cutting off the majority of fileendings
Using this approach we can have a constraint on the maximal number ofbatches we will loose during the training
 We now have all we need tocreate our IterableDataset, which is a helper class provided bytorch, for preparing constant length inputs for the model
 We just need toinherit from IterableDataset and setup the __iter__ function thatyields the next element with the logic we just walked through:While the standard practice in the transformers library that we have seen upto now is to use both input_ids and attention_mask, in thistraining, we have taken care of only providing sequences of the same,maximal, length (sequence_length tokens)
 The attention_maskinput is usually used to indicate which input token is used when inputs arepadded to a maximal length but are thus superfluous here
 To furthersimplify the training we don’t output them
 Let’s test our iterable dataset:Nice, this is working as we intended and we get nice constant length inputsfor the model
 Note that we added a shuffle operation to the dataset
 Sincethis is a iterable dataset we can’t just shuffle the whole dataset at thebeginning
 Instead we setup a buffer with size buffer_size and loadshuffle the elements in this buffer before we get elements from the dataset
Now that we have a reliable input source for the model it is time to buildthe actual training loop
Training Loop with AccelerateWe now have all the elements to write our training loop
 One obviouslimitations of training our own language model is the memory limits on theGPUs we will use
 In this example we will use several A100 GPUs whichhave the benefits of a large memory on each card, but you will probablyneed to adapt the model size depending on the GPUs you use
 However,even on a modern graphics card you can’t train a full scale GPT-2 model ona single card in reasonable time
 In this tutorial we will implement dataparallelism which will help utilize several GPUs for training
 We won’ttouch on model-parallelism, which allows you to distribute the model overseveral GPUs
 Fortunately, we can use a nifty library called Accelerate tomake our code scalable
 The Accelerate is a library designed to makedistributed training and changing the underlying hardware for training easy
Accelerate provides an easy API to make training scripts run with mixedprecision and on any kind of distributed setting (multi-GPUs, TPUs etc
)while still letting you write your own training loop
 The same code can thenruns seamlessly on your local machine for debugging or your trainingenvironment
 Accelerate also provides a CLI tool that allows you to quicklyconfigure and test your training environment then launch the scripts
 Theidea of accelerate is to provide a wrapper with the minimal amount ofmodifications allowing for your code to run on distributed, multi-GPUs,mixed-precision and novel accelerators like TPUs
 You only need to make ahandful of changes to your native PyTorch training loop:Next we setup logging for training
 Since training a model from scratch thetraining run will take a while and run on expensive infrastructure so wewant to make sure that all relevant information is stored and easilyaccessible
 We don’t show the full logging setup here but you can find thesetup_logging function in the full training script
 It sets up three levelsof logging: a standrad python Logger, Tensorboard, and Weights &Biases
 Depending on your preferences and use-case you can add or removelogging frameworks here
 It returns the Python logger, a Tensorboard writeras well as a name for the run generated by the Weights & Biases logger
In addition we setup a function to log the metrics with Tensorboard andWeights & Biases
 Note the use of accelerator
is_main_processAt the end we wrap the dataset in a DataLoader which also handles thebatching
 Accelerate will take care of distributing the dataloader on eachworker and make sure that each one receives different samples
 Anotheraspect we need to implement is optimization
 We will setup the optimizerand learning rate schedule in the main loop but we define a helper functionhere to differentiate the parameters that should receive weight decay
 Ingeneral biases and LayerNorm weights are not subject to weight decay
Finally, we want to evaluate the model on the validation set from time totime so let’s setup an evaluation function we can call which calculate theloss and perplexity on the evaluation set:Especially at the start of training when the loss is still high it can happenthat we get an overflow calculating the perplexity
 We catch this error andset the perplexity to infinity in these instances
 Now that we have all thesehelper functions in place we are now ready to write the main part of thescript:This is quite a code block but remember that we this is all the code youneed to train a large language model on a distributed infrastructure
 Let’sdeconstruct the script a little bit and highlight the most important parts:Model savingWe added the script to the model repository
 This allows us to simplyclone the model repository on a new machine and have everything toget started
 At the start we checkout a new branch with the run_namewe get from Weights & Biases but this could be any name for thisexperiment
 Later, we commit the model at each checkpoint to hub
With that setup each experiment is on a new branch and each commitrepresents a model checkpoint
 Note, that we need await_for_everyone and unwrap_model to make sure the modelis properly synchronized when we store it
OptimizationFor the model optimization we use the AdamW optimizer with a cosinelearning rate schedule after a linear warming up period
 For thehyperparameters we closely follow the parameters described in theGPT-3 paper9 for similar sized models
EvaluationWe evaluate the model on the evaluation set everytime we save that isevery save_checkpoint_steps and after training
 Along with thevalidation loss we also log the validation perplexity
Gradient accumulationSince we can not expect that a full batch size fits on the machine, evenwhen we run this on a large infrastructure
 Therefore, we implementgradient accumulation and can gather gradients from several backwardpasses and optimize once we have accumulated enough steps
One aspect that might still be a bit obscure at this point is what it actuallymeans to train a model on multiple GPUs? There are several approaches totrain models in a distributed fashion depending on the size of your modeland volume of data
 The approach utilized by Accelerate is called DataDistributed Parallelism (DDP)
 The main advantage of this approach is thatit allows you to train models faster with larger batch sizes that would fit intoany single GPU
 The process is illustrated in Figure 10-6
Figure 10-6
 Illustration of the processing steps in Data Distributed Parallelism with four GPUs
Let’s go through pipeline step by step on:1
 Each worker consists of a GPU and a fraction of the CPU cores
The data loader prepares a batch of data and sends it to the model
 
Each GPU receives a batch of data and calculates the loss andrespective gradients with a local copy of the model
2
 All gradients from each node are averaged with a reduce patternand the averaged gradients are sent back to each note
3
 The gradients are applied with the optimizer on each nodeindividually
 Although this might seem like redundant work itavoids sending copies of the models around between the nodes
 Weneed to update the models at least once anyway and for that timethe other nodes would need to wait until they receive the updatedversion
4
 Once all models are updated we go back to step one and repeatuntil we reached the end of training
This simple pattern allows us to train large models extremely fast withoutmuch complicated logic
 Sometimes, however this is not enough if forexample the model does not fit on a single GPU in which case one needsanother strategy called Model Parallelism
10Training RunNow let’s copy this training script in a file that we callcodeparrot_training
py such that we can execute it on our trainingserver
 To make life even easier we can add it to the model repository on thehub
 Remember that the models on the hub are essentially git repository sowe can just clone the repository, add any files we want and then push themback to the hub
 One the remote training server you can then spin-uptraining with the following commands:And that’s it! Note that wandb login will prompt you to authenticatewith Weights & Biases for logging and the accelerate configcommand will guide you through setting up the infrastructure
 There youcan define on what infrastructure you want to train on, if you want to enablemixed precision training among other settings
 After that setup you can runthe script locally on a CPU, on a single GPU or in the cloud on a distributedGPU infrastructure and even TPUs
 We used a a2-megagpu-16ginstance for all experiments which is a workstation with 16 x A100 GPUswith 40GB of memory each
 You can see the settings used for thisexperiment in Table 10-1
Running the training script with these settings on that infrastructure takesabout 24h
 If you train your own custom model make sure your code runssmoothly on smaller infrastructure in order to make sure that expensivelong run goes smoothly
 After the run successfully completes and you canmerge the experiment branch on the hub back into master with thefollowing commands:Naturally, experiment-branch should be the name of the experimentbranch on the hub you would like to merge
 Now that we have a trainedmodel let’s have a look at how we can investigate it’s performance
Model AnalysisSo what can we do with our freshly baked language straight out of the GPUofen? Well, we can use it to write some code for us
 There are two types ofanalysis we can conduct: qualitative and quantitative
 In the former we canlook at concrete examples and try to better understand in which cases themodel succeeds and where it fails
 In the latter case we evaluate the modelon a large set of test cases and evaluate it’s performance statistically
 In thissection we have a look at how we can use the model and have a look at afew examples and then discuss how we can evaluate the modelsystematically and more robustly
 First, let’s wrap the model in a pipelineand use it to continue some code inputs:We will use the generation pipeline to generate candidate completions givena prompt
 In order to keep the generations brief we just show the code ofone block of code by splitting off any new function or class definition aswell as comments on new lines
Let’s start with a simple example and have the model write a function for usthat converts hours to minutes:That seems work pretty well
 The function rounds to even hours but thefunction definition was also ambiguous
 Let’s have a look at anotherfunction:Although the mathematical expression go in the right direction it is clearthat the model does not know how to calculate the area of a rectangle
 Let’stest it’s knowledge on a class for complex numbers:That looks pretty good! The model correctly setup a class which contains tocomponents self
x and self
y and implements an addition operatorthe correct way! Now can it also solve a more complex task of extractingURLs from an HTML string
 Let’s give the model a few attempts to get itright:Although it didn’t quite get it the first three times the last attempt looksabout right
 Finally, let’s see if we can use the model to translate a functionfrom pure Python to numpy:Also that seems to work as expected! Experimenting with a model andinvestigating it’s performance on a few samples can give us useful insightsto when it works well
 However, to properly evaluate it we need to run it onmany more examples and run some statistics
In Chapter 8 we explored a few metrics useful to measure the quality of textgenerations
 Among these metrics was for example BLEU which isfrequently used for that purpose
 While this metric has limitations ingeneral it is particularly bad suited for our use case with code generations
The BLEU score measures the overlap of n-grams between the referencetexts and the text generations
 When writing code we have a lot of freedomnaming things such as variables and classes and the success of a programdoes not depend on the naming scheme as long as it is consistent
 However,the BLEU score would punish a generation that deviates from the referencenaming which might in fact be almost impossible to predict, even for ahuman coder
In software development there are much better and more reliable ways tomeasure the quality of code such as unit tests
 With this approach all theOpenAI Codex models were evaluated by running several code generationsfor coding tasks through a set of unit tests and calculating the fraction ofgenerations that pass the tests11
 For a proper performance measure weshould apply the same evaluation regimen to our models but this is beyondthe scope of this chapter
ConclusionLet’s take a step back for a moment and contemplate what we haveachieved in this chapter
 We set out to create a code auto-complete functionfor Python
 First we built a custom, large scale dataset suitable forpretraining a large language model
 Then we created a custom tokenizerthat is able to efficiently encode Python code with that dataset
 Finally, withthe help of accelerate we put everything together and wrote a training scriptto train a large GPT-2 model from scratch on a multi-GPU infrastructurewith as few as 200 lines of code
 Investigating the model output we saw thatit can generate reasonable code continuations and discussed how the modelcould be systematically evaluated
You now not only now how to fine-tune any of the many pretrained modelson the hub but also how to pretrain a custom model from scratch when youhave enough data and compute available
 You are now setup to tacklealmost any NLP models with transformers
 So the question is where tonext? In the next and last chapter we have a look at where the field iscurrently moving and what new exciting applications and domains beyondNLP transformer models can tackle
