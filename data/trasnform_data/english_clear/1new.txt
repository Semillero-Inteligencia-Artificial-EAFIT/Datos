Chapter 1
 Hello TransformersA NOTE FOR EARLY RELEASE READERSWith Early Release ebooks, you get books in their earliest form—the author’s raw andunedited content as they write—so you can take advantage of these technologies longbefore the official release of these titles
This will be the 1st chapter of the final book
 Please note that the GitHub repo will be madeactive later on
If you have comments about how we might improve the content and/or examples in thisbook, or if you notice missing material within this chapter, please reach out to the editor atmpotter@oreilly
com
Since their introduction in 2017, transformers have become the de facto standard for tackling awide range of natural language processing (NLP) tasks in both academia and industry
 Withoutnoticing it, you probably interacted with a transformer today: Google now uses BERT toenhance its search engine by better understanding users’ search queries
 Similarly, the GPTfamily of transformers from OpenAI have repeatedly made headlines in mainstream media fortheir ability to generate human-like text and images
 These transformers now powerapplications like GitHub’s Copilot, which as shown in Figure 1-1 can convert a comment intosource code that automatically trains a neural network for you!Figure 1-1
 An example from GitHub Copilot where given a brief description of the task, the application provides a suggestionfor the entire function (shown in gray), complete with helpful comments
So what is it about transformers that changed the field almost overnight? Like many greatscientific breakthroughs, it was the culmination of several ideas like attention, transferlearning, and scaling up neural networks that were percolating in the research community atthe time
But a fancy new method by itself is not enough to gain traction in industry - it also calls fortools to make it accessible
 The Hugging Face Transformers library and its surroundingecosystem answered that call by helping practitioners easily use, train, and share models, whichgreatly accelerated the adoption of transformers in industry
 The library is nowadays used byover 1,000 companies to run transformers in production, and throughout this book we’ll guideyou on how to train and optimize these models for practical applications
In this chapter we’ll introduce the core concepts that underlie the pervasiveness oftransformers, take a tour of some of the tasks that they excel at, and conclude with a look at theHugging Face ecosystem of tools and libraries
 Let’s start our transformer journey with a briefhistorical overview
The Transformers Origin StoryIn 2017 researchers at Google published a paper1 which proposed a novel neural networkarchitecture for sequence modeling
 Dubbed the Transformer, this architecture outperformedrecurrent neural networks (RNNs) on machine translation tasks, both in terms of translationquality and training cost
In parallel, an effective transfer learning method called ULMFiT2 showed that pretrainingLong-Short Term Memory (LSTM) networks with a language modeling objective on a verylarge and diverse corpus, and then fine-tuning on a target task could produce robust textclassifiers with little labeled data
These advances were the catalysts for two of the most well-known transformers today: GPTand BERT
 By combining the Transformer architecture with language model pretraining, thesemodels removed the need to train task-specific architectures from scratch and broke almostevery benchmark in NLP by a significant margin
 Since the release of GPT and BERT, averitable zoo of transformer models has emerged and a timeline of the recent events is shown inFigure 1-2
Figure 1-2
 The transformers timeline
But we are getting ahead of ourselves
 To understand what is novel about this approachcombining very large datasets and a novel architecture:The encoder-decoder frameworkAttention mechanismsTransfer learningLet’s start by looking at the encoder-decoder framework and the architectures which precededthe rise of transformers
NOTEIn this book we’ll use the proper noun “Transformer” (singular and with a capitalized first letter) to refer to theoriginal neural network architecture that was introduced in the now-famous Attention is All You Need paper
 Forthe general class of Transformer-based models, we’ll use “transformers” (plural and with a uncapitalized firstletter) and for the Hugging Face library we’ll use the shorthand “Transformers” (plural and with a capitalized firstletter)
 Hopefully this is not too confusing!The Encoder-Decoder FrameworkPrior to transformers, LSTMs were the state-of-the-art in NLP
 These architectures contain acycle or feedback loop in the network connections that allows information to propagate fromone step to another, making them ideal for modeling sequential data like language
 Asillustrated in the left image of Figure 1-3, an RNN receives some input x (which could be aword or character), feeds it through the network A and outputs a value called h called thehidden state
 At the same time, the model feeds some information back to itself through thefeedback loop which it can then use in the next step with input x 
 This can be more clearlyseen if we “unroll” the loop as shown on the right side of Figure 1-3, where at each step theRNN passes information about its state to the next operation in the sequence
 This allows anRNN to keep track of information from previous steps, and use it for its output predictions
TODO: Redraw or reference Chris OlahThese architectures were (and continue to be) widely used for tasks in NLP, speech processing,and time series, and you can find a wonderful exposition of their capabilities in AndrejKarpathy’s blog post, The Unreasonable Effectiveness of Recurrent Neural Networks
One area where RNNs played an important role was in the development of machine translationsystems, where the objective is to map a sequence of words in one language to another
 Thiskind of task is usually tackled with an encoder-decoder or sequence-to-sequence architecture,3which is well suited for situations where the input and output are both sequences of arbitrarylength
 As the name suggests, the job of the encoder is to encode the information from the inputsequence into a numerical representation that is often called the last hidden state
 This state isthen passed to the decoder, which generates the output sequence
In general, the encoder and decoder components can be any kind of neural network architecturethat is suited for modeling sequences, and this process is illustrated for a pair of RNNs inFigure 1-4, where the English sentence “Transformers are great!” is converted to a hidden statevector that is then decoded to produce the German translation “Transformer sind grossartig!”
In this figure, the shaded boxes represent the unrolled RNN cells where the vertical lines arethe feedback loops
 The tokens are fed sequentially through the model and the output tokens arelikewise created sequentially from top to bottom
Although elegant in its simplicity, one weakness with this architecture is that the final hiddenstate of the encoder creates an information bottleneck: it has to capture the meaning of thewhole input sequence because this is all the decoder has access to when generating the output
This is especially challenging for long sequences where information at the start of the sequencemight be lost in the process of creating a single, fixed representation
Fortunately, there is a way out of this bottleneck by allowing the decoder to have access to allof the encoder’s hidden states
 The general mechanism for this is called attention4 and is a keycomponent in many modern neural network architectures
 Understanding how attention wasdeveloped for RNNs will put us in good stead to understand one of the main building blocks ofthe Transformer; let’s take a deeper look
Attention MechanismsThe main idea behind attention is that instead of producing a single hidden state for the inputsequence, the encoder outputs a hidden state at each step which the decoder can access
However, using all states at the same time creates a huge input for the decoder, so somemechanism is needed to prioritize which states to use
 This is where attention comes in: it letsthe decoder assign a weight or “pay attention” to the specific states in the past (and the contextlength can be very long - several thousands words in the past for recent models like GPT orreformers) which are most relevant for producing the next element in the output sequence
 Thebest part is that this process is differentiable, so the process of “paying attention” can be learnedduring training!This process is illustrated in Figure 1-5 and produces much better translation results over thevanilla RNN encoder-decoder architecture
Figure 1-5
 Encoder-decoder architecture with an attention mechanism for a pair of RNNs
 The role of attention is shown forpredicting the third token in the output sequence
The Transformer architecture took this this idea several steps further and replaced the recurrentunits inside the encoder and decoder entirely with self-attention layers and simple feed-forwardnetworks! As illustrated in Figure 1-6, all the tokens are fed in parallel through the model andthe self-attention mechanism operates on all states of the same layer
 Compare this to the RNNcase in Figure 1-5, where the input tokens are fed sequentially through the model and attentionoperates between one decoder states and all the encoder states
 Moving from a sequentialprocessing to a fully parallel processing unlocked strong computational efficiency gainsallowing to train on orders of magnitude larger corpora for the same computational cost
 At thesame time, removing the sequential processing bottleneck of information makes the transformerarchitecture more efficient on several task that requires aggregating information over long timespans
Another desirable feature of self-attention is that it creates a representation for each token thatis dependent on its surrounding tokens
 This makes the representation of each token contextaware, such that the representation of the word “apple” (fruit) is different from “apple”(computer manufacturer)
 This feature is not novel about the transformer architecture andprevious architectures such as ELMo5 also used contextualized representations
 Updating thetoken representations with self-attention and feed-forward networks is then repeated acrossseveral layers or “blocks” to produce a rich encoding which is combined with the decoderinputs
 These layers are similar for the encoder and decoder part of the Transformerarchitecture and we will have a closer look at their inner workings in Chapter 3
Abandoning recurrence and replacing it with self-attention and feed-forward networks alsogreatly improves the computational efficiency of transformer models
 Research into the scalinglaws of deep learning models has revealed that larger models trained on more data in manycases yield better results
 The scalabality of transformers enables the full exploitation of thescaling laws which has started a scaling race of NLP models
 But scaling models comes at theprice of requiring large amounts of training data
 When working on practical applications ofNLP we usually do not have access to large amounts of textual data to train such large modelson
 A final piece was missing to get the transformer revolution started: transfer learning
Transfer Learning in NLPIt is common practice in computer vision to use transfer learning to train a convolutionalneural network like ResNet on one task and then adapt or fine-tune it on a new task, thusmaking use of the knowledge learned in the original task
 Architecturally, this usually works bysplitting the model in terms of a body and head, where the head is a task-specific network
During pretraining, the weights of the body learn broad features of the source domain, and it isthese weights which are used to initialize the new model for the new task
 Compared totraditional supervised learning, this approach typically produces high-quality models that canbe trained much more efficiently on a variety of downstream tasks, and with much less labeleddata
 A comparison of the two approaches is shown in Figure 1-7
In computer vision, the models are first trained on large-scale datasets such as ImageNet whichcontain millions of images
 This process is called pretraining and it’s main purpose is teach themodel the basic features of images, such as edges and filters
 These pretrained models can thenbe fine-tuned on a downstream task such as a X-ray classification with less than a thousandexamples, yet achieve a higher accuracy than training a model from scratch
Although transfer learning was the standard approach in computer vision for several years, itwas not clear what the analogous pretraining process was for NLP because language isinherently more varied and complex than the pixels of a 2D image
 As a result, NLPapplications typically required large amounts of labeled data to achieve high performance, andeven then that performance did not compare to what was achieved in the vision domain
In 2017 and 2018, several research works proposed new approaches that finally crackedtransfer learning for NLP, starting from the early proof of concept with strong performances ona sentiment classification task from unsupervised pretraining only in April 20176 before twostrongly impactful concurrent works were published in early 2018 showing significant gains oncommon benchmarks with unsupervized pretraining: ULMFiT7 and ELMo
As illustrated in Figure 1-8, ULMFiT involves three main steps:PretrainingThe initial training objective is quite simple: predict the next word based on the previouswords which is a task also referred to as language modeling
 The elegance of this approachlies in the fact that no labeled data is necessary and one can make use of abundantlyavailable text from sources such as Wikipedia
Domain adaptationOnce the language model is pretrained on a large-scale corpus, the next step is to adapt it onthe in-domain corpus, by fine-tuning the weights of the language model
Fine-tuningIn this step, the language model is fine-tuned with a classification layer for the target task(i
e
 classifying the sentiment of movie reviews in Figure 1-8)
 Fine-tuning takes orders ofmagnitude less time, compute, and labeled data as compared to training a classifier fromscratch, which made it a breakthrough for applied NLP
By introducing a viable framework for pretraining and transfer learning in NLP, ULMFiTprovided the missing piece to make transformers take off
 Soon after and closely followingeach other, GPT and BERT were released which combined self-attention and transfer learningbut with a slightly different take: GPT used only the transformer decoder and the samelanguage modeling approach from ULMFiT, while BERT used the encoder part with a specialform of language modeling called masked language modeling
 The objective of maskedlanguage modeling is to predict randomly masked tokens in a text similar to the gap texts fromschool
 GPT and BERT set a new state-of–the-art across a variety of NLP benchmarks andushered in the age of transformers
Yet another factor catalyzed the exponential impact of these models: easy availability in acommon codebase
 The research and development of these models had been lead by competingresearch labs using differing and incompatible frameworks (PyTorch, Tensorflow, etc) and as aconsequence each model’s codebase had it’s own API and idiosyncrasies, which created asubstantial barrier for practitioners to easily integrate these models in their own application
With the release of Hugging Face Transformers, a unified API across more than 50architectures and three interoperable frameworks (PyTorch, TensorFlow, and Jax) wasprogressively build which, at first catalyzed the research investigation in these models andquickly trickled down to NLP practitioners, making it easy to integrate these models in manyreal-life applications today
 Let’s have a look!Hugging Face Transformers: Bridging the GapApplying a novel machine learning architecture to a new task can be a complicatedundertaking, and usually involves the following steps:Implement the model architecture in code, typically in PyTorch or TensorFlow;Load the pretrained weights (if available) from a server;Preprocess the inputs, pass them through the model, and apply some task-specific postprocessing;Implement data loaders and define loss functions and optimizers to train the model
Each of these steps requires custom logic for each model and task
 Traditionally, research teamsthat publish a new model will often release some of the code (but not always!) along with themodel weights
 However, this code is rarely standardized and requires days of engineering toadapt to new use-cases
This is where the Transformers library came to the NLP practioner’s rescue: it provides astandardized interface to a wide range of transformer models as well as code and tools to adaptthese models to new use-cases
 The library integrates all major deep learning frameworks suchas TensorFlow, PyTorch, or JAX and allows you to switch between them
 In addition itprovides task-specific “heads” so you can easily fine-tune transformers on down-stream taskssuch as classification, named entity recognition, question-answering etc
 together with modulesto train them
 This reduces the time for a practitioner to train and test a handful of models froma week to a single afternoon!See for yourself in the next section where we show that with just a few lines of code, theTransformers library can be applied to tackle some of the most common NLP applications thatyou’re likely to encounter in the wild
A Tour of Transformer ApplicationsDepending on your application, this text could be a legal contract, a product description orsomething else entirely
 In the case of customer feedback you would probably like to knowwhether the feedback is positive or negative
 This task is called sentiment analysis and is partof the broader topic of text classification that we’ll explore in >
 For now, let’s have a look atwhat it takes to extract the sentiment from our piece of text using the Transformers library
Text ClassificationAs we’ll see in later chapters, Transformers has a layered API which allows you to interact withthe library at various levels of abstraction
 In this chapter we’ll start with the most high levelAPI pipelines, which abstract away all the steps needed to convert raw text into a set ofpredictions from a fine-tuned model
The first time you run this code you’ll see a few progress bars appear because the pipelineautomatically downloads the model weights from the Hugging Face Hub
 The second time youinstantiate the pipeline, the library will notice that you already downloaded the weights and willuse the cached version instead
 By default the sentiment-analysis pipeline uses a modelthat was fine-tuned on the Stanford Sentiment Treebank, which is an English corpus ofannotated movie reviews
Now that we have our pipeline, let’s generate some predictions! Each pipeline takes a string oftext (or a list of strings) as input and returns a list of predictions
 Each prediction is a Pythondictionary, so we can use Pandas to display them nicely as a DataFramelabelscoreNEGATIVEIn this case the model is very confident that the text has a negative sentiment, which makessense given that we’re dealing with complaint from an irate Autobot!Let’s now take a look at another common task called named entity recognition
Named Entity RecognitionPredicting the sentiment of customer feedback is a good first step, but you often want to knowif the feedback was about a particular product or service
 Names of products, places or peopleare called named entities and detecting and extracting them from text is called named entityrecognition (NER)
 We can apply NER by spinning up the corresponding pipeline and feedingour piece of text to itYou can see that the pipeline detected all the entities and also assigned a category such as ORG(organization), LOC (location) or PER (person) to them
 Here we used theaggregation_strategy argument to group the words according to the model’spredictions; for example “Optimus Prime” has two words but is assigned a single MISC(miscellaneous) category
 The scores tell us how confident the model was about the entity andwe can see that it was least confident about “Decepticons” and the first occurrence of“Megatron”, both of which it failed to group as a single entity
Extracting all the named entities is nice but sometimes we would like to ask more targetedquestions
 This is where we can use question answering
Question AnsweringIn question answering we provide the model with a passage of text called the context, alongwith a question whose answer we’d like to extract
 The model then returns the span of textcorresponding to the answer
 So let’s see what we get when we ask a specific question about thetext
We can see that along with the answer, the pipeline also returned start and end integerswhich correspond to the character indices where the answer span was found
 There are severalflavors of question answering that we will investigate later in Chapter 4, but this particular kindis called extractive question answering because the answer is extracted directly from the text
With this approach you can read and extract relevant information quickly from a customer’sfeedback
 But what if you get a mountain of long-winded complaints and you don’t have thetime to read them all? Let’s see if a summarization model can help!SummarizationThe goal of text summarization is to take a long text as input and generate a short version withall relevant facts
 This is a much more complicated task than the previous ones since it requires> Germany
 Unfortunately, when I opened the package, I discovered to my horror> that I had been sent an action figure of Megatron instead
This isn’t too bad! Although parts of the original text have been copy-pasted, the model wasable to correctly identify that “Bumblebee” (which appeared at the end) was the author of thecomplaint, and has captured the essence of the problem
 In this example you can also see thatwe passed some keyword arguments like max_length andclean_up_tokenization_spaces to the pipeline that allow us to tweak the outputs atruntime
 But what happens when you get a feedback that is in a language you don’tunderstand? You could use Google Translate or you can use your very own transformer totranslate it for you!TranslationLike summarization, translation is a task where the output consists of generated text
 Let’s usethe translation pipeline to translate the English text to German
Again, the model has produced a very good translation that correctly uses the formal pronounslike “Ihrem” and “Sie” in German! Here we’ve also shown how you can override the defaultmodel in the pipeline to pick the best one for your application, and you can find models forthousands of language pairs on the Hub
 Before we take a step back and look at the wholeHugging Face ecosystem
 let’s look at one last application with text generation
Text GenerationLet’s say you would like to write faster answers to customer feedback by having access to anautocomplete function
 With a text generation model you can continue an input text as follows:Okay, maybe we wouldn’t want to use this completion to calm Bumblebee down, but you getthe general idea
Now that we have seen a few cool applications of transformer models, you might be wonderingwhere the training happens? All of the models we used in this chapter were publicly availableand already fine-tuned for each task
 In general, however, you’ll want to fine-tune the modelson your own data and in the following chapters you will learn how to do just that
 But training amodel is just a small piece of any NLP project - being able to efficiently process data, shareresults with colleagues, and make your work reproducible are key components too
 Fortunately,the Transformers library is also surrounded by a big ecosystem of useful tools that supportmuch of the modern machine learning workflow
 Let’s have a look
The Hugging Face EcosystemWhat started with the Transformers library has quickly grown into a whole Hugging Faceecosystem consisting of many libraries and tools to accelerate your NLP and machine learningprojects
 In this section we’ll have a brief look at the various components
The Hugging Face ecosystem consists of mainly two parts: a family of libraries and the Hub asshown in Figure 1-9
 The libraries provide the code while the Hub provides the pretrainedweights, datasets, evaluation metrics, and more
 Let’s have a look at each component
 We’llskip the Transformers library as we’ve already discussed it and we will see a lot more of itthrough the course of the book
The Hugging Face HubAs outlined earlier, transfer learning is one of the key factors driving the success oftransformers because it allows to reuse pretrained models for new tasks
 Consequently, it iscrucial to be able to load pretrained models quickly and run experiments with it
On the Hugging Face Hub you can find over 10,000 models which are hosted and freelyavailable
 As shown in Figure 1-10 there are many attributes and filters for tasks, language, sizethat are designed to help you navigate and quickly find promising candidates
 As we’ve seenwith the pipelines, loading a promising model in your code is then literally just one line of codeaway
 This makes experimenting with a wide range of models lightweight and allows you tofocus on the domain-specific parts of your project
In addition to model weights, The Hub also hosts datasets and metrics which enables youreproduce published results or leverage additional data for your application
The Hub also provides model and dataset cards to document their contents and help you makean informed decision if the model or dataset is the right one for you
 One of the coolest featuresof the Hub is that you can try out any model directly through the various task-specific widgetsthat let you interact with the models as shown in Figure 1-11
Let’s continue the tour with the Hugging Face Tokenizers library
Hugging Face TokenizersBehind each of the pipeline examples we saw earlier was a tokenization step that splits the rawtext into smaller pieces called tokens
 We’ll see how this works in detail in Chapter 2, but fornow its enough to understand that a token may be a word, part of a word, or just characters likepunctuation
 Transformers are trained on numerical representations of these tokens, so gettingthis step right is pretty important for the whole NLP project!The Hugging Face Tokenizers library provides many tokenization strategies and is extremelyfast at tokenizing text thanks to its Rust backend
 In addition, it also takes care of all the modeldependent pre- and post-processing such as normalizing the inputs and transforming the modeloutputs to the required format
 With Tokenizers we can load a tokenizer in the same way wecan load pretrained model weights with Transformers
Naturally, we need a dataset and metrics to train and evaluate models so let’s have a look at theHugging Face Datasets library which is in charge of that aspect
Hugging Face DatasetsLoading, processing and storing datasets can be a cumbersome process, especially when thedatasets get too large to fit on your laptop’s RAM
 In addition, you usually need to implementvarious scripts to download the data and transform it into a standard format
The Hugging Face Datasets library simplifies this process by providing a standard interface forthousands of datasets that can found on the Hub
 It also provides smart caching (so you don’thave to redo your preprocessing each time you run your code) and avoids RAM limitations byleveraging a special mechanism called memory mapping that stores the contents of a file invirtual memory and enables multiple processes to modify a file more efficiently
 The library isalso interoperable with popular frameworks like Pandas and NumPy, so you don’t have to leavethe comfort of your favorite data-wrangling tools
Having a good dataset and powerful model is worthless, however, if you can’t reliably measurethe performance
 Unfortunately, classic NLP metrics come with many differentimplementations that can vary slightly and thus lead to deceptive results
 By providing thescripts for many metrics, the Datasets library helps make experiments more reproducible andthe results more trustworthy
With the Tokenizers, Transformers, and Datasets libraries we have everything we need to createan NLP project! However, once we start scaling from a single GPU to many or whentransitioning to TPUs we probably have to refactor a large fraction of the training logic
 That’swhere the last library of the ecosystems come into play: Hugging Face Accelerate
Hugging Face AccelerateA usual lifecycle of an ML project goes building a executable prototype on your local machineto training a small to mid sized model on a single GPU on a dedicated machine to training alarge model on a multi-GPU machine or a cluster and sometimes even transitioning to TPUtraining
 Each of these infrastructures requires some custom code to run smoothly andefficiently which causes a lot of adjustments when changing the infrastructure and just aheadache in general
 The Hugging Face Accelerate library adds a layer of abstraction to yournormal training loops that takes care of all the custom logic necessary for the traininginfrastructure
 This literally accelerates your workflow by simplifying the change ofinfrastructure when necessary
 We will encounter this library in ADD REF
Main Challenges With TransformersIn this chapter we’ve had a glimpse at the wide range of NLP tasks that can be tackled withtransformer models and reading the media headlines it can sometimes sound like theircapabilities are limitless
 However, despite their usefulness, transformers are far from being asilver bullet, and we list here a few challenges associated with them that we will explorethroughout the book:Language barrierTransformer research is dominated by the English language
 There are several models forother languages but it is harder to find pretrained models for rare or low-resourcelanguages
 In Chapter 6 we explore multilingual transformers and their ability to performzero-shot cross-lingual transfer
Data hungryAlthough we can use transfer learning to dramatically reduce the amount of labeled trainingdata, it is still a lot compared to human standards
 Tackling scenarios where you have littleto no labeled data is the subject of Chapter 7
Long documentsAttention works extremely well on paragraph-long texts, but becomes very expensive whenwe move to longer texts like whole documents
 Approaches to mitigate this are discussed inChapter 11
Black boxesAs with other deep learning models, transformers are to a large extent black boxes
 It ishard or impossible to unravel “why” a model made a certain prediction
 This is a especiallyhard challenge when these models are deployed to make critical decisions
BiasesTransformer models are predominately pretrained on text data from the Internet whichimprints all the biases that are present in the data into the models
 Making sure that theseare neither racist, sexist, or worse is a challenging task
 We discuss some of these issues inmore detail in ADD REF
Although daunting, many of these challenges can be overcome and we will touch again onthese topics in almost every chapter ahead
ConclusionHopefully, by now you are excited to learn how to train and integrate these versatile models inyour own applications! We’ve seen in this chapter that you can use state-of-the-art models forclassification, named entity recognition, question-answering and summarization models in afew lines of code, but this is really just the tip of the iceberg
In the following chapters you will learn how to adapt transformer for a wide range of use-cases,be it building a simple classifier, a lightweight model for production, and even training alanguage model from scratch
 We’ll be taking a hands-on approach, which means that for everyconcept there will be accompanying code that you can run on Google Colab or your own GPUmachine
Now that we’re armed with the basic concepts behind transformers, it’s time to get our handsdirty with our first application: text classification
