Chapter 4
 Question AnsweringA NOTE FOR EARLY RELEASE READERSWith Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as theywrite—so you can take advantage of these technologies long before the official release of these titles
This will be the 4th chapter of the final book
 Please note that the GitHub repo will be made active later on
If you have comments about how we might improve the content and/or examples in this book, or if you noticemissing material within this chapter, please reach out to the editor at mpotter@oreilly
com
Whether you’re a researcher, analyst, or data scientist, chances are that you’ve needed to wade through oceans ofdocuments to find the information you’re looking for
 To make matters worse, you’re constantly reminded byGoogle and Bing that there exist better ways to search! For instance, if we search for “When did Marie Curie winher first Nobel Prize?” on Google, we immediately get the correct answer of “1903” as illustrated in Figure 4-1
Figure 4-1
 A Google search query and corresponding answer snippet
In this example, Google first retrieved around 319,000 documents that were relevant to the query, and thenperformed an additional processing step to extract the answer snippet with the corresponding passage and webpage
 It is not hard to see why these answer snippets are useful
 For example, if we search for a trickier questionlike “Which country has the most COVID-19 cases?”, Google doesn’t provide an answer and instead we have toclick on one of the web pages returned by the search engine to find it ourselves
1The general approach behind this technology is called question answering (QA)
 There are many flavors of QA,but the most common one is extractive QA which involves questions whose answer can be identified as a span oftext in a document, where the document might be a web page, legal contract, or news article
 The two-stageprocess of first retrieving relevant documents and then extracting answers from them is also the basis for manymodern QA systems, including semantic search engines, intelligent assistants, and automated informationextractors
 In this chapter, we’ll apply this process to tackle a common problem facing e-commerce websites:helping consumers answer specific queries to evaluate a product
 We’ll see that customer reviews can be used as arich and challenging source of information for QA, and along the way we’ll learn how transformers act aspowerful reading comprehension models that can extract meaning from text
 Let’s begin by fleshing out the usecase
This chapter focuses on extractive QA, but other forms of QA may be more suitable for your use case
 For example, community QAinvolves gathering question-answer pairs that are generated by users on forums like Stack Overflow, and then using semantic similaritysearch to find the closest matching answer to a new question
 Remarkably, it is also possible to do QA over tables, and transformer modelslike TAPAS can even perform aggregations to produce the final answer! There is also long form QA, which aims to generate complexparagraph-length answers to open-ended questions like “Why is the sky blue?”
 You can find an interactive demo of long form QA on theHugging Face website
Building a Review-Based QA SystemIf you’ve ever purchased a product online, you probably relied on customer reviews to help inform your decision
These reviews can often help answer specific questions like “does this guitar come with a strap?” or “can I use thiscamera at night?” that may be hard to answer from the product description alone
 However, popular products canhave hundreds to thousands of reviews so it can be a major drag to find one that is relevant
 One alternative is topost your question on the community QA platforms provided by websites like Amazon, but it usually takes days toget an answer (if at all)
 Wouldn’t it be nice if we could get an immediate answer like the Google example fromFigure 4-1? Let’s see if we can do this using transformers!The DatasetTo build our QA system, we’ll use the SubjQA dataset 2 which consists of more than 10,000 customer reviews inEnglish about products and services in six domains: TripAdvisor, Restaurants, Movies, Books, Electronics, andGrocery
 As illustrated in Figure 4-2, each review is associated with a question that can be answered using one ormore sentences from the review
3Figure 4-2
 A question about a product and the corresponding review
 The answer span is underlined
The interesting aspect of this dataset is that most of the questions and answers are subjective, that is, they dependon the personal experience of the users
 The example in Figure 4-2 shows why this feature is potentially moredifficult than finding answers to factual questions like “What is the currency of the United Kingdom?”
 First, thequery is about “poor quality”, which is subjective and depends on the user’s definition of quality
 Second,important parts of the query do not appear in the review at all, which means it cannot be answered with shortcutslike keyword-search or paraphrasing the input question
 These features make SubjQA a realistic dataset tobenchmark our review-based QA models on, since user-generated content like that shown in Figure 4-2 resembleswhat we might encounter in the wild
NOTEQA systems are usually categorized by the domain of data that they have access to when responding to a query
 Closed-domain QA dealswith questions about a narrow topic (e
g
 a single product category), while open-domain deals with questions about almost anything (e
g
Amazon’s whole product catalogue)
 In general, closed-domain QA involves searching through fewer documents than the open-domaincase
For our use case, we’ll focus on building a QA system for the Electronics domain, so to get started let’s downloadthe dataset from the Hugging Face Hub:from datasets import load_datasetsubjqa = load_dataset("subjqa", "electronics")Next, let’s convert the dataset to the pandas format so that we can explore it a bit more easily:Number of questions in train: 1295Number of questions in test: 358Number of questions in validation: 255Notice that the dataset is relatively small, with only 1,908 examples in total
 This simulates a real-world scenario,since getting domain experts to label extractive QA datasets is labor-intensive and expensive
 For example, theCUAD dataset for extractive QA on legal contracts is estimated to have a value of $2 million to account for thelegal expertise and training of the annotators!There are quite a few columns in the SubjQA dataset, but the most interesting ones for building our QA system areshown in Table 4-1
The Amazon Standard Identification Number (ASIN) associated with each productanswers
answer_text The span of text in the review labeled by the annotatoranswers
answer_start The start character index of the answer spancontextThe customer reviewLet’s focus on these columns and take a look at a few of the training examples by using theDataFrame
sample function to select a random sample:qa_cols = ["title", "question", "answers
text","answers
answer_start", "context"]sample_df = dfs["train"][qa_cols]
sample(2, random_state=7)display_df(sample_df, index=False)I really like this keyboard
 I give it 4 stars because it doesn’thave a CAPS LOCK key so I never know if my caps are on
 Butfor the price, it really suffices as a wireless keyboard
 I have verylarge hands and this keyboard is compact, but I have nocomplaints
I bought this after the first spare gopro battery I bought wouldn’thold a charge
 I have very realistic expectations of this sort ofproduct, I am skeptical of amazing stories of charge time andbattery life but I do expect the batteries to hold a charge for acouple of weeks at least and for the charger to work like acharger
 In this I was not disappointed
 I am a river rafter andfound that the gopro burns through power in a hurry so thispurchase solved that issue
 the batteries held a charge, on shortertrips the extra two batteries were enough and on longer trips Icould use my friends JOOS Orange to recharge them
I just boughta newtrent xtreme powerpak and expect to be able to charge thesewith that so I will not run out of power again
From these examples we can make a few observations
 First, the questions are not grammatically correct, which isquite common in the FAQ sections of e-commerce websites
 Second, an empty answers
text entry denotesquestions whose answer cannot be found in the review
 Finally, we can use the start index and length of the answerspan to slice out the span of text in the review that corresponds to the answer:start_idx = sample_df["answers
answer_start"]
iloc[0][0]end_idx = start_idx + len(sample_df["answers
text"]
iloc[0][0])sample_df["context"]
iloc[0][start_idx:end_idx]'this keyboard is compact'Next, let’s get a feel for what types of questions are in the training set by counting the questions that begin with afew common starting words:We can see that questions beginning with “How”, “What”, and “Is” are the most common ones, so let’s have a lookat some examples:How is the camera?How do you like the control?How fast is the charger?What is direction?What is the quality of the construction of the bag?What is your impression of the product?Is this how zoom works?Is sound clear?Is it a wireless keyboard?To round out our exploratory analysis, let’s visualize the distribution of reviews associated with each product in thetraining setHere we see that most products have one review, while one has over fifty
 In practice, our labeled dataset would bea subset of a much larger, unlabeled corpus, so this distribution presumably reflects the limitations of theannotation procedure
 Now that we’ve explored our dataset a bit, let’s dive into understanding how transformerscan extract answers from text
Extracting Answers from TextThe first thing we’ll need for our QA system is to find a way to identify potential answers as a span of text in acustomer review
 For example, if a we have a question like “Is it waterproof?” and the review passage is “Thiswatch is waterproof at 30m depth”, then the model should output “waterproof at 30m”
 To do this we’ll need tounderstand how to:Frame the supervised learning problem
Tokenize and encode text for QA tasks
Deal with long passages that exceed a model’s maximum context size
Let’s start by taking a look at how to frame the problem
Span ClassificationThe most common way to extract answers from text is by framing the problem as a span classification task, wherethe start and end tokens of an answer span act as the labels that a model needs to predict
 This process is illustratedin Figure 4-3
Figure 4-3
 The span classification head for QA tasks
Since our training set is relatively small with only 1,295 examples, a good strategy is to start with a languagemodel that has already been fine-tuned on a large-scale QA dataset like the Stanford Question Answering Dataset(SQuAD)
4 In general, these models have strong reading comprehension capabilities and serve as a good baselineupon which to build a more accurate system
 You can find a list of extractive QA models by navigating to theHugging Face Hub and searching for “squad” under the Models tab
Figure 4-4
 A selection of extractive QA models on the Hugging Face Hub
As shown in Figure 4-4, there are more than 180 QA models to choose from, so which one should we pick?Although the answer depends on various factors like whether your corpus is mono- or multilingual, and theconstraints of running the model in a production environment, Table 4-2 collects a few models that provide a goodfoundation to build on
A distilled version of BERT-base that preserves 99% of the performance while beingtwice as fastRoBERTa-baseRoBERTa models have better performance than their BERT counterparts and can befine-tuned on most QA datasets using a single GPUALBERT-XXLState-of-the-art performance on SQuAD 2
0, but computationally intensive anddifficult to deployXLM-RoBERTalargeMultilingual model for 100 languages with strong zero-shot performanceTokenizing Text for QAFor the purposes of this chapter, we’ll use a fine-tuned MiniLM model5 since it is fast to train and will allow us toquickly iterate on the techniques that we’ll be exploring
 As usual, the first thing we need is a tokenizer to encodeour texts so let’s load the model checkpoint from the Hugging Face Hub as follows:from transformers import AutoTokenizermodel_ckpt = "deepset/minilm-uncased-squad2"tokenizer = AutoTokenizer
from_pretrained(model_ckpt)To see the model in action, let’s first try to extract an answer from a short passage of text
 In extractive QA tasks,the inputs are provided as (question, context) tuples, so we pass them both to the tokenizer as follows:Here we’ve returned torch
Tensor objects since we’ll need them to run the forward pass through the model
 Ifwe view the tokenized inputs as a table:we can also see the familiar input_ids and attention_mask tensors, while the token_type_ids tensorindicates which part of the inputs corresponds to the question and context (a 0 indicates a question token, a 1indicates a context token)
6 To understand how the tokenizer formats the inputs for QA tasks, let’s decode theinput_ids tensor:We see that for each QA example, the inputs take the format:[CLS] question tokens [SEP] context tokens [SEP]where the location of the first [SEP] token is determined by the token_type_ids
 Now that our text istokenized, we just need to instantiate the model with a QA head and run the inputs through the forward pass:As illustrated in Figure 4-3, the QA head corresponds to a linear layer that takes the hidden-states from theencoder7 and computes the logits for the start and end spans
 To convert the outputs into an answer span, we firstneed to get the logits for the start and end tokens:start_scores = outputs
start_logitsend_scores = outputs
end_logitsAs illustrated in Figure 4-5, each input token is given a score by the model, with larger, positive scorescorresponding to more likely candidates for the start and end tokens
 In this example we can see that the modelassigns the highest start token scores to the numbers “1” and “6000” which makes sense since our question isasking about a quantity
 Similarly, we see that the highest scored end tokens are “minute” and “hours”
Figure 4-5
 Predicted logits for the start and end tokens
 The token with the highest score is colored in orange
To get the final answer, we can compute the argmax over the start and end token scores and then slice the spanfrom the inputs
 The following code does these steps and decodes the result so we can print the resulting text:Question: How much music can this hold?Answer: 6000 hoursGreat, it worked! In Transformers, all of these pre-processing and post-processing steps are conveniently wrappedin a dedicated QuestionAnsweringPipeline
 We can instantiate the pipeline by passing our tokenizer andfine-tuned model as follows:In addition to the answer, the pipeline also returns the model’s probability estimate (obtained by taking a softmaxover the logits), which is handy when we want to compare multiple answers within a single context
 We’ve alsoshown that the model can predict multiple answers by specifying the topk parameter
 Sometimes, it is possible tohave questions for which no answer is possible, like the empty answers
answer_start examples in SubjQA
In these cases, the model will assign a high start and end score to the [CLS] token and the pipeline maps thisoutput to an empty string:NOTEIn our simple example, we obtained the start and end indices by taking the argmax of the corresponding logits
 However, this heuristic canproduce out-of-scope answers (e
g
 it can select tokens that belong to the question instead of the context), so in practice the pipelinecomputes the best combination of start and end indices subject to various constraints such as being in-scope, the start indices have toprecede end indices and so on
Dealing With Long PassagesOne subtlety faced by reading comprehension models is that the context often contains more tokens than themaximum sequence length of the model, which is usually a few hundred tokens at most
 As illustrated in Figure 46, a decent portion of the SubjQA training set contains question-context pairs that won’t fit within the model’scontext
Figure 4-6
 Distribution of tokens for each question-context pair in the SubjQA training set
For other tasks like text classification, we simply truncated long texts under the assumption that enoughinformation was contained in the embedding of the [CLS] token to generate accurate predictions
 For QAhowever, this strategy is problematic because the answer to a question could lie near the end of the context andwould be removed by truncation
 As illustrated in Figure 4-7, the standard way to deal with this is to apply asliding window across the inputs, where each window contains a passage of tokens that fit in the model’s context
Figure 4-7
 How the sliding window creates multiple question-context pairs for long documents
In Transformers, the sliding window is enabled by setting return_overflowing_tokens=True in thetokenizer, with the size of the sliding window controlled by the max_seq_length argument and the size of thestride controlled by doc_stride
 Let’s grab the first example from our training set and define a small window toillustrate how this works:example = dfs["train"]
iloc[0][["question", "context"]]tokenized_example = tokenizer(example["question"], example["context"],return_overflowing_tokens=True, max_length=100,stride=25)In this case we now get a list of input_ids, one for each window
 Let’s check the number of tokens we have ineach window:Finally we can see where two windows overlap by decoding the inputs:[CLS] how is the bass? [SEP] and don't feel heavy or bear down on your ears even> after listening to music with them on all day
 the sound is night and day> better than any ear - bud could be and are almost as good as the pro 4aa
> they are " open air " headphones so you cannot match the bass to the sealed> types, but it comes close
 for $ 32, you cannot go wrong
 [SEP]Now that we have some intuition about how QA models can extract answers from text, let’s look at the othercomponents we need to build an end-to-end QA pipeline
THE STANFORD QUESTION ANSWERING DATASETThe (question, review, [answer sentences]) format of SubjQA is commonly used in extractive QA datasets andwas pioneered in SQuAD, which is a famous dataset used to test the ability of machines to read a passage oftext and answer questions about it
 The dataset was created by sampling several hundred English articles fromWikipedia, partitioning each article into paragraphs, and then asking crowdworkers to generate a set ofquestions and answers for each paragraph
 In the first version of SQuAD, each answer to a question wasguaranteed to exist in the corresponding passage and it wasn’t long before sequence models performed betterthan humans at extracting the correct span of text with the answer
 To make the task more difficult, SQuAD2
08 was created by augmenting SQuAD 1
1 with a set of adversarial questions that are relevant to a givenpassage but cannot be answered from the text alone
 As of this book’s writing, the state-of-the-art is shown inFigure 4-8, with most models since 2019 surpassing human performance
Figure 4-8
 Progress on the SQuAD 2
0 benchmark
 Image from Papers With CodeHowever, this superhuman performance does not appear to reflect genuine reading comprehension since theunanswerable answers can be identified through patterns in the passages like antonyms
 To solve theseproblems, Google released the Natural Questions (NQ) dataset9 which involves fact-seeking questionsobtained from Google Search users
 The answers in NQ are much longer than SQuAD and present a morechallenging benchmark
Using Haystack to Build a QA PipelineIn our simple answer extraction example, we provided both the question and the context to the model
 However, inreality our system’s users will only provide a question about a product, so we need some way of selecting relevantpassages from among all the reviews in our corpus
 One way to do this would be to concantenate all the reviews ofa given product together and feed them to the model as a single, long context
 Although simple, the drawback ofthis approach is that the context can become extremely long and thereby introduce an unacceptable latency for ourusers’ queries
 For example, let’s suppose that on average, each product has 30 reviews and each review takes 100milliseconds to process
 If we need to process all the reviews to get an answer, this would give an average latencyof three seconds per user query - much too long for e-commerce websites!To handle this, modern QA systems are typically based on the Retriever-Reader architecture, which has two maincomponents:RetrieverResponsible for retrieving relevant documents for a given query
 Retrievers are usually categorized as sparseor dense
 Sparse Retrievers use sparse vector representations of the documents to measure which terms matchwith a query
 Dense Retrievers use encoders like transformers or LSTMs to encode a query and document intwo respective vectors of identical length
 The relevance of a query and a document is then determined bycomputing an inner product of the vectors
ReaderResponsible for extracting an answer from the documents provided by the Retriever
 The Reader is usually areading comprehension model, although we’ll see at the end of the chapter examples of models that cangenerate free-form answers
As illustrated in Figure 4-9, there can also be other components that apply post-processing to the documentsfetched by the Retriever or to the answers extracted by the Reader
 For example, the retrieved documents may needre-ranking to eliminate noisy or irrelevant ones that can confuse the Reader
 Similarly, post-processing of theReader’s answers is often needed when the correct answer comes from various passages in a long document
Figure 4-9
 The Retriever-Reader architecture for modern QA systems
To build our QA system, we’ll use the Haystack library which is developed by deepset, a German companyfocused on NLP
 The advantage of using Haystack is that it’s based on the Retriever-Reader architecture, abstractsmuch of the complexity involved in building these systems, and integrates tightly with Transformers
 You caninstall Haystack with the following pip command:pip install farm-haystackIn addition to the Retriever and Reader, there are two more components involved when building a QA pipelinewith Haystack:Document storeA document-oriented database that stores documents and metadata which are provided to the Retriever at querytime
PipelineCombines all the components of a QA system to enable custom query flows, merging documents from multipleRetrievers, and more
In this section we’ll look at how we can use these components to quickly build a prototype QA pipeline, and laterexamine how we can improve its performance
Initializing a Document StoreIn Haystack, there are various document stores to choose from and each one can be paired with a dedicated set ofRetrievers
 This is illustrated in Table 4-3, where the compatibility of sparse (TF-IDF, BM25) and dense(Embedding, DPR) Retrievers is shown for each of the available document stores
Since we’ll be exploring both sparse and dense retrievers in this chapter, we’ll use theElasticsearchDocumentStore which is compatible with both retriever types
 Elasticsearch is a searchengine that is capable of handling a diverse range of data, including textual, numerical, geospatial, structured, andunstructured
 Its ability to store huge volumes of data and quickly filter it with full-text search features makes itespecially well suited for developing QA systems
 It also has the advantage of being the industry standard forinfrastructure analytics, so there’s a good chance your company already has a cluster that you can work with
To initialize the document store, we first need to download and install Elasticsearch
 By following Elasticsearch’sguide, let’s grab the latest release for Linux10 with wget and Next we need to start the Elasticsearch server
 Since we’re running all the code in this book within Jupyternotebooks, we’ll need to use Python’s subprocess
Popen module to spawn a new process
 While we’re at it,let’s also run the subprocess in the background using the chown shell command:In the Popen module, the args specify the program we wish to execute, while stdout=PIPE creates a newpipe for the standard output, and stderr=STDOUT collects the errors in the same pipe
 The preexec_fnargument specifies the ID of the subprocess we wish to use
 By default, Elasticsearch runs locally on port 9200, sowe can test the connection by sending a HTTP request to Now that our Elasticsearch server is up and running, the next thing to do is instantiate the document store:By default, ElasticsearchDocumentStore creates two indices on Elasticsearch: one called document for(you guessed it) storing documents, and another called label for storing the annotated answer spans
 For now,we’ll just populate the document index with the SubjQA reviews, and Haystack’s document stores expect a listof dictionaries with text and meta keys as follows:The fields in meta can be used for applying filters during retrieval, so for our purposes we’ll include theitem_id and q_review_id columns of SubjQA so we can filter by product and question ID, along with thecorresponding training split
 We can then loop through the examples in each DataFrame and add them to theindex with the write_documents function as follows:for split, df in dfs
items():Great, we’ve loaded all our reviews into an index! To search the index we’ll need a Retriever, so let’s look at howwe can intialize one for Elasticsearch
Initializing a RetrieverThe Elasticsearch document store can be paired with any of the Haystack retrievers, so let’s start by using a sparseretriever based on BM25 (short for “Best Match 25”)
 BM25 is an improved version of the classic TF-IDF metricand represents the question and context as sparse vectors that can be searched efficiently on Elasticsearch
 TheBM25 score measures how much matched text is about a search query and improves on TF-IDF by saturating TFvalues quickly and normalizing the document length so that short documents are favoured over long ones
11In Haystack, the BM25 Retriever is included in ElasticsearchRetriever, so let’s initialize this class byspecifying the document store we wish to search over:from haystack
retriever
sparse import ElasticsearchRetrieveres_retriever = ElasticsearchRetriever(document_store=document_store)Next, let’s look at a simple query for a single electronics product in the training set
 For review-based QA systemslike ours, it’s important to restrict the queries to a single item because otherwise the Retriever would sourcereviews about products that are not related to a user’s query
 For example, asking “Is the camera quality anygood?” without a product filter could return reviews about phones, when the user might be asking about a specificlaptop camera instead
 By themselves, the ASIN values in our dataset are a bit cryptic, but we can decipher themwith online tools like amazon ASIN or by simply appending the value of item_id to the www
amazon
com/dp/URL
 The item ID below corresponds to one of Amazon’s Fire tablets, so let’s use the Retriever’s retrievefunction to ask if it’s any good for reading with:Here we’ve specified how many documents to return with the top_k argument and applied a filter on both theitem_id and split keys that were included in the meta field of our documents
 Each element ofretrieved_docs is a Haystack Document object that is used to represent documents and includes theRetriever’s query score along with other metadata
 Let’s have a look at one of the retrieved documents:retrieved_docs[0]In addition to the document’s text, we can see the score that Elasticsearch computed for its relevance to thequery (larger scores imply a better match)
 Under the hood, Elasticsearch relies on Lucene for indexing and search,so by default it uses Lucene’s practical scoring function
 You can find the nitty gritty details behind the scoringfunction in the Elasticsearch documentation, but in brief terms the scoring function first filters the candidatedocuments by applying a boolean test (does the document match the query?), and then applies a similarity metricthat’s based on representing both the document and query as vectors
Now that we have a way to retrieve relevant documents, the next thing we need is a way to extract answers fromthem
 This is where the Reader comes in, so let’s take a look at how we can load our MiniLM model in Haystack
Initializing a ReaderIn Haystack, there are two types of Readers one can use to extract answers from a given context:FARMReaderBased on deepset’s FARM framework for fine-tuning and deploying transformers
 Compatible with modelstrained using Transformers and can load models directly from the Hugging Face Hub
TransformersReaderBased on the QuestionAnsweringPipeline from Transformers
 Suitable for running inference only
Although both Readers handle a model’s weights in the same way, there are some differences in the way thepredictions are converted to produce answers:In Transformers, the QuestionAnsweringPipeline normalizes the start and end logits with asoftmax in each passage
 This means that it is only meaningful to compare answer scores betweenanswers extracted from the same passage, where the probabilities sum to one
 For example, an answerscore of 0
9 from one passage is not necessarily better than a score of 0
8 in another
 In FARM, the logitsare not normalized, so inter-passage answers can be compared more easily
The TransformersReader sometimes predicts the same answer twice, but with different scores
 Thiscan happen in long contexts if the answer lies across two overlapping windows
 In FARM, theseduplicates are removed
Since we will be fine-tuning the Reader later in the chapter, we’ll use the FARMReader
 Similar to Transformers,to load the model we just need to specify the MiniLM checkpoint on the Hugging Face Hub along with some QAspecific arguments:from haystack
reader
farm import FARMReadermodel_ckpt = "deepset/minilm-uncased-squad2"max_seq_length, doc_stride = 384, 128reader = FARMReader(model_name_or_path=model_ckpt, progress_bar=False,max_seq_len=max_seq_length, doc_stride=doc_stride,return_no_answer=True)NOTEIt is also possible to fine-tune a reading comprehension model directly in Transformers and then load it in TransformersReader to runinference
 For details on how to do the fine-tuning step, see the question-answering tutorial in the Transformers’ Big Table of Tasks
In FARMReader, the behavior of the sliding window is controlled by the same max_seq_length anddoc_stride arguments that we saw for the tokenizer, and we’ve used the values from the MiniLM paper
 As asanity check, let’s now test the Reader on our simple example from earlier:Great, the Reader appears to be working as expected, so next let’s tie together all our components using one ofHaystack’s pipelines
Putting It All TogetherHaystack provides a Pipeline abstraction that allows us to combine Retrievers, Readers, and other componentstogether as a graph that can be easily customized for each use case
 There are also predefined pipelines analogousto those in Transformers, but specialized for QA systems
 In our case, we’re interested in extracting answers sowe’ll use the ExtractiveQAPipeline which takes a single Retriever-Reader pair as its arguments:Each Pipeline has a run function that specifies how the query flow should be executed
 ForExtractiveQAPipeline we just need to pass the query, the number of documents to retrieve withtop_k_retriever, and number of answers to extract from these documents with top_k_reader
 In ourcase, we also need to specify a filter over the item ID which can be done using the filters argument as we didwith the Retriever earlier
 Let’s run a simple example using our question about the Amazon Fire tablet again, butthis time returning the extracted answers:Great, we now have an end-to-end QA system for Amazon product reviews! This is a good start, but notice that thesecond and third answer are closer to what the question is actually asking
 To do better, we’ll first need somemetrics to quantify the performance of the Retriever and Reader
 Let’s take a look
Improving Our QA PipelineAlthough much of the recent research on QA has focused on improving reading comprehension models, in practiceit doesn’t matter how good your Reader is if the Retriever can’t find the relevant documents in the first place! Inparticular, the Retriever sets an upper bound on the performance of the whole QA system, so it’s important tomake sure it’s doing a good job
 With this in mind, let’s start by introducing metrics to evaluate the Retriever andcomparing the performance of sparse and dense representations
Evaluating the RetrieverA common metric for evaluating Retrievers is recall, which measures the fraction of all relevant documents thatare retrieved
 In this context, relevant simply means whether the answer is present in a passage of text or not, sogiven a set of questions, we can compute recall by counting the number of times an answer appears in the top-kdocuments returned by the Retriever
NOTEA complementary metric to recall is mean average precision (mAP), which rewards Retrievers that can place the correct answers higher upin the document ranking
In Haystack there are two ways to evaluate Retrievers:Use the Retriever’s in-built eval function
 This can be used for both open- and closed-domain QA, butnot for datasets like SubjQA where each document is paired with a single product and we need to filter byproduct ID for every query
Build a custom Pipeline that combines a Retriever with the EvalRetriever class
 This enables thepossibility to implement custom metrics and query flows
Since we need to evaluate the recall per product and then aggregate across all products, we’ll opt for the secondapproach
 Each node in the Pipeline graph represents a class that takes some inputs and produces some ouputsvia a run function:class PipelineNode:Here kwargs corresponds to the outputs from the previous node in the graph, which is manipulated within run toreturn a tuple of the outputs for the next node, along with a name for the outgoing edge
 The only otherrequirement is to include an outgoing_edge attribute that indicates the number of outputs from the node (inmost cases outgoing_edge=1 unless you have branches in the pipeline that route the inputs according to somecriterion)
In our case, we need a node to evaluate the Retriever, so we’ll use the EvalRetriever class whose runfunction keeps track of which documents have answers that match the ground truth
 With this class we can thenbuild up a Pipeline graph by adding the evaluation node after a node that represents the Retriever itself:Notice that each node is given a name and a list of inputs
 In most cases, each node has a single outgoing edge,so we just need to include the name of the previous node in inputs
Now that we have our evaluation pipeline, we need to pass some queries and their corresponding answers
 To dothis, we’ll add the answers to a dedicated label index on our document store
 Haystack provides a Label objectthat represents the answer spans and their metadata in a standardized fashion
 To populate the label index, we’llfirst create a list of Labels objects by looping over each question in the test set and extracting the matchinganswers and additional metadata:If we peek at one of these labelswe can see the question-answer pair along with an origin field that contains the unique question ID so we canfilter the document store per question
 We’ve also added the product ID to the model_id field so we can filter thelabels by product
 Now that we have our labels, we can write them to the label index on Elasticsearch asfollows:Next, we need to build up a mapping between our question IDs and corresponding answers that we can pass to thepipeline
 To get all the labels, we can use the get_all_labels_aggregated function from the documentstore that will aggregate all question-answer pairs associated with a unique ID
 This function returns a list ofMultiLabel objects, but in our case we only get one element since we’re filtering by question ID, so we canbuild up a list of aggregated labels as follows:By peeking at one of these labels we can see that all the answer associated with a given question are aggregatedtogether in a multiple_answers field:labels_agg[14]Since we’ll soon evaluate both the Retriever and Reader in the same run, we need to provide the gold labels forboth components in the Pipeline
run function
 The simplest way to achieve this is by creating a dictionarythat maps the unique question ID with a dictionary of labels, one for each component:qid2label = {l
origin: {"retriever": l, "reader": l} for l in labels_agg}We now have all the ingredients for evaluating the Retriever, so let’s define a function that feeds each questionanswers pair associated with each product to the evaluation pipeline and tracks the correct retrievals in our pipeobject:Great, it works! Notice that we picked a specific value for top_k_retriever to specify the number ofdocuments to retrieve
 In general, increasing this parameter will improve the recall, but at the expense of providingmore documents to the Reader and slowing down the end-to-end pipeline
 To guide our decision on which value topick, we’ll create a function that loops over several k values and compute the recall across the whole test set foreach k:If we plot the results, we can see how the recall improves as we increase k:average each product has three reviews, soreturning five or more documents means that we are very likely to get the correct context
Dense Passage RetrievalWe’ve seen that we get almost perfect recall when our sparse Retriever returns k = 10 documents, but can we dobetter at smaller values of k? The advantage of doing so is that we can pass fewer documents to the Reader andthereby reduce the overall latency of our QA pipeline
 One well known limitation of sparse Retrievers like BM25is that they can fail to capture the relevant documents if the user query contains terms that don’t match exactlythose of the review
 One promising alternative is to use dense embeddings to represent the question and documentand the current state-of-the-art is an architecture known as Dense Passage Retrieval (DPR)
12 The main ideabehind DPR is to use two BERT models as encoders E (⋅) and E (⋅) for the question and passage
 As illustratedin Figure 4-10, these encoders map the input text into a d-dimensional vector representation of the [CLS] token
Figure 4-10
 DPR’s bi-encoder architecture for computing the relevance of a document and query
In Haystack, we can initialize a Retriever for DPR in a similar way we did for BM25
 In addition to specifying thedocument store, we also need to pick the BERT encoders for the question and passage
 These encoders are trainedby giving them questions with relevant (positive) passages and irrelevant (negative) passages, where the goal is tolearn that relevant question-passage pairs have a higher similarity
 For our use case, we’ll use encoders that havebeen fine-tuned on the NQ corpus in this way:Here we’ve also set embed_title=False since concatenating the document’s title (i
e
 item_id) doesn’tprovide any additional information because we filter per product
 Once we’ve initialized the dense Retriever, thenext step is iterate over all the indexed documents on our Elasticseach index and apply the encoders to update theembedding representation
 This can be done as follows:document_store
update_embeddings(retriever=dpr_retriever)We’re now set to go! We can evaluate the dense Retriever in the same way we did for BM25 and compare the topk recall:Here we can see that DPR does not provide a boost in recall over Performing similarity search of the embeddings can be sped up by using Facebook’s FAISS library as the document store
 Similarly, theperformance of the DPR Retriever can be improved by fine-tuning on the target domain
Now that we’ve explored the evaluation of the Retriever, let’s turn to evaluating the Reader
Evaluating the ReaderIn extractive QA, there are two main metrics that are used for evaluating Readers:scoreWe encountered this metric in Chapter 2 and it measures the harmonic mean of the precision and recall
Let’s see how these metrics work by importing some helper functions from FARM and applying them to a simpleexample:from farm
evaluation
squad_evaluation import compute_f1, compute_exactUnder the hood, these functions first normalise the prediction and label by removing punctuation, fixingwhitespace, and converting to lowercase
 The normalized strings are then tokenized as a bag-of-words, beforefinally computing the metric at the token level
 From this simple example we can see that EM is a much strictermetric than the F1 score: adding a single token to the prediction gives an EM of zero
 On the other hand, the F1score can fail to catch truly incorrect answers
 For example, suppose our predicted answer span was “about 6000Relying on just the F1 score is thus misleading, and tracking both metrics is a good strategy to balance the tradeoff between underestimating (EM) and overestimating (F1 score) model performance
Now in general, there are multiple valid answers per question, so these metrics are calculated for each questionanswer pair in the evaluation set, and the best score is selected over all possible answers
 The overall EM and Fscores for the model are then obtained by averaging over the individual scores of each question-answer pair
To evaluate the Reader we’ll create a new pipeline with two nodes: a Reader node and a node to evaluate theReader
 We’ll use the EvalReader class that takes the predictions from the Reader and computes thecorresponding EM and F scores
 To compare with the SQuAD evaluation, we’ll take the best answers for eachquery with the top_1_em and top_1_f1 metrics that are stored in EvalReader:Notice that we specified skip_incorrect_retrieval=False; this is needed to ensure that the Retrieveralways passes the context to the Reader (as done in the SQuAD evaluation)
 Now that we’ve run through everyquestion through the reader, let’s print out the scores:Okay, it seems that the fine-tuned model performs significantly worse on SubjQA than on SQuAD 2
0, whereMiniLM achieves an EM and F score of 76
1 and 79
5 respectively
 One reason for the performance drop is thatcustomer reviews are quite a different domain from Wikipedia (where SQuAD 2
0 is generated from), and thelanguage is often quite informal
 Another reason is likely due to the inherent subjectivity of our dataset, whereboth questions and answers differ from the factual information contained in Wikipedia
 Let’s have a look at howwe can fine-tune these models on a dataset to get better results with domain adaptation
Domain AdaptationAlthough models that are fine-tuned on SQuAD will often generalize well to other domains, we’ve seen that forSubjQA that the EM and F scores are more than halved compared to the SQuAD validation set
 This failure togeneralize has also been observed in other extractive QA datasets13 and is understood as evidence that transformermodels are particularly adept at overfitting to SQuAD
 The most straightforward way to improve the Reader is byfine-tuning our MiniLM model further on the SubjQA training set
 The FARMReader has a train method thatis designed for this purpose and expects the data to be in SQuAD JSON format, where all the question-answerpairs are grouped together for each item as illustrated in Figure 4-11
Figure 4-11
 Visualization of the SQuAD JSON format
You can download the pre-processed data from the book’s GitHub repository ADD LINK
 Now that we have thesplits in the right format, let’s fine-tune our Reader by specifying the location of the train and dev splits, along withthe location of where to save the fine-tuned model:Wow, domain adaptation has increased our EM score by a factor of six and more than doubled the F score!However, you might ask why didn’t we just fine-tune a pretrained language model directly on the SubjQA trainingset? One answer is that we only have a 1,295 training examples in SubjQA while SQuAD has over 100,000 so wecan run into challenges with overfitting
 Nevertheless, let’s take a look at what naive fine-tuning produces
 For afair comparison, we’ll use the same language model that was used for fine-tuning our baseline on SQuAD
 Asbefore, we’ll load up the model with the FARMReader:WARNINGWhen dealing with small datasets, it is best practice to use cross-validation when evaluating transformers as they can be prone to overfitting
You can find an example for how to perform cross-validation with SQuAD-formatted datasets in the FARM repository
Evaluating the Whole QA PipelineNow that we’ve seen how to evaluate the Reader and Retriever components individually, let’s tie them together tomeasure the overall performance of our pipeline
 To do so, we’ll need to augment our Retriever pipeline withnodes for the Reader and its evaluation
 We’ve seen that we get almost perfect recall at k = 10, so we can fix thisvalue and assess the impact this has on the Reader’s performance (since it will now receive multiple contexts perquery compared to the SQuAD-style evaluation)
We can then compare the top-1 and top-3 EM and F scores for the model to predict an answer in the documentsreturned by the Retriever:Figure 4-12
 Comparison of EM and F1 scores for the Reader against the whole QA pipelineFrom this plot we can see the effect that the Retriever has on the overall performance
 In particular, there is anoverall degradation of performance compared to matching the question-context pairs as is done in the SQuADstyle evaluation
 This can be circumvented by increasing the number of possible answers that the Reader isallowed to predict
 Until now we have only extracted answer spans from the context but in general it could be thatbits and pieces of the answer are scattered throughout the document and we would like our model to synthesizethese fragments into a single, coherent answer
 Let’s have a look how we can use generative QA to succeed at thistask
Going Beyond Extractive QAOne interesting alternative to extracting answers as spans of text in a document is to generate them with apretrained language model
 This approach is often referred to as abstractive or generative QA and has the potentialto produce better phrased answers that synthesize evidence across multiple passages
 Although less mature thanextractive QA, this is a fast-moving field of research, so chances are that these approaches will be widely adoptedin industry by the time you are reading this! In this section we’ll briefly touch the current state-of-the-art: RetrievalAugmented Generation (RAG)
14Retrieval Augmented GenerationRAG extends the classic Retriever-Reader architecture that we’ve seen in this chapter by swapping the Reader fora Generator and using DPR as the Retriever
 The Generator is a pretrained sequence-to-sequence transformer likeT5 or BART which receives latent vectors of documents from DPR and then iteratively generates an answer basedon the query and these documents
 Since DPR and the Generator are differentiable, the whole process can be finetuned end-to-end as illustrated in Figure 4-13
 You can find an interactive demo of RAG on the Hugging Facewebsite
Figure 4-13
 The RAG architecture for fine-tuning a Retriever and Generator end-to-end (courtesy of Ethan Perez)
To show RAG in action, we’ll use the DPRetriever from earlier so we just need to instantiate a Generator
There are two types of RAG models to choose from:RAG-SequenceUses the same retrieved document to generate the complete answer
 In particular, the top-k documents from theRetriever are fed to the Generator which produces an output sequence for each document, and the result ismarginalized to obtain the best answer
RAG-TokenCan use a different document to generate each token in the answer
 This allows the Generator to synthesizeevidence from multiple documents
Since RAG-Token models tend to perfom better than RAG-Sequence ones, we’ll use the token model that wasfine-tuned on NQ as our Generator
 Instantiating a Generator in Haystack is similar to the Reader, but instead ofspecifying the max_seq_length and doc_stride parameters for a sliding window over the contexts, wespecify hyperparameters that control the text generation:Here max_length and min_length control the length of the generated answers, while num_beams specifiesthe number of beams to use in beam search (text generation is covered at length in Chapter 8)
 As we did with theDPR Retriever, we don’t embed the document titles since our corpus is always filtered per product ID
The next thing to do it tie together the Retriever and Generator using Haystack’s GenerativeQAPipeline:from haystack
pipeline import GenerativeQAPipelinepipe = GenerativeQAPipeline(generator=generator, retriever=dpr_retriever)NOTEIn RAG, both the query encoder and the generator are trained end-to-end, while the context encoder is frozen
 In Haystack, theGenerativeQAPipeline uses the query encoder from RAGenerator and the context encoder from DensePassageRetriever
Let’s now give RAG a spin by feeding some queries about the Amazon Fire tablet from before
 To simplify thequerying, let’s write a simple function that takes query and prints out the top answers:Hmm, this result it a bit disappointing and suggests that the subjective nature of the question is confusing theGenerator
 Let’s try with something a bit more factual:generate_answers("What is the main drawback?")Question: What is the main drawback?Okay, this is more sensible! To get better results we could fine-tune RAG end-to-end on SubjQA, and if you’reinterested in exploring this there are scripts in the Transformers repository to help you get started
ConclusionWell, that was a whirlwind tour of QA and you probably have many more questions that you’d like answered (punintended!)
 We have discussed two approaches to QA (extractive and generative), and examined two differentretrieval algorithms (BM25 and DPR)
 Along the way, we saw that domain adaptation can be a simple technique toboost the performance of our QA system by a significant margin, and we looked at a few of the most commonmetrics that are used for evaluating such systems
 Although we focused on closed-domain QA (i
e
 a single domainof electronic products), the techniques in this chapter can easily be generalized to the open-domain case and werecommend reading Cloudera’s excellent Fast Forward QA series to see what’s involved
Deploying QA systems in the wild can be a tricky business to get right, and our experience is that a significant partof the value comes from first providing end-users with useful search capabilities, followed by an extractivecomponent
 In this respect, the Reader can be used in novel ways beyond answering on-demand user queries
 Forexample, Grid Dynamics were able to use their Reader to automatically extract a set of pros and cons for eachproduct in their client’s catalogue
 Similarly, they show that a Reader can also be used to extract named entities ina zero-shot fashion by creating queries like “What kind of camera?”
 Given its infancy and subtle fail-modes, werecommend exploring answering generation only once the other two approaches have been exhausted
 This“hierarchy of needs” for tackling QA problems is illustrated in Figure 4-14
Figure 4-14
 The QA hierarchy of needs
Looking towards the future, one exciting research area to keep an eye on is multimodal QA which involves QAover multiple modalities like text, tables, and images
 As described in the MultiModalQA benchmark 15, suchsystems can potentially enable users to answer complex questions like “When was the famous painting with twotouching fingers completed?” which integrate information across different modalities
 Another area with practicalbusiness applications is QA over a knowledge graph, where the nodes of the graph correspond to real-worldentities and their relations are defined by the edges
 By encoding factoids as (subject, predicate, object) triples, onecan use the graph to answer questions about one of the missing elements
 You can find an example that combinestransformers with knowledge graphs in the Haystack tutorials
 One last promising direction is “automatic questiongeneration” as a way to do some form of unsupervised/weakly supervized training from unlabelled data or dataaugmentation
 Two recent examples of papers on this include the Probably Answered Questions (PAQ)benchmark16 and synthetic data augmentation17 for cross-lingual settings
In this chapter we’ve seen that in order to successfully use QA models in real world use-cases we need to apply afew tricks such as a fast retrieval pipeline to make predictions in near real-time
 Still, applying a QA model to ahandful of preselected documents can take a couple of seconds on production hardware
 Although this does notsound like much imagine how different your experience would be if you had to wait a few seconds to get theresults of your Google search
 A few seconds of wait time can decide the fate of your transformer poweredapplication and in the next chapter we have a look at a few methods to accelerate the model predictions further
1 In this particular case, providing no answer at all may actually be the right choice, since the answer depends on when the question is asked andconcerns a global pandemic where having accurate health information is essential
2 SUBJQA: A Dataset for Subjectivity and Review Comprehension, J
 Bjerva et al
 (2020)3 As we’ll soon see, there are also unanswerable questions that are designed to produce more robust reading comprehension models
4 SQuAD: 100,000+ Questions for Machine Comprehension of Text, P
 Rajpurkar et al
 (2016)5 MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers, W
 Wang et al (2020)6 Note that the token_type_ids are not present in all transformer models
 In the case of BERT-like models such as MiniLM, thetoken_type_ids are also used during pretraining to incorporate the next-sentence prediction task
7 See Chapter 2 for details on how these hidden states can be extracted
8 Know What You Don’t Know: Unanswerable Questions for SQuAD, P
 Rajpurkar, R
 Jia, and P
 Liang (2018)9 Natural Questions: a Benchmark for Question Answering Research, T
 Kwiatkowski et al (2019)10 The guide also provides installation instructions for mac OS and Windows
11 For an in-depth explanation on document scoring with TF-IDF and BM25 see Chapter 23 of Speech and Language Processing, D
 Jurafsky andJ
H
 Martin (2020)12 Dense Passage Retrieval for Open-Domain Question Answering, V
 Karpukhin et al (2020)13 Learning and Evaluating General Linguistic Intelligence D
 Yogatama et al
 (2019)14 Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, P
 Lewis et al (2020)15 MultiModalQA: Complex question answering over text, tables and images, A
 Talmor et al (2021)16 PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them, P
 Lewis et al (2021)
17 Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering, A
 Riabi et al (2020)
