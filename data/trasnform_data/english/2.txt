
Chapter 2. Text Classification

Now imagine that you are a data scientist who needs to build a system that can automatically
identify emotional states such as “anger” or “joy” that people express towards your company’s
product on Twitter. Until 2018, the deep learning approach to this problem typically involved
finding a suitable neural architecture for the task and training it from scratch on a dataset of
labeled tweets. This approach suffered from three major drawbacks:
You needed a lot of labeled data to train accurate models like recurrent or
convolutional neural networks.
Training these models from scratch was time consuming and expensive.

The trained model could not be easily adapted to a new task, e.g. with a different set of
labels.
Nowadays, these limitations are largely overcome via transfer learning, where typically a
Transformer-based architecture is pretrained on a generic task such as language modeling and
then reused for a wide variety of downstream tasks. Although pretraining a Transformer can
involve significant data and computing resources, many of these language models are made
freely available by large research labs and can be easily downloaded from the Hugging Face
Model Hub!
This chapter will guide you through several approaches to emotion detection using a famous
Transformer model called BERT, short for Bidirectional Encoder Representations from
Transformers.1 This will be our first encounter with the three core libraries from the Hugging
Face ecosystem: Datasets, Tokenizers, and Transformers. As shown in Figure 2-2, these
libraries will allow us to quickly go from raw text to a fine-tuned model that can be used for
inference on new tweets. So in the spirit of Optimus Prime, let’s dive in, “transform and
rollout!”

The Dataset
To build our emotion detector we’ll use a great dataset from an article2 that explored how
emotions are represented in English Twitter messages. Unlike most sentiment analysis datasets
that involve just “positive” and “negative” polarities, this dataset contains six basic emotions:
anger, disgust, fear, joy, sadness, and surprise. Given a tweet, our task will be to train a model
that can classify it into one of these emotions!

A First Look at Hugging Face Datasets
We will use the Hugging Face Datasets library to download the data from the Hugging Face
Dataset Hub. This library is designed to load and process large datasets efficiently, share them
with the community, and simplify interoperatibility between NumPy, Pandas, PyTorch, and
TensorFlow. It also contains many NLP benchmark datasets and metrics, such as the Stanford
Question Answering Dataset (SQuAD), General Language Understanding Evaluation (GLUE),
and Wikipedia. We can use the list_datasets function to see what datasets are available
in the Hub.

This looks like the dataset we’re after, so next we can load it with the load_dataset
function from Datasets.

In each case the resulting data structure depends on the type of query; although this may feel
strange at first, it’s part of the secret sauce that makes Datasets so flexible!
So now that we’ve seen how to load and inspect data with Datasets let’s make a few sanity
checks about the content of our tweets.

From Datasets to DataFrames
Although Datasets provides a lot of low-level functionality to slice and dice our data, it is often
convenient to convert a Dataset object to a Pandas DataFrame so we can access highlevel APIs for data visualization. To enable the conversion, Datasets provides a
Dataset.set_format function that allow us to change the output format of the Dataset.
This does not change the underlying data format which is Apache Arrow and you can switch to
another format later if needed.

As we can see, the column headers have been preserved and the first few rows match our
previous views of the data.

Before diving into building a classifier let’s take a closer look at the dataset. As Andrej
Karpathy famously put it, becoming “one with the data”3 is essential to building great models.

Look at the Class Distribution

Whenever you are working on text classification problems, it is a good idea to examine the
distribution of examples among each class. For example, a dataset with a skewed class
distribution might require a different treatment in terms of the training loss and evaluation
metrics than a balanced one.

We can see that the dataset is heavily imbalanced; the joy and sadness classes appear
frequently whereas love and sadness are about 5-10 times rarer. There are several ways to
deal with imbalanced data such as resampling the minority or majority classes. Alternatively,
we can also weight the loss function to account for the underrepresented classes. However, to
keep things simple in this first practical application we leave these techniques as an exercise for
the reader and move on to examining the length of our tweets.

How Long Are Our Tweets?
Transformer models have a maximum input sequence length that is referred to as the maximum
context size. For most applications with BERT, the maximum context size is 512 tokens, where
a token is defined by the choice of tokenizer and can be a word, subword, or character. Let’s
make a rough estimate of our tweet lengths per emotion by looking at the distribution of words
per tweet.

From the plot we see that for each emotion, most tweets are around 15 words long and the
longest tweets are well below BERT’s maximum context size of 512 tokens. Texts that are
longer than a model’s context window need to be truncated, which can lead to a loss in
performance if the truncated text contains crucial information. Let’s now figure out how we can
convert these raw texts into a format suitable for Transformers!

From Text to Tokens
Transformer models like BERT cannot receive raw strings as input; instead they assume the
text has been tokenized into numerical vectors. Tokenization is the step of breaking down a
string into the atomic units used in the model. There are several tokenization strategies one can
adopt and the optimal splitting of words in sub-units is usually learned from the corpus. Before
looking at the tokenizer used for BERT, let’s motivate it by looking at two extreme cases:
character and word tokenizers.

Character Tokenization
The simplest tokenization scheme is to feed each character individually to the model. In
Python, str objects are really arrays under the hood which allows us to quickly implement
character-level tokenization with just one line of code.

This is a good start but we are not yet done because our model expects each character to be
converted to an integer, a process called numericalization.

We are almost done! Each token has been mapped to a unique, numerical identifier, hence the
name input_ids. The last step is to convert input_ids into a 2d tensor of one-hot vectors
which are better suited for neural networks than the categorical representation of input_ids.
The reason for this is that the elements of input_ids create an ordinal scale, so adding or
subtracting two IDs is a meaningless operation since the result in a new ID that represents
another random token. On the other hand, the result of the adding two one-hot encodings can be
easily interpreted: the two entries that are “hot” indicate that the corresponding two tokens cooccur. 

From our simple example we can see that character-level tokenization ignores any structure in
the texts such as words and treats them just as streams of characters. Although this helps deal
with misspellings and rare words, the main drawback is that linguistic structures such as words
need to be learned, and that process requires significant compute and memory. For this reason,
character tokenization is rarely used in practice. Instead, some structure of the text such as
words is preserved during the tokenization step. Word tokenization is a straightforward
approach to achieve this - let’s have a look at how it works!

Word Tokenization
Instead of splitting the text into characters, we can split it into words and map each word to an
integer. By using words from the outset, the model can skip the step of learning words from
characters and thereby eliminate complexity from the training process.

From here we can take the same steps we took for the character tokenizer and map each word
to a unique identifier. However, we can already see one potential problem with this
tokenization scheme; punctuation is not accounted for, so NLP. is treated as a single token.
Given that words can include declinations, conjugations, or misspellings, the size of the
vocabulary can easily grow into the millions!

NOTE
There are variations of word tokenizers that have extra rules for punctuation. One can also apply stemming which
normalises the words to their stem (e.g. “great”, “greater”, and “greatest” all become “great”) at the expense of
losing some information in the text.

The reason why having a large vocabulary is a problem is that it requires neural networks with
an enormous number of parameters. To illustrate this, suppose we have 1 million unique words
and want to compress the 1-million dimensional input vectors to 1-thousand dimensional
vectors in the first layer of a neural network. This is a standard step in most NLP architectures
and the resulting weight matrix of this vector would contain 1 million × 1 thousand weights =
1 billion weights. This is already comparable to the largest GPT-2 model which has 1.4 billion
parameters in total!
Naturally, we want to avoid being so wasteful with our model parameters since they are
expensive to train and larger models are more difficult to maintain. A common approach is to
limit the vocabulary and discard rare words by considering say the 100,000 most common

words in the corpus. Words that are not part of the vocabulary are classified as “unknown” and
mapped to a shared UNK token. This means that we lose some potentially important information
in the process of word tokenization since the model has no information about which words
were associated with the UNK tokens.
Wouldn’t it be nice if there was a compromise between character and word tokenization that
preserves all input information and some of the input structure? There is! Let’s look at the main
ideas behind subword tokenization.

Subword Tokenization
The idea behind subword tokenization is to take the best of both worlds from character and
word tokenization. On one hand we want to use characters since they allow the model to deal
with rare character combinations and misspellings. On the other hand, we want to keep frequent
words and word parts as unique entities.
WARNING
Changing the tokenization of a model after pretraining would be catastrophic since the learned word and subword
representations would become obsolete! The Transformers library provides functions to make sure the right
tokenizer is loaded for the corresponding Transformer.

There are several subword tokenization algorithms such as Byte-Pair-Encoding, WordPiece,
Unigram, and SentencePiece. Most of them adopt a similar strategy:
Simple tokenization
The text corpus is split into words, usually according to whitespace and punctuation rules.
Counting
All the words in the corpus are counted and the tally is stored.
Splitting
The words in the tally are split into subwords. Initially these are characters.
Subword pairs counting
Using the tally, the subword pairs are counted.
Merging
Based on a rule, some of the subword pairs are merged in the corpus.
Stopping
The process is stopped when a predefined vocabulary size is reached.

There are several variations of this procedure in the above algorithms and the Tokenizer
Summary in the Transformers documentation provides detailed information about each
tokenization strategy. The main distinguishing feature of subword tokenization (as well as word
tokenization) is that it is learned from the corpus used for pretraining. Let’s have a look at how
subword tokenization actually works using the Hugging Face Transformers library!

Using Pretrained Tokenizers
We’ve noted that loading the right pretrained tokenizer for a given pretrained model is crucial
to getting sensible results. The Transformers library provides a convenient
from_pretrained function that can be used to load both objects, either from the Hugging
Face Model Hub or from a local path.
To build our emotion detector we’ll use a BERT variant called DistilBERT,4 which is a
downscaled version of the original BERT model. The main advantage of this model is that it
achieves comparable performance to BERT while being significantly smaller and more
efficient. This enables us to train a model within a few minutes and if you want to train a larger
BERT model you can simply change the model_name of the pretrained model. The interface
of the model and the tokenizer will be the same, which highlights the flexibility of the
Transformers library; we can experiment with a wide variety of Transformer models by just
changing the name of the pretrained model in the code!
TIP
It is a good idea to start with a smaller model so that you can quickly build a working prototype. Once you’re
confident that the pipeline is working end-to-end, you can experiment with larger models for performance gains.

where the AutoTokenizer class ensures we pair the correct tokenizer and vocabulary with
the model architecture.

We can examine a few attributes of the tokenizer such as the vocabulary size:


We can observe two things. First, the [CLS] and [SEP] tokens have been added
automatically to the start and end of the sequence, and second, the long word
complicatedtest has been split into two tokens. The ## prefix in ##test signifies that
the preceding string is not a whitespace and that it should be merged with the previous token.
Now that we have a basic understanding of the tokenization process we can use the tokenizer to
feed tweets to the model.

Training a Text Classifier
As discussed in Chapter 2, BERT models are pretrained to predict masked words in a sequence
of text. However, we can’t use these language models directly for text classification, so instead
we need to modify them slightly. To understand what modifications are necessary let’s revisit
the BERT architecture depicted in Figure 2-3.

First, the text is tokenized and represented as one-hot vectors whose dimension is the size of
the tokenizer vocabulary, usually consisting of 50k-100k unique tokens. Next, these token
encodings are embedded in lower dimensions and passed through the encoder block layers to
yield a hidden state for each input token. For the pretraining objective of language modeling,
each hidden state is connected to a layer that predicts the token for the input token, which is
only non-trivial if the input token was masked. For the classification task, we replace the
language modeling layer with a classification layer. BERT sequences always start with a
classification token [CLS], therefore we use the hidden state for the classification token as
input for our classification layer.

NOTE
In practice, PyTorch skips the step of creating a one-hot vector because multiplying a matrix with a one-hot vector
is the same as extracting a column from the embedding matrix. This can be done directly by getting the column
with the token ID from the matrix.

We have two options to train such a model on our Twitter dataset:

Feature extraction
We use the hidden states as features and just train a classifier on them.
Fine-tuning
We train the whole model end-to-end, which also updates the parameters of the pretrained
BERT model.
In this section we explore both options for DistilBert and examine their trade-offs.

Transformers as Feature Extractors
To use a Transformer as a feature extractor is fairly simple; as shown in Figure 2-4 we freeze
the body’s weights during training and use the hidden states as features for the classifier. The
advantage of this approach is that we can quickly train a small or shallow model. Such a model
could be a neural classification layer or a method that does not rely on gradients such a
Random Forest. This method is especially convenient if GPUs are unavailable since the hidden
states can be computed relatively fast on a CPU.

Figure 2-4. In the feature-based approach, the BERT model is frozen and just provides features for a classifier.

The feature-based method relies on the assumption that the hidden states capture all the
information necessary for the classification task. However, if some information is not required
for the pretraining task, it may not be encoded in the hidden state, even if it would be crucial
for the classification task. In this case the classification model has to work with suboptimal
data, and it is better to use the fine-tuning approach discussed in the following section.

Here we’ve used PyTorch to check if a GPU is available and then chained the PyTorch
nn.Module.to("cuda") method to the model loader; without this, we would execute the
model on the CPU which can be considerably slower.
The AutoModel class corresponds to the input encoder that translates the one-hot vectors to
embeddings with positional encodings and feeds them through the encoder stack to return the
hidden states. The language model head that takes the hidden states and decodes them to the
masked token prediction is excluded since it is only needed for pretraining. If you want to use
that model head you can load the complete model with AutoModelForMaskedLM.

We can now pass this tensor to the model to extract the hidden states. Depending on the model
configuration, the output can contain several objects such as the hidden states, losses, or
attentions, that are arranged in a class that is similar to a namedtuple in Python. In our
example, the model output is a Python dataclass called BaseModelOutput, and like any
class, we can access the attributes by name. Since the current model returns only one entry
which is the last hidden state, let’s pass the encoded text and examine the outputs:

Looking at the hidden state tensor we see that it has the shape [batch_size, n_tokens,
hidden_dim]. The way BERT works is that a hidden state is returned for each input, and the
model uses these hidden states to predict masked tokens in the pretraining task. For
classification tasks, it is common practice to use the hidden state associated with the [CLS]
token as the input feature, which is located at the first position in the second dimension.

Tokenizing the Whole Dataset
Now that we know how to extract the hidden states for a single string, let’s tokenize the whole
dataset! To do this, we can write a simple function that will tokenize our examples.

we see that the result is a dictionary, where each value is a list of lists generated by the
tokenizer. In particular, each sequence in input_ids starts with 101 and ends with 102,
followed by zeroes, corresponding to the [CLS], [SEP], and [PAD] tokens respectively:

Also note that in addition to returning the encoded tweets as input_ids, the tokenizer also
returns list of attention_mask arrays. This is because we do not want the model to get
confused by the additional padding tokens, so the attention mask allows the model to ignore the
padded parts of the input. See Figure 2-5 for a visual explanation on how the input IDs and
attention masks are formatted.

WARNING
Since the input tensors are only stacked when passing them to the model, it is important that the batch size of the
tokenization and training match and that there is no shuffling. Otherwise the input tensors may fail to be stacked
because they have different lengths. This happens because they are padded to the maximum length of the
tokenization batch which can be different for each batch. When in doubt, set batch_size=None in the
tokenization step since this will apply the tokenization globally and all input tensors will have the same length.
This will, however, use more memory. We will introduce an alternative to this approach with a collate function
which only joins the tensors when they are needed and pads them accordingly.

To apply our tokenize function to the whole emotions corpus, we’ll use the
DatasetDict.map function. This will apply tokenize across all the splits in the corpus,
so our training, validation and test data will be preprocessed in a single line of code:
emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)

By default, DatasetDict.map operates individually on every example in the corpus, so
setting batched=True will encode the tweets in batches, while batch_size=None
applies our tokenize function in one single batch and ensures that the input tensors and
attention masks have the same shape globally. We can see that this operation has added two
new features to the dataset: input_ids and the attention mask.

From Input IDs to Hidden States
Now that we have converted our tweets to numerical inputs, the next step is to extract the last
hidden states so that we can feed them to a classifier. If we had a single example we could
simply pass the input_ids and attention_mask to the model as follows

but what we really want are the hidden states across the whole dataset. For this, we can use the
DatasetDict.map function again! Let’s define a forward_pass function that takes a
batch of input IDs and attention masks, feeds them to the model, and adds a new
hidden_state feature to our batch.

Creating a Feature Matrix
The preprocessed dataset now contains all the information we need to train a clasifier on it. We
will use the hidden states as input features and the labels as targets. We can easily create the
corresponding arrays in the well known Scikit-Learn format as follows.

Dimensionality Reduction with UMAP
Before we train a model on the hidden states, it is good practice to perform a sanity check that
they provide a useful representation of the emotions we want to classify. Since visualising the
hidden states in 768 dimensions is tricky to say the least, we’ll use the powerful UMAP5
algorithm to project the vectors down to 2D. Since UMAP works best when the features are
scaled to lie in the [0,1] interval, we’ll first apply a MinMaxScaler and then use UMAP to
reduce the hidden states.

The result is an array with the same number of training samples, but with only 2 features
instead of the 768 we started with! Let us investigate the compressed data a little bit further and
plot the density of points for each category separately.

NOTE
These are only projections onto a lower dimensional space. Just because some categories overlap does not mean
that they are not separable in the original space. Conversely, if they are separable in the projected space they will
be separable in the original space.

Now there seem to be clearer patterns; the negative feelings such as sadness, anger and
fear all occupy a similar regions with slightly varying distributions. On the other hand, joy
and love are well separated from the negative emotions and also share a similar space.
Finally, surprise is scattered all over the place. We hoped for some separation but this in no
way guaranteed since the model was not trained to know the difference between this emotions
but learned them implicitly by predicting missing words.
Training a Simple Classifier
We have seen that the hidden states are somewhat different between the emotions, although for
several of them there is not an obvious boundary. Let’s use these hidden states to train a simple
logistic regressor with Scikit-Learn! Training such a simple model is fast and does not require a
GPU.

By looking at the accuracy it might appear that our model is just a bit better than random, but
since we are dealing with an unbalanced multiclass dataset this is significantly better than
random. We can get a better feeling for whether our model is any good by comparing against a
simple baseline. In Scikit-Learn there is a DummyClassifier that can be used to build a
classifier with simple heuristics such as always choose the majority class or always draw a random class. 


which yields an accuracy of about 35%. So our simple classifier with BERT embeddings is
significantly better than our baseline. We can further investigate the performance of the model
by looking at the confusion matrix of the classifier, which tells us the relationship between the
true and predicted labels.

We can see that anger and fear are most often confused with sadness, which agrees with
the observation we made when visualizing the embeddings. Also love and surprise are
frequently mistaken for joy.
To get an even better picture of the classification performance we can print Scikit-Learn’s
classification report and look at the precision, recall and F -score for each class:

In the next section we will explore the fine-tuning approach which leads to superior
classification performance. It is however important to note, that doing this requires much more
computational resources, such as GPUs, that might not be available in your company. In cases
like this, a feature-based approach can be a good compromise between doing traditional
machine learning and deep learning.

Fine-tuning Transformers
Let’s now explore what it takes to fine-tune a Transformer end-to-end. With the fine-tuning
approach we do not use the hidden states as fixed features, but instead train them as shown in
Figure 2-6. This requires the classification head to be differentiable, which is why this method
usually uses a neural network for classification. Since we retrain all the DistilBERT parameters,
this approach requires much more compute than the feature extraction approach and typically
requires a GPU.

Since we train the hidden states that serve as inputs to the classification model, we also avoid
the problem of working with data that may not be well suited for the classification task. Instead,
the initial hidden states adapt during training to decrease the model loss and thus increase its
performance. If the necessary compute is available, this method is commonly chosen over the
feature-based approach since it usually outperforms it.
We’ll be using the Trainer API from Transformers to simplify the training loop - let’s look at
the ingredients we need to set one up!

he first thing we need is a pretrained DistilBERT model like the one we used in the featurebased approach. The only slight modification is that we use the
AutoModelForSequenceClassification model instead of AutoModel. The
difference is that the AutoModelForSequenceClassification model has a
classification head on top of the model outputs which can be easily trained with the base model.
We just need to specify how many labels the model has to predict (six in our case), since this
dictates the number of outputs the classification head has.

You will probably see a warning that some parts of the models are randomly initialized. This is
normal since the classification head has not yet been trained.
Preprocess the Tweets
In addition to the tokenization we also need to set the format of the columns to
torch.Tensor. This allows us to train the model without needing to change back and forth
between lists, arrays, and tensors. With Datasets we can use the set_format function to
change the data type of the columns we wish to keep, while dropping all the rest.

Furthermore, we define some metrics that are monitored during training. This can be any
function that takes a prediction object, that contains the model predictions as well as the correct
labels and returns a dictionary with scalar metric values. We will monitor the F -score and the
accuracy of the model.

Training the Model

Here we also set the batch size, learning rate, number of epochs, and also specify to load the
best model at the end of the training run. With this final ingredient, we can instantiate and finetune our model with the Trainer

Epoch

Training Loss

Validation Loss

Accuracy

Looking at the logs we can see that our model has an F score on the validation set of around
92% - this is a significant improvement over the feature-based approach! We can also see that
the best model was saved by running the evaluate method:

Let’s have a more detailed look at the training metrics by calculating the confusion matrix.
Visualize the Confusion Matrix
To visualise the confusion matrix, we first need to get the predictions on the validation set. The
predict function of the Trainer class returns several useful objects we can use for
evaluation.

It also contains the raw predictions for each class. We decode the predictions greedily with an
argmax. This yields the predicted label and has the same format as the labels returned by the
Scikit-Learn models in the feature-based approach.


With the predictions we can plot the confusion matrix again:

We can see that the predictions are much closer to the ideal diagonal confusion matrix. The
love category is still often confused with joy which seems natural. Furthermore, surprise
and fear are often confused and surprise is additionally frequently mistaken for joy.
Overall the performance of the model seems very good.
Also, looking at the classification report reveals that the model is also performing much better
for minority classes like surprise.

recall

f1-score

support

sadness
joy
love
anger
fear
surprise

Making Predictions
We can also use the fine-tuned model to make predictions on new tweets. First, we need to
tokenize the text, pass the tensor through the model, and extract the logits.

The model predictions are not normalized meaning that they are not a probability distribution
but the raw outputs before the softmax layer.


We can easily make the predictions a probability distribution by applying a softmax function to
them. Since we have a batch size of 1, we can get rid of the first dimension and convert the
tensor to a NumPy array for processing on the CPU.

We can see that the probabilities are now properly normalized by looking at the sum which
adds up to 1.

Error Analysis
Before moving on we should investigate our model’s prediction a little bit further. A simple, yet
powerful tool is to sort the validation samples by the model loss. When passing the label during
the forward pass, the loss is automatically calculated and returned. Below is a function that
returns the loss along with the predicted label.
following.

Wrong labels
Every process that adds labels to data can be flawed; annotators can make mistakes or
disagree, inferring labels from other features can fail. If it was easy to automatically
annotate data then we would not need a model to do it. Thus, it is normal that there are
some wrongly labeled examples. With this approach we can quickly find and correct them.
Quirks of the dataset
Datasets in the real world are always a bit messy. When working with text it can happen
that there are some special characters or strings in the inputs that throw the model off.
Inspecting the model’s weakest predictions can help identify such features, and cleaning the
data or injecting similar examples can make the model more robust.
Lets first have a look at the data samples with the highest losses.

im lazy my characters fall into categories of smug and or blas joy
people and their foils people who feel inconvenienced by
smug and or blas people

fear

i called myself pro life and voted for perry without knowing
this information i would feel betrayed but moreover i would
feel that i had betrayed god by supporting a man who
mandated a barely year old vaccine for little girls putting
them in danger to financially support people close to him

joy

sadness

i also remember feeling like all eyes were on me all the time
and not in a glamorous way and i hated it

joy

anger

im kind of embarrassed about feeling that way though
because my moms training was such a wonderfully defining
part of my own life and i loved and still love

love

sadness

i feel badly about reneging on my commitment to bring
donuts to the faithful at holy family catholic church in
columbus ohio

love

sadness

i guess i feel betrayed because i admired him so much and for joy
someone to do this to his wife and kids just goes beyond the
pale

sadness

when i noticed two spiders running on the floor in different
directions

anger

fear

id let you kill it now but as a matter of fact im not feeling
frightfully well today

joy

fear

i feel like the little dorky nerdy kid sitting in his backyard all joy
by himself listening and watching through fence to the little
popular kid having his birthday party with all his cool friends
that youve always wished were yours

We can clearly see that the model predicted some of the labels wrong. On the other hand it
seems that there are quite a few examples with no clear class which might be either mislabelled
or require an new class altogether. In particular, joy seems to be mislabelled several times. With
this information we can refine the dataset which often can lead to as much or more performance
gain as having more data or larger models!
When looking at the samples with the lowest losses, we observe that the model seems to be
most confident when predicting the sadness class. Deep learning models are exceptionally
good at finding and exploiting shortcuts to get to a prediction. A famous analogy to illustrate

This is the German horse Hans from the early 20th century. Hans was a big sensation since he
was apparently able to do simple arithmetic such as adding two numbers by tapping the result;
a skill which earned him the nickname Clever Hans. Later studies revealed that Hans was
actually not able to do arithmetic but could read the face of the questioner and determine based
on the facial expression when he reached the correct result.

Deep learning models tend to find similiar exploits if the features allow it. Imagine we build a
sentiment model to analyze customer feedback. Let’s suppose that by accident the number of
stars the customer gave are also included in the text. Instead of actually analysing the text, the
model can then simply learn to count the stars in the review. When we deploy that model in
production and it no longer has access to that information it will perform poorly and therefore
we want to avoid such situations. For this reason it is worth investing time by looking at the
examples that the model is most confident about so that we can be confident that the model
does not exploit certain features of the text.

We now know that the joy is sometimes mislabelled and that the model is most confident about
giving the label sadness. With this information we can make targeted improvements to our dataset and also keep an eye on the class the model seems to be very 
confident about. The last
step before serving the trained model is to save it for later usage. The Transformer library
allows to do this in a few steps which we show in the next section.
Saving the Model
Finally, we want to save the model so we can reuse it in another session or later if we want to
put it in production. We can save the model together with the right tokenizer in the same folder

The NLP community benefits greatly from sharing pretrained and fine-tuned models, and
everybody can share their models with others via the Hugging Face Model Hub. Through the
Hub, all community-generated models can be downloaded just like we downloaded the
DistilBert model.

Once you are logged in with your Model Hub credentials, the next step is to create a Git
repository for storing your model, tokenizer, and any other configuration files:
transformers-cli repo create distilbert-emotion

This creates a repository on the Model Hub which can be cloned and versioned like any other
Git repository. The only subtlety is that the Model Hub uses Git Large File Storage for model
versioning, so make sure you install that before cloning the repository:

Now we have saved our first model for later. This is not the end of the journey but just the first
iteration. Building performant models requires many iterations and thorough analysis and in the
next section we list a few points to get further performance gains.

Further Improvements
There are a number of things we could try to improve the feature-based model we trained this
chapter. For example, since the hidden states are just features for the model, we could include
additional features or manipulate the existing ones. The following steps could yield further
improvement and would be good exercises:
Address the class imbalance by up- or down-sampling the minority or majority classes
respectively. Alternatively, the imbalance could also be adressed in the classification
model by weighting the classes.
Add more embeddings from different models. There are many BERT-like models that
have a hidden state or output we could use such as ALBERT, GPT-2 or ELMo. You
could concatenate the tweet embedding from each model to create one large input
feature.
Apply traditional feature engineering. Besides using the embeddings from Transformer
models, we could also add features such as the length of the tweet or whether certain
emojis or hashtags are present.
Although the performance of the fine-tuned model already looks promising there are still a few
things you can try to improve it: - We used default values for the hyperparameters such as
learning rate, weight decay, and warmup steps, which work well for standard classification
tasks. However the model could still be improved with tuning them and see Chapter 5 where
we use Optuna to systematically tune the hyperparameters. - Distilled models are great for their
performance with limited computational resources. For some applications (e.g. batch-based
deployments), efficiency may not be the main concern, so you can try to improve the
performance by using the full model. To squeeze out every last bit of performance you can also
try ensembling several models. - We discovered that some labels might be wrong which is
sometimes referred to as label noise. Going back to the dataset and cleaning up the labels is an
essential step when developing NLP applications. - If label noise is a concern you can also
think about applying label smoothing.6 Smoothing out the target labels ensures that the model
does not get overconfident and draws clearer decision boundaries. Label smoothing is already
built-in the Trainer and can be controlled via the label_smoothing_factor argument.

Conclusion
Congratulations, you now know how to train a Transformer model to classify the emotions in
tweets! We have seen two complimentary approaches useing features and fine-tuning and
investigated their strengths and weaknesses. Improving either model is an open-ended
endeavour and we listed several avenues to further improve the model and the dataset.
However, this is just the first step towards building a real-world application with Transformers
so where to from here? Here’s a list of challenges you’re likely to experience along the way
that we cover in this book:
My boss wants my model in production yesterday! - In the next chapter we’ll show you
how to package our model as a web application that you can deploy and share with
your colleagues.
My users want faster predictions! - We’ve already seen in this chapter that DistilBERT
is one approach to this problem and in later chapters we’ll dive deep into how
distillation actually works, along with other tricks to speed up your Transformer
models.
Can your model also do X? - As we’ve alluded to in this chapter, Transformers are
extremely versatile and for the rest of the book we will be exploring a range of tasks
like question-answering and named entity recognition, all using the same basic
architecture.
None of my text is in English! - It turns out that Transformers also come in a
multilingual variety, and we’ll use them to tackle tasks in several languages at once.
I don’t have any labels! - Transfer learning allows you to fine-tune on few labels and
we’ll show you how they can even be used to efficiently annotate unlabeled data.
In the next chapter we’ll look at how Transformers can be used to retrieve information from
large corpora and find answers to specific questions.
