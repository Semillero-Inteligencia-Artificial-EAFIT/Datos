Capítulo 4.
Respuesta a preguntas
UNA NOTA PARA LOS LECTORES DE SALIDA TEMPRANA
Con los libros electrónicos de lanzamiento anticipado, obtiene libros en su forma más antigua: el contenido sin editar y sin editar del autor a medida que
escribir, para que pueda aprovechar estas tecnologías mucho antes del lanzamiento oficial de estos títulos.
Este será el capítulo 4 del libro final..
Tenga en cuenta que el repositorio de GitHub se activará más adelante.
Si tiene comentarios sobre cómo podríamos mejorar el contenido y/o los ejemplos de este libro, o si observa
falta material dentro de este capítulo, comuníquese con el editor en mpotter@oreilly.
com.
Ya sea investigador, analista o científico de datos, lo más probable es que haya tenido que atravesar océanos de
documentos para encontrar la información que está buscando.
Para empeorar las cosas, constantemente te recuerdan
¡Google y Bing que existen mejores formas de buscar! Por ejemplo, si buscamos “¿Cuándo ganó Marie Curie
su primer premio Nobel? en Google, obtenemos inmediatamente la respuesta correcta de "1903" como se ilustra en la Figura 4-1.
Figura 4-1.
Una consulta de búsqueda de Google y el fragmento de respuesta correspondiente.
En este ejemplo, Google primero recuperó alrededor de 319 000 documentos que eran relevantes para la consulta y luego
realizó un paso de procesamiento adicional para extraer el fragmento de respuesta con el pasaje correspondiente y la web
página.
No es difícil ver por qué estos fragmentos de respuesta son útiles.
Por ejemplo, si buscamos una pregunta más complicada
como "¿Qué país tiene más casos de COVID-19?", Google no proporciona una respuesta y en su lugar tenemos que
haga clic en una de las páginas web devueltas por el motor de búsqueda para encontrarlo nosotros mismos.
1
El enfoque general detrás de esta tecnología se llama respuesta a preguntas (QA).
Hay muchos sabores de control de calidad,
pero el más común es el control de calidad extractivo, que involucra preguntas cuya respuesta puede identificarse como un lapso de
texto en un documento, donde el documento puede ser una página web, un contrato legal o un artículo de noticias.
el de dos etapas
El proceso de recuperar primero documentos relevantes y luego extraer respuestas de ellos también es la base de muchos
modernos sistemas de control de calidad, incluidos motores de búsqueda semántica, asistentes inteligentes e información automatizada
extractores.
En este capítulo, aplicaremos este proceso para abordar un problema común que enfrentan los sitios web de comercio electrónico:
ayudar a los consumidores a responder consultas específicas para evaluar un producto.
Veremos que las opiniones de los clientes se pueden utilizar como
rica y desafiante fuente de información para control de calidad, y en el camino aprenderemos cómo los transformadores actúan como
poderosos modelos de comprensión de lectura que pueden extraer significado del texto.
Comencemos por desarrollar el uso
caso.
Este capítulo se centra en el control de calidad extractivo, pero otras formas de control de calidad pueden ser más adecuadas para su caso de uso..
Por ejemplo, control de calidad de la comunidad
implica recopilar pares de preguntas y respuestas que generan los usuarios en foros como Stack Overflow, y luego usar la similitud semántica
buscar para encontrar la respuesta más parecida a una nueva pregunta.
Sorprendentemente, también es posible hacer control de calidad sobre tablas y modelos de transformadores.
¡como TAPAS puede incluso realizar agregaciones para producir la respuesta final! También existe el control de calidad de forma larga, cuyo objetivo es generar
respuestas de un párrafo a preguntas abiertas como "¿Por qué el cielo es azul?".
Puede encontrar una demostración interactiva de control de calidad de formato largo en el
Sitio web de Hugging Face.
Creación de un sistema de control de calidad basado en revisiones
Si alguna vez compró un producto en línea, probablemente confió en las reseñas de los clientes para ayudar a informar su decisión..
Estas revisiones a menudo pueden ayudar a responder preguntas específicas como "¿Esta guitarra viene con una correa?" o “¿puedo usar esto
cámara por la noche? eso puede ser difícil de responder solo con la descripción del producto.
Sin embargo, los productos populares pueden
tener cientos o miles de reseñas, por lo que puede ser un gran lastre encontrar una que sea relevante.
Una alternativa es
publique su pregunta en las plataformas de control de calidad de la comunidad proporcionadas por sitios web como Amazon, pero generalmente toma días para
obtener una respuesta (si es que la hay).
¿No sería bueno si pudiéramos obtener una respuesta inmediata como el ejemplo de Google de
Figura 4-1? ¡Veamos si podemos hacer esto usando transformadores!

El conjunto de datos
Para construir nuestro sistema de control de calidad, utilizaremos el conjunto de datos SubjQA 2, que consta de más de 10 000 reseñas de clientes en
Inglés sobre productos y servicios en seis dominios: TripAdvisor, Restaurantes, Películas, Libros, Electrónica y
Tienda de comestibles.
Como se ilustra en la Figura 4-2, cada revisión está asociada con una pregunta que se puede responder usando uno o
más frases de la reseña.
3

Figura 4-2.
Una pregunta sobre un producto y la correspondiente reseña.
El lapso de respuesta está subrayado.
El aspecto interesante de este conjunto de datos es que la mayoría de las preguntas y respuestas son subjetivas, es decir, dependen
en la experiencia personal de los usuarios.
El ejemplo de la Figura 4-2 muestra por qué esta característica es potencialmente más
difícil que encontrar respuestas a preguntas objetivas como "¿Cuál es la moneda del Reino Unido?".
Primero el
la consulta es sobre "mala calidad", que es subjetiva y depende de la definición de calidad del usuario.
Segundo,
las partes importantes de la consulta no aparecen en absoluto en la revisión, lo que significa que no se puede responder con atajos
como búsqueda de palabras clave o parafrasear la pregunta de entrada.
Estas características hacen de SubjQA un conjunto de datos realista para
comparar nuestros modelos de control de calidad basados ​​en revisión, ya que el contenido generado por el usuario como el que se muestra en la Figura 4-2 se parece
lo que podríamos encontrar en la naturaleza.
NOTA
Los sistemas de control de calidad generalmente se clasifican por el dominio de los datos a los que tienen acceso cuando responden a una consulta..
Acuerdos de control de calidad de dominio cerrado
con preguntas sobre un tema específico (p..
gramo.
una sola categoría de producto), mientras que el dominio abierto se ocupa de preguntas sobre casi cualquier cosa (p..
gramo.
Todo el catálogo de productos de Amazon).
En general, el control de calidad de dominio cerrado implica buscar en menos documentos que el dominio abierto.
caso.
Para nuestro caso de uso, nos centraremos en crear un sistema de control de calidad para el dominio de Electrónica, así que para empezar, descarguemos
el conjunto de datos del Hugging Face Hub:
desde conjuntos de datos importar load_dataset
subjqa = load_dataset("subjqa", "electrónica")

A continuación, vamos a convertir el conjunto de datos al formato pandas para que podamos explorarlo un poco más fácilmente:

Número de preguntas en tren: 1295
Número de preguntas en la prueba: 358
Número de preguntas en validación: 255

Tenga en cuenta que el conjunto de datos es relativamente pequeño, con solo 1908 ejemplos en total.
Esto simula un escenario del mundo real,
ya que hacer que los expertos en el dominio etiqueten los conjuntos de datos de control de calidad extractivos requiere mucha mano de obra y es costoso.
por ejemplo, el
Se estima que el conjunto de datos CUAD para el control de calidad extractivo en contratos legales tiene un valor de $ 2 millones para dar cuenta de la
experiencia legal y formación de los anotadores!
Hay bastantes columnas en el conjunto de datos de SubjQA, pero las más interesantes para construir nuestro sistema de control de calidad son
se muestra en la Tabla 4-1.
El número de identificación estándar de Amazon (ASIN) asociado con cada producto

respuestas.
answer_text El lapso de texto en la revisión etiquetado por el anotador
respuestas.
answer_start El índice de caracteres de inicio del intervalo de respuestas
contexto

La revisión del cliente

Centrémonos en estas columnas y echemos un vistazo a algunos de los ejemplos de entrenamiento usando el
Marco de datos.
función de muestra para seleccionar una muestra aleatoria:
qa_cols = ["título", "pregunta", "respuestas.
texto",
"respuestas.
answer_start", "contexto"]
muestra_df = dfs["tren"][qa_cols].
muestra(2, estado_aleatorio=7)
display_df(muestra_df, índice=Falso)

me gusta mucho este teclado.
Le doy 4 estrellas porque no
tengo una tecla BLOQ MAYÚS así que nunca sé si mis mayúsculas están en mayúsculas.
Pero
por el precio, realmente es suficiente como teclado inalámbrico.
Tengo muy
manos grandes y este teclado es compacto, pero no tengo
quejas.
Compré esto después de que la primera batería gopro de repuesto que compré no lo hiciera
mantener un cargo.
Tengo expectativas muy realistas de este tipo de
producto, soy escéptico de historias asombrosas de tiempo de carga y
duración de la batería, pero espero que las baterías mantengan la carga durante un
un par de semanas por lo menos y que el cargador funcione como un
cargador.
En esto no me decepcionó..
Soy un balsero del río y
Descubrió que la GoPro consume energía rápidamente, por lo que esta
la compra soluciono ese problema.
las baterías mantuvieron una carga, en períodos más cortos
viajes, las dos baterías adicionales fueron suficientes y en viajes más largos
podría usar mis amigos JOOS Orange para recargarlos.
acabo de comprar
un newtrent xtreme powerpak y espera poder cargar estos
con eso para no volver a quedarme sin energía.
A partir de estos ejemplos podemos hacer algunas observaciones.
Primero, las preguntas no son gramaticalmente correctas, lo cual es
bastante común en las secciones de preguntas frecuentes de los sitios web de comercio electrónico.
En segundo lugar, una respuesta vacía..
la entrada de texto denota

preguntas cuya respuesta no se encuentra en el repaso.
Finalmente, podemos usar el índice de inicio y la longitud de la respuesta.
span para cortar el tramo de texto en la revisión que corresponde a la respuesta:
start_idx = sample_df["respuestas.
respuesta_inicio"].
iloc[0][0]
end_idx = start_idx + len(sample_df["respuestas.
texto"].
iloc[0][0])
muestra_df["contexto"].
iloc[0][start_idx:end_idx]
'este teclado es compacto'

A continuación, tengamos una idea de qué tipos de preguntas hay en el conjunto de entrenamiento contando las preguntas que comienzan con un
algunas palabras iniciales comunes:

Podemos ver que las preguntas que comienzan con "Cómo", "Qué" y "Es" son las más comunes, así que echemos un vistazo.
en algunos ejemplos:

¿Cómo es la cámara?
¿Qué te parece el mando?
¿Qué tan rápido es el cargador?
¿Qué es la dirección?
¿Cuál es la calidad de la construcción de la bolsa?
¿Cuál es su impresión del producto?
¿Es así como funciona el zoom?
¿El sonido es claro?
¿Es un teclado inalámbrico?

Para completar nuestro análisis exploratorio, visualicemos la distribución de reseñas asociadas con cada producto en el
conjunto de entrenamiento
Aquí vemos que la mayoría de los productos tienen una revisión, mientras que uno tiene más de cincuenta.
En la práctica, nuestro conjunto de datos etiquetados sería
un subconjunto de un corpus mucho más grande, sin etiqueta, por lo que esta distribución presumiblemente refleja las limitaciones de la
procedimiento de anotación.
Ahora que hemos explorado un poco nuestro conjunto de datos, profundicemos en la comprensión de cómo los transformadores
puede extraer respuestas del texto.
Extraer respuestas del texto
Lo primero que necesitaremos para nuestro sistema de control de calidad es encontrar una manera de identificar posibles respuestas como un tramo de texto en un
revisión del cliente.
Por ejemplo, si tenemos una pregunta como "¿Es resistente al agua?" y el pasaje de revisión es "Este
el reloj es resistente al agua a 30 m de profundidad", entonces el modelo debe mostrar "resistente al agua a 30 m".
Para hacer esto necesitaremos
Entender cómo:
Enmarcar el problema de aprendizaje supervisado.
Tokenice y codifique texto para tareas de control de calidad.
Manejar pasajes largos que excedan el tamaño máximo de contexto de un modelo.
Comencemos por echar un vistazo a cómo enmarcar el problema..
Clasificación de tramos
La forma más común de extraer respuestas del texto es formular el problema como una tarea de clasificación de intervalos, donde
los tokens de inicio y final de un intervalo de respuestas actúan como las etiquetas que un modelo necesita para predecir.
Este proceso se ilustra
en la Figura 4-3.
Figura 4-3.
El cabezal de clasificación de tramos para tareas de control de calidad.
Dado que nuestro conjunto de entrenamiento es relativamente pequeño con solo 1295 ejemplos, una buena estrategia es comenzar con un idioma
modelo que ya ha sido ajustado en un conjunto de datos de control de calidad a gran escala como el conjunto de datos de preguntas y respuestas de Stanford
(Equipo).
4 En general, estos modelos tienen fuertes capacidades de comprensión de lectura y sirven como una buena base
sobre el cual construir un sistema más preciso.
Puede encontrar una lista de modelos de control de calidad extractivos navegando a la
Abrazando Face Hub y buscando "escuadrón" en la pestaña Modelos.
Figura 4-4.
Una selección de modelos de control de calidad extractivos en Hugging Face Hub.
Como se muestra en la Figura 4-4, hay más de 180 modelos de control de calidad para elegir, entonces, ¿cuál deberíamos elegir?
Aunque la respuesta depende de varios factores, como si su corpus es monolingüe o multilingüe, y el
limitaciones de ejecutar el modelo en un entorno de producción, la Tabla 4-2 recopila algunos modelos que proporcionan una buena
base para construir.
Una versión destilada de base BERT que conserva el 99% del rendimiento mientras se
el doble de rápido

Roberta-base

Los modelos RoBERTa tienen un mejor rendimiento que sus contrapartes BERT y pueden ser
ajustado en la mayoría de los conjuntos de datos de control de calidad usando una sola GPU

ALBERT-XXL

Rendimiento de última generación en SQuAD 2.
0, pero computacionalmente intensivo y
difícil de implementar

XLM-RoBERTalarge

Modelo multilingüe para 100 idiomas con un sólido rendimiento de disparo cero

Tokenización de texto para control de calidad
Para los propósitos de este capítulo, usaremos un modelo MiniLM5 ajustado ya que es rápido de entrenar y nos permitirá
repetir rápidamente las técnicas que exploraremos.
Como de costumbre, lo primero que necesitamos es un tokenizador para codificar
nuestros textos, así que carguemos el punto de control del modelo desde Hugging Face Hub de la siguiente manera:
desde transformadores importar AutoTokenizer
model_ckpt = "deepset/minilm-uncased-squad2"
tokenizador = Autotokenizador.
from_pretrained(modelo_ckpt)

Para ver el modelo en acción, primero intentemos extraer una respuesta de un breve pasaje de texto..
En tareas extractivas de control de calidad,
las entradas se proporcionan como tuplas (pregunta, contexto), por lo que las pasamos al tokenizador de la siguiente manera:

Aquí hemos devuelto la antorcha.
Objetos tensores, ya que los necesitaremos para ejecutar el pase hacia adelante a través del modelo..
Si
vemos las entradas tokenizadas como una tabla:
también podemos ver los tensores familiares input_ids ytention_mask, mientras que el tensor token_type_ids
indica qué parte de las entradas corresponde a la pregunta y al contexto (un 0 indica un token de pregunta, un 1
indica un token de contexto).
6 Para comprender cómo el tokenizador formatea las entradas para las tareas de control de calidad, decodifiquemos el
tensor input_ids:

Vemos que para cada ejemplo de QA, las entradas toman el formato:
[CLS] tokens de pregunta [SEP] tokens de contexto [SEP]

donde la ubicación del primer token [SEP] está determinada por token_type_ids.
Ahora que nuestro texto es
tokenizado, solo necesitamos crear una instancia del modelo con un cabezal de control de calidad y ejecutar las entradas a través del paso hacia adelante:

Como se ilustra en la Figura 4-3, la cabeza QA corresponde a una capa lineal que toma los estados ocultos de la
encoder7 y calcula los logits para los tramos inicial y final.
Para convertir las salidas en un rango de respuesta, primero
necesita obtener los logits para los tokens de inicio y fin:
start_scores = salidas.
inicio_logits
end_scores = resultados.
end_logits

Como se ilustra en la Figura 4-5, el modelo otorga una puntuación a cada token de entrada, con puntuaciones positivas más altas
correspondientes a los candidatos más probables para los tokens de inicio y finalización.
En este ejemplo podemos ver que el modelo
asigna las puntuaciones de fichas de inicio más altas a los números "1" y "6000", lo que tiene sentido ya que nuestra pregunta es
preguntando por una cantidad.
Del mismo modo, vemos que los tokens finales con mayor puntuación son "minuto" y "horas".
Figura 4-5.
Logits previstos para los tokens de inicio y finalización.
La ficha con la puntuación más alta está coloreada en naranja..
Para obtener la respuesta final, podemos calcular el argmax sobre las puntuaciones del token inicial y final y luego dividir el intervalo
de las entradas.
El siguiente código realiza estos pasos y decodifica el resultado para que podamos imprimir el texto resultante:
Pregunta: ¿Cuánta música puede contener esto?
Respuesta: 6000 horas

¡Genial, funcionó! En Transformers, todos estos pasos de preprocesamiento y posprocesamiento están convenientemente envueltos
en un QuestionAnsweringPipeline dedicado.
Podemos instanciar la canalización pasando nuestro tokenizador y
modelo ajustado de la siguiente manera:

Además de la respuesta, la canalización también devuelve la estimación de probabilidad del modelo (obtenida al tomar un softmax
sobre los logits), lo cual es útil cuando queremos comparar múltiples respuestas dentro de un solo contexto.
también hemos
demostrado que el modelo puede predecir múltiples respuestas especificando el parámetro topk.
A veces, es posible
tienen preguntas para las que no hay respuesta posible, como las respuestas vacías.
Ejemplos de answer_start en SubjQA.
En estos casos, el modelo asignará un puntaje inicial y final alto al token [CLS] y la canalización mapea este
salida a una cadena vacía:

NOTA
En nuestro ejemplo simple, obtuvimos los índices inicial y final tomando el argmax de los logits correspondientes.
Sin embargo, esta heurística puede
producir respuestas fuera de alcance (e.
gramo.
puede seleccionar tokens que pertenecen a la pregunta en lugar del contexto), por lo que en la práctica la canalización
calcula la mejor combinación de índices de inicio y final sujeto a varias restricciones, como estar dentro del alcance, los índices de inicio tienen que
preceder a los índices finales y así sucesivamente.
Lidiando con pasajes largos
Una sutileza que enfrentan los modelos de comprensión de lectura es que el contexto a menudo contiene más tokens que el
longitud máxima de secuencia del modelo, que suele ser de unos pocos cientos de tokens como máximo.
Como se ilustra en la Figura 46, una parte decente del conjunto de entrenamiento de SubjQA contiene pares de pregunta-contexto que no encajarán dentro del modelo.
contexto.
Figura 4-6.
Distribución de tokens para cada par pregunta-contexto en el conjunto de entrenamiento SubjQA.
Para otras tareas como la clasificación de texto, simplemente truncamos textos largos bajo el supuesto de que suficiente
la información estaba contenida en la incrustación del token [CLS] para generar predicciones precisas.
para control de calidad
sin embargo, esta estrategia es problemática porque la respuesta a una pregunta podría estar cerca del final del contexto y
sería eliminado por truncamiento.
Como se ilustra en la Figura 4-7, la forma estándar de lidiar con esto es aplicar un
ventana deslizante a través de las entradas, donde cada ventana contiene un pasaje de tokens que encajan en el contexto del modelo.
Figura 4-7.
Cómo la ventana deslizante crea múltiples pares de pregunta-contexto para documentos largos.
En Transformers, la ventana deslizante se habilita configurando return_overflowing_tokens=True en el
tokenizer, con el tamaño de la ventana deslizante controlado por el argumento max_seq_length y el tamaño de la
zancada controlada por doc_stride.
Tomemos el primer ejemplo de nuestro conjunto de entrenamiento y definamos una pequeña ventana para
ilustrar cómo funciona esto:
ejemplo = dfs["tren"].
iloc[0][["pregunta", "contexto"]]
tokenized_example = tokenizer(ejemplo["pregunta"], ejemplo["contexto"],
return_overflowing_tokens=Verdadero, max_length=100,
zancada = 25)

En este caso, ahora obtenemos una lista de input_ids, uno para cada ventana.
Verifiquemos la cantidad de tokens que tenemos en
cada ventana:

Finalmente, podemos ver dónde se superponen dos ventanas al decodificar las entradas:
[CLS] ¿Cómo está el bajo? [SEP] y no te sientas pesado ni te presiones las orejas ni siquiera
> después de escuchar música con ellos todo el día.
el sonido es noche y dia
> mejor que cualquier auricular podría ser y es casi tan bueno como el pro 4aa.
> son auriculares "abiertos" por lo que no se puede hacer coincidir el bajo con el sellado
> tipos, pero se acerca.
por $ 32, no te puedes equivocar.
[SEP]

Ahora que tenemos cierta intuición acerca de cómo los modelos de control de calidad pueden extraer respuestas del texto, veamos el otro
componentes que necesitamos para construir una canalización de control de calidad de extremo a extremo.
EL CONJUNTO DE DATOS DE RESPUESTAS A LAS PREGUNTAS DE STANFORD
El formato (pregunta, revisión, [frases de respuesta]) de SubjQA se usa comúnmente en conjuntos de datos de control de calidad extractivos y
fue pionero en SQuAD, que es un famoso conjunto de datos utilizado para probar la capacidad de las máquinas para leer un pasaje de
enviar un mensaje de texto y responder preguntas al respecto.
El conjunto de datos se creó tomando muestras de varios cientos de artículos en inglés de
Wikipedia, dividiendo cada artículo en párrafos y luego pidiendo a los trabajadores colaborativos que generen un conjunto de
preguntas y respuestas para cada párrafo.
En la primera versión de SQuAD, cada respuesta a una pregunta se
garantizado que exista en el pasaje correspondiente y no pasó mucho tiempo antes de que los modelos de secuencia funcionaran mejor
que los humanos a la hora de extraer la extensión correcta del texto con la respuesta.
Para hacer la tarea más difícil, SQuAD
2.
08 fue creado aumentando SQuAD 1.
1 con un conjunto de preguntas contradictorias que son relevantes para un determinado
pasaje, pero no se puede responder solo con el texto.
Al momento de escribir este libro, el estado del arte se muestra en
Figura 4-8, con la mayoría de los modelos desde 2019 superando el rendimiento humano.
Figura 4-8.
Progreso en el SQuAD 2.
0 punto de referencia.
Imagen de Papeles con código

Sin embargo, esta actuación sobrehumana no parece reflejar una comprensión lectora genuina ya que el
las respuestas sin respuesta se pueden identificar a través de patrones en los pasajes como antónimos.
Para resolver estos
problemas, Google lanzó el conjunto de datos de Preguntas naturales (NQ)9 que involucra preguntas de búsqueda de hechos
obtenido de los usuarios de la Búsqueda de Google.
Las respuestas en NQ son mucho más largas que en SQuAD y presentan un
punto de referencia desafiante.
Uso de Haystack para construir una canalización de control de calidad
En nuestro ejemplo de extracción de respuesta simple, proporcionamos tanto la pregunta como el contexto al modelo..
Sin embargo, en
En realidad, los usuarios de nuestro sistema solo proporcionarán una pregunta sobre un producto, por lo que necesitamos alguna forma de seleccionar
pasajes de entre todas las reseñas de nuestro corpus.
Una forma de hacerlo sería concatenar todas las reseñas de
un producto dado juntos y alimentarlos al modelo como un solo contexto largo.
Aunque simple, el inconveniente de
este enfoque es que el contexto puede llegar a ser extremadamente largo y por lo tanto introducir una latencia inaceptable para nuestro
consultas de los usuarios.
Por ejemplo, supongamos que, en promedio, cada producto tiene 30 revisiones y cada revisión toma 100
milisegundos para procesar.
Si necesitamos procesar todas las revisiones para obtener una respuesta, esto daría una latencia promedio
de tres segundos por consulta de usuario: ¡demasiado tiempo para los sitios web de comercio electrónico!
Para manejar esto, los sistemas modernos de control de calidad generalmente se basan en la arquitectura Retriever-Reader, que tiene dos principales
componentes:
Perdiguero

Responsable de recuperar documentos relevantes para una consulta dada.
Los perros perdigueros generalmente se clasifican como escasos
o denso.
Sparse Retrievers utilizan representaciones vectoriales dispersas de los documentos para medir qué términos coinciden
con una consulta.
Los Dense Retrievers usan codificadores como transformadores o LSTM para codificar una consulta y un documento en
dos vectores respectivos de idéntica longitud.
La relevancia de una consulta y un documento es entonces determinada por
calculando un producto interno de los vectores.
Lector
Responsable de extraer una respuesta de los documentos proporcionados por el Retriever.
El lector suele ser un
modelo de comprensión lectora, aunque al final del capítulo veremos ejemplos de modelos que pueden
generar respuestas de forma libre.
Como se ilustra en la Figura 4-9, también puede haber otros componentes que apliquen el procesamiento posterior a los documentos.
recuperada por el Retriever o a las respuestas extraídas por el Lector.
Por ejemplo, los documentos recuperados pueden necesitar
reclasificación para eliminar los ruidosos o irrelevantes que pueden confundir al Lector.
Del mismo modo, el procesamiento posterior de la
Las respuestas del lector a menudo se necesitan cuando la respuesta correcta proviene de varios pasajes en un documento largo.
Figura 4-9.
La arquitectura Retriever-Reader para sistemas de control de calidad modernos.
Para construir nuestro sistema de control de calidad, usaremos la biblioteca Haystack desarrollada por deepset, una empresa alemana.
enfocado en PNL.
La ventaja de usar Haystack es que se basa en la arquitectura Retriever-Reader, resúmenes
gran parte de la complejidad involucrada en la construcción de estos sistemas, y se integra estrechamente con Transformers.
Puede
instala Haystack con el siguiente comando pip:
pip install granja-pajar

Además de Retriever y Reader, hay dos componentes más involucrados al construir una canalización de control de calidad
con Pajar:
Almacén de documentos
Una base de datos orientada a documentos que almacena documentos y metadatos que se proporcionan al Retriever en la consulta
tiempo.
Tubería
Combina todos los componentes de un sistema de control de calidad para permitir flujos de consulta personalizados, fusionando documentos de múltiples
Retrievers y más.
En esta sección, veremos cómo podemos usar estos componentes para crear rápidamente un canal de control de calidad prototipo, y más adelante
examinar cómo podemos mejorar su rendimiento.
Inicialización de un almacén de documentos
En Haystack, hay varios almacenes de documentos para elegir y cada uno se puede combinar con un conjunto dedicado de
perros perdigueros.
Esto se ilustra en la Tabla 4-3, donde la compatibilidad de dispersa (TF-IDF, BM25) y densa
(Incrustación, DPR) Recuperadores se muestra para cada uno de los almacenes de documentos disponibles.
Dado que en este capítulo exploraremos tanto los perros perdigueros dispersos como los densos, utilizaremos el
ElasticsearchDocumentStore que es compatible con ambos tipos de recuperadores.
Elasticsearch es una búsqueda
motor que es capaz de manejar una amplia gama de datos, incluidos textuales, numéricos, geoespaciales, estructurados y
desestructurado.
Su capacidad para almacenar grandes volúmenes de datos y filtrarlos rápidamente con funciones de búsqueda de texto completo lo hace
especialmente adecuado para desarrollar sistemas de control de calidad.
También tiene la ventaja de ser el estándar de la industria para
análisis de infraestructura, por lo que es muy probable que su empresa ya tenga un clúster con el que pueda trabajar.
Para inicializar el almacén de documentos, primero debemos descargar e instalar Elasticsearch.
Siguiendo las instrucciones de Elasticsearch
guía, tomemos la última versión para Linux10 con wget y

A continuación, debemos iniciar el servidor Elasticsearch..
Dado que estamos ejecutando todo el código de este libro dentro de Jupyter
cuadernos, necesitaremos usar el subproceso de Python.
Módulo Popen para generar un nuevo proceso.
Mientras estamos en eso,
también ejecutemos el subproceso en segundo plano usando el comando chown shell:

En el módulo Popen, los argumentos especifican el programa que deseamos ejecutar, mientras que stdout=PIPE crea un nuevo
tubería para la salida estándar, y stderr=STDOUT recopila los errores en la misma tubería.
El preexec_fn
argumento especifica el ID del subproceso que deseamos usar.
De forma predeterminada, Elasticsearch se ejecuta localmente en el puerto 9200, por lo que
podemos probar la conexión enviando una solicitud HTTP a
Ahora que nuestro servidor de Elasticsearch está en funcionamiento, lo siguiente que debe hacer es crear una instancia del almacén de documentos:

De forma predeterminada, ElasticsearchDocumentStore crea dos índices en Elasticsearch: uno llamado documento para
(lo adivinó) almacenar documentos, y otra etiqueta llamada para almacenar los intervalos de respuesta anotados.
Por ahora,
simplemente completaremos el índice de documentos con las revisiones de SubjQA, y las tiendas de documentos de Haystack esperan una lista
de diccionarios con texto y claves meta de la siguiente manera:

Los campos en meta se pueden usar para aplicar filtros durante la recuperación, por lo que para nuestros propósitos incluiremos el
columnas item_id y q_review_id de SubjQA para que podamos filtrar por producto e ID de pregunta, junto con el
división de entrenamiento correspondiente.
Luego podemos recorrer los ejemplos en cada DataFrame y agregarlos al
índice con la función write_documents de la siguiente manera:
para dividir, df en dfs.
elementos():

¡Genial, hemos cargado todas nuestras reseñas en un índice! Para buscar en el índice necesitaremos un Retriever, así que veamos cómo
podemos inicializar uno para Elasticsearch.
Inicializar un recuperador

El almacén de documentos de Elasticsearch se puede emparejar con cualquiera de los recuperadores de Haystack, así que comencemos usando un escaso
retriever basado en BM25 (abreviatura de "Best Match 25").
BM25 es una versión mejorada de la clásica métrica TF-IDF
y representa la pregunta y el contexto como vectores dispersos que se pueden buscar de manera eficiente en Elasticsearch.
El
La puntuación BM25 mide cuánto texto coincidente se trata de una consulta de búsqueda y mejora en TF-IDF al saturar TF
valores rápidamente y normalizando la longitud del documento para que los documentos cortos se vean favorecidos sobre los largos.
11
En Haystack, el BM25 Retriever está incluido en ElasticsearchRetriever, así que vamos a inicializar esta clase
especificando el almacén de documentos que deseamos buscar:
del pajar.
perdiguero.
importación escasa ElasticsearchRetriever
es_retriever = ElasticsearchRetriever(document_store=document_store)

A continuación, veamos una consulta simple para un solo producto electrónico en el conjunto de entrenamiento.
Para sistemas de control de calidad basados ​​en revisión
como el nuestro, es importante restringir las consultas a un solo elemento porque, de lo contrario, el Retriever obtendría
opiniones sobre productos que no están relacionados con la consulta de un usuario.
Por ejemplo, preguntar "¿La calidad de la cámara es
¿bien?" sin un filtro de producto podría devolver reseñas sobre teléfonos, cuando el usuario podría estar preguntando sobre un producto específico
cámara portátil en su lugar.
Por sí mismos, los valores ASIN en nuestro conjunto de datos son un poco crípticos, pero podemos descifrarlos
con herramientas en línea como amazon ASIN o simplemente agregando el valor de item_id a www.
Amazonas.
com/dp/
URL.
El ID del artículo a continuación corresponde a una de las tabletas Fire de Amazon, así que usemos la recuperación del Retriever
función para preguntar si es bueno para leer con:

Aquí especificamos cuántos documentos devolver con el argumento top_k y aplicamos un filtro tanto en el
item_id y claves divididas que se incluyeron en el campo meta de nuestros documentos.
cada elemento de
retrieved_docs es un objeto Haystack Document que se utiliza para representar documentos e incluye el
Puntuación de consulta de Retriever junto con otros metadatos.
Echemos un vistazo a uno de los documentos recuperados:
docs_recuperados[0]

Además del texto del documento, podemos ver la puntuación que Elasticsearch calculó por su relevancia para el
consulta (las puntuaciones más altas implican una mejor coincidencia).
Bajo el capó, Elasticsearch se basa en Lucene para la indexación y la búsqueda,
por lo que por defecto utiliza la práctica función de puntuación de Lucene.
Puede encontrar los detalles esenciales detrás de la puntuación.
función en la documentación de Elasticsearch, pero en términos breves, la función de puntuación primero filtra el candidato
documentos aplicando una prueba booleana (¿coincide el documento con la consulta?), y luego aplica una métrica de similitud
que se basa en representar tanto el documento como la consulta como vectores.
Ahora que tenemos una forma de recuperar documentos relevantes, lo siguiente que necesitamos es una forma de extraer respuestas de
a ellos.
Aquí es donde entra en juego el Lector, así que echemos un vistazo a cómo podemos cargar nuestro modelo MiniLM en Haystack.
Inicializar un lector

En Haystack, hay dos tipos de lectores que se pueden usar para extraer respuestas de un contexto dado:
FARMReader
Basado en el marco FARM de deepset para ajustar e implementar transformadores.
Compatible con modelos
entrenado usando Transformers y puede cargar modelos directamente desde Hugging Face Hub.
Lector de transformadores
Basado en QuestionAnsweringPipeline de Transformers.
Adecuado solo para ejecutar inferencia.
Aunque ambos lectores manejan los pesos de un modelo de la misma manera, existen algunas diferencias en la forma en que el
Las predicciones se convierten para producir respuestas:
En Transformers, QuestionAnsweringPipeline normaliza los logits inicial y final con un
softmax en cada pasaje.
Esto significa que solo tiene sentido comparar puntuaciones de respuestas entre
respuestas extraídas del mismo pasaje, donde las probabilidades suman uno.
Por ejemplo, una respuesta
puntuación de 0.
9 de un pasaje no es necesariamente mejor que una puntuación de 0.
8 en otro.
En FARM, los logits
no están normalizados, por lo que las respuestas entre pasajes se pueden comparar más fácilmente.
El TransformersReader a veces predice la misma respuesta dos veces, pero con puntuaciones diferentes.
Este
puede ocurrir en contextos largos si la respuesta se encuentra en dos ventanas superpuestas.
En FARM, estos
se eliminan los duplicados.
Dado que ajustaremos el Reader más adelante en este capítulo, usaremos el FARMReader.
Similar a los transformadores,
para cargar el modelo, solo necesitamos especificar el punto de control MiniLM en Hugging Face Hub junto con algunos argumentos específicos de control de calidad:
del pajar.
lector.
granja importación FARMReader
model_ckpt = "deepset/minilm-uncased-squad2"
max_seq_length, doc_zancada = 384, 128
lector = FARMReader(modelo_nombre_o_ruta=modelo_ckpt, barra_progreso=Falso,
max_seq_len=max_seq_length, doc_stride=doc_stride,
return_no_answer=Verdadero)

NOTA
También es posible ajustar un modelo de comprensión de lectura directamente en Transformers y luego cargarlo en TransformersReader para ejecutarlo.
inferencia.
Para obtener detalles sobre cómo realizar el paso de ajuste fino, consulte el tutorial de preguntas y respuestas en la Gran tabla de tareas de Transformers..
En FARMReader, el comportamiento de la ventana deslizante está controlado por el mismo max_seq_length y
argumentos doc_stride que vimos para el tokenizador, y hemos usado los valores del documento MiniLM.
Como un
control de cordura, ahora probemos el Lector en nuestro ejemplo simple de antes:
Genial, el Lector parece estar funcionando como se esperaba, así que a continuación, unamos todos nuestros componentes usando uno de
Los oleoductos de Haystack.
Poniendolo todo junto
Haystack proporciona una abstracción de Pipeline que nos permite combinar Retrievers, Readers y otros componentes
juntos como un gráfico que se puede personalizar fácilmente para cada caso de uso.
También hay tuberías predefinidas análogas
a los de Transformers, pero especializados en sistemas de control de calidad.
En nuestro caso, estamos interesados ​​en extraer respuestas para que
Usaremos el ExtractiveQAPipeline que toma un solo par Retriever-Reader como sus argumentos:

Cada tubería tiene una función de ejecución que especifica cómo se debe ejecutar el flujo de consulta.
Para
ExtractiveQAPipeline solo necesitamos pasar la consulta, la cantidad de documentos para recuperar con
top_k_retriever y número de respuestas para extraer de estos documentos con top_k_reader.
En nuestro
En este caso, también necesitamos especificar un filtro sobre el ID del elemento, lo que se puede hacer usando el argumento de filtros como lo hicimos nosotros.
con el Retriever antes.
Ejecutemos un ejemplo simple usando nuestra pregunta sobre la tableta Amazon Fire nuevamente, pero
esta vez devolviendo las respuestas extraídas:

Genial, ¡ahora tenemos un sistema de control de calidad de extremo a extremo para las revisiones de productos de Amazon! Este es un buen comienzo, pero tenga en cuenta que el
la segunda y la tercera respuesta están más cerca de lo que realmente está preguntando la pregunta.
Para hacerlo mejor, primero necesitaremos algunos
métricas para cuantificar el rendimiento del Retriever y Reader.
Vamos a ver.
Mejorando nuestra canalización de control de calidad
Aunque gran parte de la investigación reciente sobre control de calidad se ha centrado en mejorar los modelos de comprensión lectora, en la práctica
¡no importa cuán bueno sea su Reader si el Retriever no puede encontrar los documentos relevantes en primer lugar! En
En particular, el Retriever establece un límite superior en el rendimiento de todo el sistema de control de calidad, por lo que es importante

asegúrese de que está haciendo un buen trabajo.
Con esto en mente, comencemos introduciendo métricas para evaluar el Retriever y
comparar el rendimiento de representaciones dispersas y densas.
Evaluación del Retriever
Una métrica común para evaluar los Retrievers es el recuerdo, que mide la fracción de todos los documentos relevantes que
se recuperan.
En este contexto, relevante simplemente significa si la respuesta está presente en un pasaje de texto o no, por lo que
dado un conjunto de preguntas, podemos calcular el recuerdo contando el número de veces que aparece una respuesta en la parte superior-k
documentos devueltos por el Retriever.
NOTA
Una métrica complementaria para recordar es la precisión promedio media (mAP), que recompensa a los Retrievers que pueden ubicar las respuestas correctas más arriba
en el ranking de documentos.
En Haystack hay dos formas de evaluar los Retrievers:
Use la función de evaluación incorporada del Retriever.
Esto se puede usar para el control de calidad de dominio abierto y cerrado, pero
no para conjuntos de datos como SubjQA donde cada documento se empareja con un solo producto y necesitamos filtrar por
ID de producto para cada consulta.
Cree una canalización personalizada que combine un Retriever con la clase EvalRetriever.
Esto permite que el
posibilidad de implementar métricas personalizadas y flujos de consulta.
Dado que necesitamos evaluar el retiro por producto y luego agregar todos los productos, optaremos por el segundo
acercarse.
Cada nodo en el gráfico Pipeline representa una clase que toma algunas entradas y produce algunas salidas
a través de una función de ejecución:
clase PipelineNode:

Aquí kwargs corresponde a las salidas del nodo anterior en el gráfico, que se manipula dentro de ejecutar para
devolver una tupla de las salidas para el siguiente nodo, junto con un nombre para el borde saliente.
El único otro
requisito es incluir un atributo outgoing_edge que indique el número de salidas del nodo (en
la mayoría de los casos outgoing_edge=1 a menos que tenga ramas en la canalización que enruten las entradas de acuerdo con algún
criterio).
En nuestro caso, necesitamos un nodo para evaluar el Retriever, por lo que usaremos la clase EvalRetriever cuya ejecución
la función realiza un seguimiento de qué documentos tienen respuestas que coinciden con la verdad básica.
Con esta clase podemos entonces
construya un gráfico Pipeline agregando el nodo de evaluación después de un nodo que represente al propio Retriever:

Observe que a cada nodo se le da un nombre y una lista de entradas.
En la mayoría de los casos, cada nodo tiene un solo borde saliente,
entonces solo necesitamos incluir el nombre del nodo anterior en las entradas.
Ahora que tenemos nuestro canal de evaluación, necesitamos pasar algunas consultas y sus correspondientes respuestas..
Hacer
esto, agregaremos las respuestas a un índice de etiquetas dedicado en nuestro almacén de documentos.
Haystack proporciona un objeto Label
que representa los tramos de respuesta y sus metadatos de forma estandarizada.
Para llenar el índice de la etiqueta, vamos a
primero cree una lista de objetos Labels recorriendo cada pregunta en el conjunto de prueba y extrayendo la coincidencia
respuestas y metadatos adicionales:

Si nos asomamos a una de estas etiquetas

podemos ver el par pregunta-respuesta junto con un campo de origen que contiene el ID de pregunta único para que podamos
filtrar el almacén de documentos por pregunta.
También agregamos el ID del producto al campo model_id para que podamos filtrar el
etiquetas por producto.
Ahora que tenemos nuestras etiquetas, podemos escribirlas en el índice de etiquetas en Elasticsearch como
sigue:

A continuación, debemos crear un mapeo entre nuestros ID de preguntas y las respuestas correspondientes que podemos pasar al
tubería.
Para obtener todas las etiquetas, podemos usar la función get_all_labels_aggregated del documento
tienda que agregará todos los pares de preguntas y respuestas asociados con una identificación única.
Esta función devuelve una lista de
objetos MultiLabel, pero en nuestro caso solo obtenemos un elemento ya que estamos filtrando por ID de pregunta, por lo que podemos
construya una lista de etiquetas agregadas de la siguiente manera:

Al mirar una de estas etiquetas, podemos ver que todas las respuestas asociadas con una pregunta determinada se agregan
juntos en un campo multiple_answers:
etiquetas_agg[14]

Dado que pronto evaluaremos tanto el Retriever como el Reader en la misma ejecución, debemos proporcionar las etiquetas doradas para
ambos componentes en el Pipeline.
ejecutar función.
La forma más sencilla de lograr esto es creando un diccionario.
que mapea el ID de pregunta único con un diccionario de etiquetas, una para cada componente:
qid2etiqueta = {l.
origen: {"recuperador": l, "lector": l} para l en etiquetas_agg}

Ahora tenemos todos los ingredientes para evaluar el Retriever, así que definamos una función que alimente cada par de preguntas y respuestas asociado con cada producto a la tubería de evaluación y rastree las recuperaciones correctas en nuestra tubería.
objeto:

¡Genial, funciona! Tenga en cuenta que elegimos un valor específico para top_k_retriever para especificar el número de
documentos para recuperar.
En general, aumentar este parámetro mejorará el recuerdo, pero a expensas de proporcionar
más documentos al Lector y ralentizando la canalización de un extremo a otro.
Para guiar nuestra decisión sobre qué valor
elegir, crearemos una función que recorra varios valores de k y calcule la recuperación en todo el conjunto de prueba para
cada k:

Si graficamos los resultados, podemos ver cómo mejora el recuerdo a medida que aumentamos k:

promedio cada producto tiene tres reseñas, por lo que
devolver cinco o más documentos significa que es muy probable que obtengamos el contexto correcto.
Recuperación de pasajes densos
Hemos visto que obtenemos un recuerdo casi perfecto cuando nuestro Retriever disperso devuelve k = 10 documentos, pero ¿podemos hacerlo?
mejor en valores más pequeños de k? La ventaja de hacerlo es que podemos pasar menos documentos al Lector y
por lo tanto, reducir la latencia general de nuestra canalización de control de calidad.
Una limitación bien conocida de los Retrievers dispersos como BM25
es que pueden fallar al capturar los documentos relevantes si la consulta del usuario contiene términos que no coinciden exactamente
los de la reseña.
Una alternativa prometedora es usar incrustaciones densas para representar la pregunta y documentar
y el estado del arte actual es una arquitectura conocida como Dense Passage Retrieval (DPR).
12 La idea principal
detrás de DPR es usar dos modelos BERT como codificadores E (⋅) y E (⋅) para la pregunta y el pasaje.
Como se ilustra
en la Figura 4-10, estos codificadores asignan el texto de entrada a una representación vectorial de dimensión d del token [CLS].
Figura 4-10.
Arquitectura de dos codificadores de DPR para calcular la relevancia de un documento y una consulta.
En Haystack, podemos inicializar un Retriever para DPR de forma similar a como lo hicimos para BM25.
Además de especificar la
almacén de documentos, también debemos elegir los codificadores BERT para la pregunta y el pasaje.
Estos codificadores están entrenados
dándoles preguntas con pasajes relevantes (positivos) y pasajes irrelevantes (negativos), donde el objetivo es

aprender que los pares de preguntas y pasajes relevantes tienen una mayor similitud.
Para nuestro caso de uso, usaremos codificadores que tienen
ha sido ajustado en el corpus NQ de esta manera:

Aquí también establecimos embed_title=False desde que concatenamos el título del documento (i.
mi.
item_id) no
proporcione cualquier información adicional porque filtramos por producto.
Una vez que hemos inicializado el Retriever denso, el
El siguiente paso es iterar sobre todos los documentos indexados en nuestro índice de Elasticseach y aplicar los codificadores para actualizar el
representación incrustada.
Esto puede hacerse de la siguiente manera:
document_store.
actualizar_embeddings(recuperador=dpr_retriever)

¡Ya estamos listos para empezar! Podemos evaluar el Retriever denso de la misma manera que lo hicimos para el BM25 y comparar el retiro del topk:

Aquí podemos ver que DPR no proporciona un impulso en la recuperación sobre

La búsqueda de similitudes de las incrustaciones se puede acelerar utilizando la biblioteca FAISS de Facebook como almacén de documentos..
Del mismo modo, el
el rendimiento del DPR Retriever se puede mejorar ajustando el dominio de destino.
Ahora que hemos explorado la evaluación del Retriever, pasemos a evaluar el Lector.
Evaluación del lector
En el control de calidad extractivo, hay dos métricas principales que se utilizan para evaluar a los lectores:

puntaje
Encontramos esta métrica en el Capítulo 2 y mide la media armónica de la precisión y recuperación.
Veamos cómo funcionan estas métricas importando algunas funciones auxiliares de FARM y aplicándolas a un simple
ejemplo:
de la granja.
evaluación.
escuadrón_evaluación importar computar_f1, computar_exacta

Bajo el capó, estas funciones primero normalizan la predicción y la etiqueta eliminando la puntuación, corrigiendo
espacios en blanco y conversión a minúsculas.
Las cadenas normalizadas luego se tokenizan como una bolsa de palabras, antes
finalmente calculando la métrica a nivel de token.
De este simple ejemplo podemos ver que EM es mucho más estricto
métrica que el puntaje F1: agregar un solo token a la predicción da un EM de cero.
Por otro lado, la F1
la puntuación puede fallar en la captura de respuestas verdaderamente incorrectas.
Por ejemplo, supongamos que nuestro intervalo de respuestas pronosticado fue "alrededor de 6000

Por lo tanto, confiar solo en la puntuación F1 es engañoso, y el seguimiento de ambas métricas es una buena estrategia para equilibrar el equilibrio entre subestimar (EM) y sobrestimar (puntuación F1) el rendimiento del modelo..
Ahora, en general, hay varias respuestas válidas por pregunta, por lo que estas métricas se calculan para cada par de preguntas y respuestas en el conjunto de evaluación, y se selecciona la mejor puntuación entre todas las respuestas posibles..
El EM general y F
Luego, las puntuaciones para el modelo se obtienen promediando las puntuaciones individuales de cada par de preguntas y respuestas..
Para evaluar el Lector, crearemos una nueva canalización con dos nodos: un nodo Lector y un nodo para evaluar el
Lector.
Usaremos la clase EvalReader que toma las predicciones del Reader y calcula el
puntuaciones EM y F correspondientes.
Para comparar con la evaluación de SQuAD, tomaremos las mejores respuestas para cada
consulta con las métricas top_1_em y top_1_f1 que están almacenadas en EvalReader:

Observe que especificamos skip_incorrect_retrieval=False; esto es necesario para asegurar que el Retriever
siempre pasa el contexto al Lector (como se hizo en la evaluación SQuAD).
Ahora que hemos repasado cada
pregunta a través del lector, imprimamos las puntuaciones:

De acuerdo, parece que el modelo ajustado funciona significativamente peor en SubjQA que en SQuAD 2.
0, donde
MiniLM logra una puntuación EM y F de 76.
1 y 79.
5 respectivamente.
Una de las razones de la caída del rendimiento es que
las reseñas de los clientes son un dominio bastante diferente de Wikipedia (donde SQuAD 2.
0 se genera a partir de), y el
el lenguaje es a menudo bastante informal.
Es probable que otra razón se deba a la subjetividad inherente de nuestro conjunto de datos, donde
tanto las preguntas como las respuestas difieren de la información fáctica contenida en Wikipedia.
Echemos un vistazo a cómo
podemos ajustar estos modelos en un conjunto de datos para obtener mejores resultados con la adaptación del dominio.
Adaptación de dominio
Aunque los modelos que están ajustados en SQuAD a menudo se generalizarán bien a otros dominios, hemos visto que para
SubjQA que los puntajes EM y F se reducen a más de la mitad en comparación con el conjunto de validación SQuAD.
Este fracaso para
generalize también se ha observado en otros conjuntos de datos de control de calidad extractivos13 y se entiende como evidencia de que el transformador
los modelos son particularmente hábiles para adaptarse a SQuAD.
La forma más directa de mejorar el Lector es
afinando nuestro modelo MiniLM aún más en el conjunto de entrenamiento SubjQA.
FARMReader tiene un método de entrenamiento que
está diseñado para este propósito y espera que los datos estén en formato SQuAD JSON, donde todas las preguntas y respuestas
los pares se agrupan para cada artículo como se ilustra en la Figura 4-11.
Figura 4-11.
Visualización del formato SQuAD JSON.
Puede descargar los datos preprocesados ​​del repositorio de GitHub del libro AGREGAR ENLACE.
Ahora que tenemos la
divisiones en el formato correcto, ajustemos nuestro Reader especificando la ubicación de las divisiones de tren y desarrollo, junto con
la ubicación de dónde guardar el modelo ajustado:

¡Guau, la adaptación del dominio ha aumentado nuestra puntuación EM en un factor de seis y más del doble de la puntuación F!
Sin embargo, es posible que se pregunte por qué no ajustamos un modelo de lenguaje preentrenado directamente en la capacitación de SubjQA.
¿colocar? Una respuesta es que solo tenemos 1295 ejemplos de capacitación en SubjQA, mientras que SQuAD tiene más de 100 000, por lo que
puede encontrarse con desafíos con el sobreajuste.
Sin embargo, echemos un vistazo a lo que produce el ajuste fino ingenuo.
Para
comparación justa, usaremos el mismo modelo de lenguaje que se usó para ajustar nuestra línea de base en SQuAD.
Como
antes, cargaremos el modelo con FARMReader:

ADVERTENCIA
Cuando se trata de conjuntos de datos pequeños, la mejor práctica es utilizar la validación cruzada al evaluar los transformadores, ya que pueden ser propensos al sobreajuste..
Puede encontrar un ejemplo de cómo realizar una validación cruzada con conjuntos de datos con formato SQuAD en el repositorio FARM.
Evaluación de toda la canalización de control de calidad
Ahora que hemos visto cómo evaluar los componentes Reader y Retriever individualmente, vinculémoslos para
Medir el rendimiento general de nuestro pipeline..
Para hacerlo, necesitaremos aumentar nuestra canalización de Retriever con
nodos para el Reader y su evaluación.
Hemos visto que obtenemos un recuerdo casi perfecto en k = 10, por lo que podemos arreglar esto
valore y evalúe el impacto que esto tiene en el rendimiento del Lector (ya que ahora recibirá múltiples contextos por
consulta en comparación con la evaluación de estilo SQuAD).
A continuación, podemos comparar las puntuaciones EM y F de los primeros 1 y 3 primeros del modelo para predecir una respuesta en los documentos.
devuelto por el Retriever:

Figura 4-12.
Comparación de las puntuaciones de EM y F1 para Reader con todo el proceso de control de calidad

A partir de este gráfico podemos ver el efecto que tiene el Retriever en el rendimiento general.
En particular, hay un
degradación general del rendimiento en comparación con la coincidencia de los pares pregunta-contexto como se hace en la evaluación SQuADstyle.
Esto se puede eludir aumentando el número de posibles respuestas que el Lector está
permitió predecir.
Hasta ahora solo hemos extraído intervalos de respuesta del contexto, pero en general podría ser que
fragmentos de la respuesta están dispersos a lo largo del documento y nos gustaría que nuestro modelo sintetice
estos fragmentos en una sola respuesta coherente.
Veamos cómo podemos usar el control de calidad generativo para tener éxito en esto
tarea.
Ir más allá del control de calidad extractivo
Una alternativa interesante a la extracción de respuestas como fragmentos de texto en un documento es generarlas con un
modelo de lenguaje preentrenado.
Este enfoque a menudo se conoce como control de calidad abstracto o generativo y tiene el potencial
para producir respuestas mejor redactadas que sinteticen evidencia a través de múltiples pasajes.
Aunque menos maduro que
control de calidad extractivo, este es un campo de investigación en rápido movimiento, por lo que es probable que estos enfoques sean ampliamente adoptados
en la industria para cuando estés leyendo esto! En esta sección tocaremos brevemente el estado del arte actual: Recuperación
Generación Aumentada (RAG).
14

Recuperación Generación Aumentada

RAG amplía la arquitectura clásica Retriever-Reader que hemos visto en este capítulo al cambiar el Reader por
un generador y usando DPR como el recuperador.
El Generador es un transformador de secuencia a secuencia preentrenado como
T5 o BART que recibe vectores latentes de documentos de DPR y luego genera iterativamente una respuesta basada
sobre la consulta y estos documentos.
Dado que DPR y el Generador son diferenciables, todo el proceso se puede ajustar de principio a fin, como se ilustra en la Figura 4-13..
Puede encontrar una demostración interactiva de RAG en Hugging Face
sitio web.
Figura 4-13.
La arquitectura RAG para ajustar un Retriever y un Generador de principio a fin (cortesía de Ethan Perez).
Para mostrar RAG en acción, usaremos el DPRetriever de antes, por lo que solo necesitamos instanciar un Generador.
Hay dos tipos de modelos RAG para elegir:
Secuencia RAG
Utiliza el mismo documento recuperado para generar la respuesta completa.
En particular, los documentos top-k de la
Retriever se alimentan al Generador que produce una secuencia de salida para cada documento, y el resultado es
marginado para obtener la mejor respuesta.
Ficha RAG
Puede usar un documento diferente para generar cada token en la respuesta.
Esto permite que el Generador sintetice
evidencia de varios documentos.
Dado que los modelos RAG-Token tienden a funcionar mejor que los de RAG-Sequence, usaremos el modelo de token que fue
afinado en NQ como nuestro generador.
Instanciar un Generador en Haystack es similar al Lector, pero en lugar de
especificando los parámetros max_seq_length y doc_stride para una ventana deslizante sobre los contextos,
especificar hiperparámetros que controlan la generación de texto:

Aquí max_length y min_length controlan la longitud de las respuestas generadas, mientras que num_beams especifica
el número de haces que se utilizarán en la búsqueda de haces (la generación de texto se trata detalladamente en el Capítulo 8).
Como hicimos con el
DPR Retriever, no incrustamos los títulos de los documentos ya que nuestro corpus siempre se filtra por ID de producto.
Lo siguiente que debe hacer es unir el Retriever y el Generador usando GenerativeQAPipeline de Haystack:
del pajar.
importación de tubería GenerativeQAPipeline
tubería = GenerativeQAPipeline(generador=generador, recuperador=dpr_retriever)

NOTA
En RAG, tanto el codificador de consultas como el generador se entrenan de extremo a extremo, mientras que el codificador de contexto está congelado..
En Pajar, el
GenerativeQAPipeline usa el codificador de consultas de RAGenerator y el codificador de contexto de DensePassageRetriever.
Ahora demos una vuelta a RAG alimentando algunas consultas sobre la tableta Amazon Fire de antes.
Para simplificar el
consultando, escribamos una función simple que tome la consulta e imprima las respuestas principales:

Hmm, este resultado es un poco decepcionante y sugiere que la naturaleza subjetiva de la pregunta está confundiendo al
Generador.
Intentemos con algo un poco más factual:
generar_respuestas("¿Cuál es el principal inconveniente?")
Pregunta: ¿Cuál es el principal inconveniente?

Bien, ¡esto es más sensato! Para obtener mejores resultados, podríamos ajustar RAG de extremo a extremo en SubjQA, y si está
Si está interesado en explorar esto, hay scripts en el repositorio de Transformers para ayudarlo a comenzar..
Conclusión
Bueno, ese fue un recorrido relámpago por el control de calidad y probablemente tenga muchas más preguntas que le gustaría que respondieran (juego de palabras).
¡destinado!).
Hemos discutido dos enfoques de control de calidad (extractivo y generativo), y examinado dos diferentes
algoritmos de recuperación (BM25 y DPR).
En el camino, vimos que la adaptación del dominio puede ser una técnica simple para
aumentar el rendimiento de nuestro sistema de control de calidad por un margen significativo, y analizamos algunos de los más comunes
métricas que se utilizan para evaluar tales sistemas.
Aunque nos centramos en el control de calidad de dominio cerrado (i.
mi.
un solo dominio
de productos electrónicos), las técnicas en este capítulo se pueden generalizar fácilmente al caso de dominio abierto y
recomendamos leer la excelente serie Fast Forward QA de Cloudera para ver lo que implica.
La implementación de sistemas de control de calidad en la naturaleza puede ser un negocio difícil de hacer bien, y nuestra experiencia es que una parte significativa
del valor proviene primero de proporcionar a los usuarios finales capacidades de búsqueda útiles, seguidas de una extracción
componente.
En este sentido, el Lector se puede utilizar de formas novedosas más allá de responder a las consultas de los usuarios bajo demanda..
Para
Por ejemplo, Grid Dynamics pudo usar su Reader para extraer automáticamente un conjunto de pros y contras para cada
producto en el catálogo de su cliente.
De manera similar, muestran que un Lector también se puede usar para extraer entidades nombradas en
una moda de disparo cero mediante la creación de consultas como "¿Qué tipo de cámara?".
Dada su infancia y sus sutiles modos de falla,
recomienda explorar la generación de respuestas solo una vez que se hayan agotado los otros dos enfoques.
Este
La "jerarquía de necesidades" para abordar los problemas de garantía de calidad se ilustra en la Figura 4-14.
Figura 4-14.
La jerarquía de necesidades de QA.
Mirando hacia el futuro, un área de investigación interesante a tener en cuenta es el control de calidad multimodal, que involucra control de calidad.
sobre múltiples modalidades como texto, tablas e imágenes.
Como se describe en el benchmark 15 de MultiModalQA, tales
Los sistemas pueden potencialmente permitir a los usuarios responder preguntas complejas como "¿Cuándo fue la famosa pintura con dos
tocar los dedos completado? que integran información a través de diferentes modalidades.
Otra zona con prácticas
aplicaciones comerciales es el control de calidad sobre un gráfico de conocimiento, donde los nodos del gráfico corresponden al mundo real
entidades y sus relaciones están definidas por los bordes.
Al codificar factoides como (sujeto, predicado, objeto) triples, uno
puede usar el gráfico para responder preguntas sobre uno de los elementos que faltan.
Puedes encontrar un ejemplo que combina
transformers con grafos de conocimiento en los tutoriales de Haystack.
Una última dirección prometedora es "pregunta automática".
generación” como una forma de realizar algún tipo de entrenamiento no supervisado/débilmente supervisado a partir de datos o datos no etiquetados.
aumento.
Dos ejemplos recientes de documentos sobre esto incluyen las Preguntas probablemente respondidas (PAQ)
benchmark16 y aumento de datos sintéticos17 para entornos multilingües.
En este capítulo, hemos visto que para usar con éxito los modelos de control de calidad en casos de uso del mundo real, necesitamos aplicar un
algunos trucos, como una canalización de recuperación rápida para hacer predicciones casi en tiempo real.
Aun así, aplicar un modelo de control de calidad a un
un puñado de documentos preseleccionados puede tardar un par de segundos en el hardware de producción.
Aunque esto no
parece mucho imagina lo diferente que sería tu experiencia si tuvieras que esperar unos segundos para obtener el
resultados de tu búsqueda en Google.
Unos segundos de tiempo de espera pueden decidir el destino de su transformador alimentado
aplicación y en el próximo capítulo veremos algunos métodos para acelerar aún más las predicciones del modelo.
1 En este caso particular, no dar ninguna respuesta en realidad puede ser la elección correcta, ya que la respuesta depende de cuándo se hace la pregunta y
se refiere a una pandemia global donde tener información de salud precisa es esencial.
2 SUBJQA: un conjunto de datos para la subjetividad y la comprensión de revisión, J.
Bjerva et al..
(2020)
3 Como pronto veremos, también hay preguntas sin respuesta que están diseñadas para producir modelos de comprensión de lectura más sólidos..
4 SQuAD: más de 100 000 preguntas para la comprensión automática de texto, P.
Rajpurkar et al..
(2016)
5 MINILM: Destilación profunda de autoatención para la compresión agnóstica de tareas de transformadores preentrenados, W.
Wang y otros (2020)
6 Tenga en cuenta que token_type_ids no están presentes en todos los modelos de transformadores.
En el caso de modelos tipo BERT como MiniLM, el
token_type_ids también se utilizan durante el entrenamiento previo para incorporar la tarea de predicción de la siguiente oración.
7 Consulte el Capítulo 2 para obtener detalles sobre cómo se pueden extraer estos estados ocultos.
8 Sepa lo que no sabe: Preguntas sin respuesta para SQuAD, P.
Rajpurkar, R..
Jia y P..
Liang (2018)
9 preguntas naturales: un punto de referencia para la investigación de respuestas a preguntas, T.
Kwiatkowski y otros (2019)
10 La guía también proporciona instrucciones de instalación para mac OS y Windows.
11 Para obtener una explicación detallada sobre la puntuación de documentos con TF-IDF y BM25, consulte el Capítulo 23 de Procesamiento del habla y el lenguaje, D.
Jurafsky y
j.
H.
Martín (2020)
12 Recuperación de pasajes densos para responder preguntas de dominio abierto, V.
Karpukhin et al (2020)
13 Aprendizaje y evaluación de la inteligencia lingüística general D.
Yogatama et al..
(2019)
14 Generación de recuperación aumentada para tareas de PNL intensivas en conocimiento, P.
Lewis y otros (2020)
15 MultiModalQA: respuesta a preguntas complejas sobre texto, tablas e imágenes, A.
Talmor y otros (2021)
16 PAQ: 65 millones de preguntas probables y qué puede hacer con ellas, P.
Lewis y otros (2021).
17 Aumento de datos sintéticos para respuesta a preguntas multilingües Zero-Shot, A.
Riabi et al (2020).